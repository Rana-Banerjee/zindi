{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find out all tokens getting mapped to unk with Marian Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rana/Projects/zindi/transformers/src/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-af-fr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 8065/8065 [00:03<00:00, 2247.00 examples/s]\n",
      "Map: 100%|██████████| 1471/1471 [00:00<00:00, 2715.60 examples/s]\n",
      "Map: 100%|██████████| 1393/1393 [00:00<00:00, 2073.88 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset,concatenate_datasets\n",
    "zindi_ds = load_dataset(\"uvci/Koumankan_mt_dyu_fr\")\n",
    "def preprocess_function(examples):\n",
    "    inputs = [example[\"dyu\"] for example in examples[\"translation\"]]\n",
    "    targets = [example[\"fr\"] for example in examples[\"translation\"]]\n",
    "    model_inputs = tokenizer(inputs, text_target=targets, max_length=48, truncation=True, padding=\"max_length\")\n",
    "    # Check for None values in input_ids and labels\n",
    "    if None in model_inputs[\"input_ids\"] or None in model_inputs[\"labels\"]:\n",
    "        print(\"Warning: None values found in tokenized output\")\n",
    "        # Remove examples with None values\n",
    "        valid_indices = [i for i, (inp, lab) in enumerate(zip(model_inputs[\"input_ids\"], model_inputs[\"labels\"]))\n",
    "                         if inp is not None and lab is not None]\n",
    "        for key in model_inputs.keys():\n",
    "            model_inputs[key] = [model_inputs[key][i] for i in valid_indices]\n",
    "    return model_inputs\n",
    "tokenized_zds = zindi_ds.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=zindi_ds[\"train\"].column_names  # Remove original columns\n",
    ")\n",
    "concat_ds = concatenate_datasets([tokenized_zds['train'], tokenized_zds['test']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/rana/Projects/zindi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rana/.cache/pypoetry/virtualenvs/zindi-z3yfXQo9-py3.9/lib/python3.9/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15563, 15563, 31126)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('data/all_dyu.txt', 'r') as f:\n",
    "    dyu_data = [line.strip() for line in f]\n",
    "with open('data/all_fr.txt', 'r') as f:\n",
    "    fr_data = [line.strip() for line in f]\n",
    "\n",
    "all_data = dyu_data+fr_data\n",
    "len(dyu_data), len(fr_data), len(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rana/Projects/zindi/transformers/src/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens mapped to <unk>:\n",
      "ɲ\n",
      "–\n",
      "’\n",
      "»\n",
      "”\n",
      "«\n",
      "ɛɲɛ\n",
      "ɛɲ\n",
      "ɲɛɛ\n",
      "ğ\n",
      "ɛɛ\n",
      "—«\n",
      "υ\n",
      "—\n",
      "’«\n",
      "ɲɛɲɛ\n",
      "“\n",
      "Õ\n",
      "ɲɛ\n",
      "ŋ\n",
      "̋\n",
      "ʻ\n",
      "Ɔ\n",
      "ɛ\n",
      "ɛ”\n",
      "ễ\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-af-fr\")\n",
    "\n",
    "# Your corpus of data (replace this with your actual data)\n",
    "corpus = all_data\n",
    "\n",
    "# Function to find tokens mapped to <unk>\n",
    "def find_unk_tokens(text):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    unk_tokens = [token for token, id in zip(tokens, ids) if id == 1]\n",
    "    return unk_tokens\n",
    "\n",
    "# Process the corpus\n",
    "all_unk_tokens = set()\n",
    "for sentence in corpus:\n",
    "    unk_tokens = find_unk_tokens(sentence)\n",
    "    all_unk_tokens.update(unk_tokens)\n",
    "\n",
    "# Print the results\n",
    "print(\"Tokens mapped to <unk>:\")\n",
    "for token in all_unk_tokens:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/unmapped_fr_tokens.txt','w') as f:\n",
    "    for token in all_unk_tokens:\n",
    "         f.write(token + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_unk_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.add_tokens(list(all_unk_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('tokenizer_custom/marian_v2/tokenizer_config.json',\n",
       " 'tokenizer_custom/marian_v2/special_tokens_map.json',\n",
       " 'tokenizer_custom/marian_v2/vocab.json',\n",
       " 'tokenizer_custom/marian_v2/source.spm',\n",
       " 'tokenizer_custom/marian_v2/target.spm',\n",
       " 'tokenizer_custom/marian_v2/added_tokens.json')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"tokenizer_custom/marian_v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zindi-z3yfXQo9-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
