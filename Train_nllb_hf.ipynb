{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer, AutoConfig, GenerationConfig\n",
    "from transformers.integrations import TensorBoardCallback\n",
    "import evaluate\n",
    "import numpy as np\n",
    "metric = evaluate.load(\"bleu\")\n",
    "source_lang = \"dyu_Latn\"\n",
    "target_lang = \"fra_Latn\"\n",
    "checkpoint = \"facebook/nllb-200-distilled-600M\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#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\n",
    "#huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.80k/1.80k [00:00<00:00, 11.7kB/s]\n",
      "Downloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 530k/530k [00:00<00:00, 948kB/s]\n",
      "Downloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 102k/102k [00:00<00:00, 317kB/s]\n",
      "Downloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55.8k/55.8k [00:00<00:00, 115kB/s]\n",
      "Generating train split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8065/8065 [00:00<00:00, 781730.95 examples/s]\n",
      "Generating validation split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1471/1471 [00:00<00:00, 443872.03 examples/s]\n",
      "Generating test split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1393/1393 [00:00<00:00, 444207.82 examples/s]\n"
     ]
    }
   ],
   "source": [
    "zindi_ds = load_dataset(\"uvci/Koumankan_mt_dyu_fr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import unicodedata\n",
    "from sacremoses import MosesPunctNormalizer\n",
    "\n",
    "mpn = MosesPunctNormalizer(lang=\"fr\")\n",
    "mpn.substitutions = [\n",
    "    (re.compile(r), sub) for r, sub in mpn.substitutions\n",
    "]\n",
    "\n",
    "def get_non_printing_char_replacer(replace_by: str = \" \"):\n",
    "    non_printable_map = {\n",
    "        ord(c): replace_by\n",
    "        for c in (chr(i) for i in range(sys.maxunicode + 1))\n",
    "        # same as \\p{C} in perl\n",
    "        # see https://www.unicode.org/reports/tr44/#General_Category_Values\n",
    "        if unicodedata.category(c) in {\"C\", \"Cc\", \"Cf\", \"Cs\", \"Co\", \"Cn\"}\n",
    "    }\n",
    "\n",
    "    def replace_non_printing_char(line) -> str:\n",
    "        return line.translate(non_printable_map)\n",
    "\n",
    "    return replace_non_printing_char\n",
    "\n",
    "replace_nonprint = get_non_printing_char_replacer(\" \")\n",
    "\n",
    "def preproc(text):\n",
    "    clean = mpn.normalize(text)\n",
    "    clean = replace_nonprint(clean)\n",
    "    # replace ùìïùîØùîûùî´ùî†ùî¢ùî∞ùî†ùîû by Francesca\n",
    "    clean = unicodedata.normalize(\"NFKC\", clean)\n",
    "    return clean\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [preproc(example[\"dyu\"]) for example in examples[\"translation\"]]\n",
    "    targets = [preproc(example[\"fr\"]) for example in examples[\"translation\"]]\n",
    "    model_inputs = tokenizer(inputs, text_target=targets, max_length=50, truncation=True, padding=\"max_length\")\n",
    "    # Check for None values in input_ids and labels\n",
    "    if None in model_inputs[\"input_ids\"] or None in model_inputs[\"labels\"]:\n",
    "        print(\"Warning: None values found in tokenized output\")\n",
    "        # Remove examples with None values\n",
    "        valid_indices = [i for i, (inp, lab) in enumerate(zip(model_inputs[\"input_ids\"], model_inputs[\"labels\"]))\n",
    "                         if inp is not None and lab is not None]\n",
    "        for key in model_inputs.keys():\n",
    "            model_inputs[key] = [model_inputs[key][i] for i in valid_indices]\n",
    "    return model_inputs\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [[label.strip()] for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    print(result)\n",
    "    result = {\"bleu\": result[\"bleu\"]}\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/zindi/transformers/src/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8065/8065 [00:01<00:00, 7898.22 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1471/1471 [00:00<00:00, 7768.09 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1393/1393 [00:00<00:00, 8712.94 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# source_lang = \"dyu_Latn\"\n",
    "# target_lang = \"fra_Latn\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint, src_lang=source_lang, tgt_lang=target_lang)\n",
    "# Apply preprocessing to the dataset\n",
    "tokenized_zds = zindi_ds.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=zindi_ds[\"train\"].column_names  # Remove original columns\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_ds = concatenate_datasets([tokenized_zds['train'], tokenized_zds['test']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Max len of 50 is enough\n",
    "# def length_excluding_terminating_ones(list_of_lists):\n",
    "#     lengths = []\n",
    "#     for lst in list_of_lists:\n",
    "#         # Reverse the list and find the first occurrence of a number not equal to 1\n",
    "#         index = next((i for i, x in enumerate(reversed(lst)) if x != 1), len(lst))\n",
    "#         # Calculate the length excluding the trailing 1s\n",
    "#         lengths.append(len(lst) - index)\n",
    "#     return lengths\n",
    "# max(length_excluding_terminating_ones(tokenized_zds['validation']['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# M2M100Config {\n",
    "#   \"_name_or_path\": \"facebook/nllb-200-distilled-600M\",\n",
    "#   \"activation_dropout\": 0.0,\n",
    "#   \"activation_function\": \"relu\",\n",
    "#   \"architectures\": [\n",
    "#     \"M2M100ForConditionalGeneration\"\n",
    "#   ],\n",
    "#   \"attention_dropout\": 0.1,\n",
    "#   \"bos_token_id\": 0,\n",
    "#   \"d_model\": 1024,\n",
    "#   \"decoder_attention_heads\": 16,\n",
    "#   \"decoder_ffn_dim\": 4096,\n",
    "#   \"decoder_layerdrop\": 0,\n",
    "#   \"decoder_layers\": 12,\n",
    "#   \"decoder_start_token_id\": 2,\n",
    "#   \"dropout\": 0.1,\n",
    "#   \"encoder_attention_heads\": 16,\n",
    "#   \"encoder_ffn_dim\": 4096,\n",
    "#   \"encoder_layerdrop\": 0,\n",
    "#   \"encoder_layers\": 12,\n",
    "#   \"eos_token_id\": 2,\n",
    "#   \"init_std\": 0.02,\n",
    "#   \"is_encoder_decoder\": true,\n",
    "#   \"max_length\": 200,\n",
    "#   \"max_position_embeddings\": 1024,\n",
    "#   \"model_type\": \"m2m_100\",\n",
    "#   \"num_hidden_layers\": 12,\n",
    "#   \"pad_token_id\": 1,\n",
    "#   \"scale_embedding\": true,\n",
    "#   \"tokenizer_class\": \"NllbTokenizer\",\n",
    "#   \"torch_dtype\": \"float32\",\n",
    "#   \"transformers_version\": \"4.45.0.dev0\",\n",
    "#   \"use_cache\": true,\n",
    "#   \"vocab_size\": 256206\n",
    "# }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!rm -rf models/nllb/nllb_output/base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 50}\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "# GenerationConfig {\n",
    "#   \"bos_token_id\": 0,\n",
    "#   \"decoder_start_token_id\": 2,\n",
    "#   \"eos_token_id\": 2,\n",
    "#   \"max_length\": 200,\n",
    "#   \"pad_token_id\": 1\n",
    "# }\n",
    "# Create a GenerationConfig object\n",
    "\n",
    "#Load model and config\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n",
    "config = AutoConfig.from_pretrained(checkpoint)\n",
    "\n",
    "#Save model,config and tokenizer\n",
    "model.save_pretrained('models/nllb/nllb_output/base_model')\n",
    "tokenizer.save_pretrained('models/nllb/nllb_output/base_model')\n",
    "\n",
    "#Update config\n",
    "config.dropout=0.5\n",
    "config.max_length=50\n",
    "config.save_pretrained('models/nllb/nllb_output/base_model')\n",
    "\n",
    "#Update generation config\n",
    "generation_config = GenerationConfig(\n",
    "  bos_token_id= 0,\n",
    "  decoder_start_token_id= 2,\n",
    "  eos_token_id= 2,\n",
    "  max_length= 50,\n",
    "  pad_token_id= 1\n",
    ")\n",
    "generation_config.save_pretrained('models/nllb/nllb_output/base_model')\n",
    "del model\n",
    "gc.collect()\n",
    "\n",
    "#Reload model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained('models/nllb/nllb_output/base_model')\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model='models/nllb/nllb_output/base_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc, torch\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/root/.cache/pypoetry/virtualenvs/zindi-LtLKIbXv-py3.9/lib/python3.9/site-packages/accelerate/accelerator.py:488: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='62' max='4600000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [     62/4600000 04:08 < 5287:24:31, 0.24 it/s, Epoch 2.58/200000]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>7.606600</td>\n",
       "      <td>6.207423</td>\n",
       "      <td>0.049500</td>\n",
       "      <td>10.530300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5.417800</td>\n",
       "      <td>4.421810</td>\n",
       "      <td>0.040900</td>\n",
       "      <td>8.951100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu': 0.04952015179256702, 'precisions': [0.2826850842118018, 0.07490415806546741, 0.038036587574714724, 0.019626168224299065], 'brevity_penalty': 0.7853631067016231, 'length_ratio': 0.8054064604274421, 'translation_length': 8253, 'reference_length': 10247}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 50}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu': 0.040949269502868375, 'precisions': [0.2959963325183374, 0.08791642026414351, 0.044444444444444446, 0.023378333882120513], 'brevity_penalty': 0.5678705119379831, 'length_ratio': 0.6386259392993071, 'translation_length': 6544, 'reference_length': 10247}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 50}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"models/nllb/nllb_output\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=1e-4,\n",
    "    per_device_train_batch_size=20,\n",
    "    per_device_eval_batch_size=20,\n",
    "    # weight_decay=0.01,\n",
    "    num_train_epochs=200000,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    "    # push_to_hub=False,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    gradient_accumulation_steps=20,\n",
    "    logging_dir= \"models/nllb/nllb_output/logs\",\n",
    "    logging_steps = 1,\n",
    "    save_strategy = 'epoch',\n",
    "    save_steps = 2,\n",
    "    save_total_limit = 3,\n",
    "    seed = 42,\n",
    "    dataloader_drop_last = False,\n",
    "    eval_steps = 1,\n",
    "    # label_smoothing_factor: float = 0.0,\n",
    "    optim = 'adafactor',\n",
    "    # resume_from_checkpoint: Optional[str] = None,\n",
    "    # fp16_backend: str = 'auto',\n",
    "    # batch_eval_metrics: bool = False,\n",
    "    # eval_on_start=True,\n",
    "    # generation_max_length= 50,\n",
    "    generation_num_beams=2,\n",
    "    generation_config = \"models/nllb/nllb_output/base_model/generation_config.json\",\n",
    "    run_name=\"Test1\"\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=concat_ds,\n",
    "    eval_dataset=tokenized_zds[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 100}\n"
     ]
    }
   ],
   "source": [
    "# trainer.save_model(\"/root/zindi/models/nllb/nllb_output/checkpoint-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zindi-z3yfXQo9-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
