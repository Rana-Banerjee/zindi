{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:  8000\n",
      "Vocab size: 8000\n",
      "Tokenized: ['▁an', '▁ni', '▁sɔgɔma', '▁hɛrɛ', '▁sira', '▁so', 'mɔgɔw', '▁do', '?']\n",
      "\n",
      "Training:  16000\n",
      "Vocab size: 16000\n",
      "Tokenized: ['▁an', '▁ni', '▁sɔgɔma', '▁hɛrɛ', '▁sira', '▁so', 'mɔgɔw', '▁do', '?']\n",
      "\n",
      "Training:  32000\n",
      "Vocab size: 32000\n",
      "Tokenized: ['▁an', '▁ni', '▁sɔgɔma', '▁hɛrɛ', '▁sira', '▁somɔgɔw', '▁do', '?']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vocab_sizes = [8000, 16000, 32000]\n",
    "\n",
    "for size in vocab_sizes:\n",
    "    print(\"Training: \", size)\n",
    "    spm.SentencePieceTrainer.train(\n",
    "        input='/root/zindi/data/all_dyu.txt',\n",
    "        model_prefix=f'dyula_{size}',\n",
    "        vocab_size=size,\n",
    "        character_coverage=1.0,\n",
    "        model_type='bpe',\n",
    "        input_sentence_size=1000000,\n",
    "        shuffle_input_sentence=True\n",
    "    )\n",
    "\n",
    "    # Load the model and test it on some sample text\n",
    "    sp = spm.SentencePieceProcessor()\n",
    "    sp.load(f'dyula_{size}.model')\n",
    "    \n",
    "    sample_text = \"an ni sɔgɔma hɛrɛ sira somɔgɔw do?\"\n",
    "    print(f\"Vocab size: {size}\")\n",
    "    print(f\"Tokenized: {sp.encode(sample_text, out_type=str)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 8000\n",
      "Tokenized: ['▁Votre', '▁opér', 'ateur', '▁téléph', 'onique', '▁local', '▁devrait', '▁pouvoir', '▁vous', '▁en', '▁dire', '▁plus', '▁au', '▁sujet', '▁de', '▁la', '▁connexion', '▁à', '▁ce', '▁service', '.']\n",
      "\n",
      "Vocab size: 16000\n",
      "Tokenized: ['▁Votre', '▁opér', 'ateur', '▁téléphonique', '▁local', '▁devrait', '▁pouvoir', '▁vous', '▁en', '▁dire', '▁plus', '▁au', '▁sujet', '▁de', '▁la', '▁connexion', '▁à', '▁ce', '▁service', '.']\n",
      "\n",
      "Vocab size: 32000\n",
      "Tokenized: ['▁Votre', '▁opérateur', '▁téléphonique', '▁local', '▁devrait', '▁pouvoir', '▁vous', '▁en', '▁dire', '▁plus', '▁au', '▁sujet', '▁de', '▁la', '▁connexion', '▁à', '▁ce', '▁service', '.']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vocab_sizes = [8000, 16000, 32000]\n",
    "\n",
    "for size in vocab_sizes:\n",
    "    spm.SentencePieceTrainer.train(\n",
    "        input='/root/zindi/data/all_fr.txt',\n",
    "        model_prefix=f'fr_{size}',\n",
    "        vocab_size=size,\n",
    "        character_coverage=1.0,\n",
    "        model_type='bpe',\n",
    "        input_sentence_size=1000000,\n",
    "        shuffle_input_sentence=True\n",
    "    )\n",
    "\n",
    "    # Load the model and test it on some sample text\n",
    "    sp = spm.SentencePieceProcessor()\n",
    "    sp.load(f'fr_{size}.model')\n",
    "    \n",
    "    sample_text = \"Votre opérateur téléphonique local devrait pouvoir vous en dire plus au sujet de la connexion à ce service.\"\n",
    "    print(f\"Vocab size: {size}\")\n",
    "    print(f\"Tokenized: {sp.encode(sample_text, out_type=str)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train Dyula SentencePiece model\n",
    "spm.SentencePieceTrainer.train(\n",
    "    input='/root/zindi/data/all_dyu.txt',\n",
    "    model_prefix='dyu',\n",
    "    vocab_size=16000,\n",
    "    character_coverage=1.0,\n",
    "    model_type='bpe',\n",
    "    input_sentence_size=1000000,\n",
    "    shuffle_input_sentence=True\n",
    ")\n",
    "\n",
    "# Train French SentencePiece model\n",
    "spm.SentencePieceTrainer.train(\n",
    "    input='/root/zindi/data/all_fr.txt',\n",
    "    model_prefix='fr',\n",
    "    vocab_size=16000,\n",
    "    character_coverage=1.0,\n",
    "    model_type='bpe',\n",
    "    input_sentence_size=1000000,\n",
    "    shuffle_input_sentence=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create combined vocabulary\n",
    "def load_vocab(file_path):\n",
    "    vocab = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            piece = line.strip().split('\\t')[0]\n",
    "            if not piece.startswith('<'):  # Skip special tokens\n",
    "                vocab.append(piece)\n",
    "    return vocab\n",
    "\n",
    "dyula_vocab = load_vocab('/root/zindi/common/dyu.vocab')\n",
    "french_vocab = load_vocab('/root/zindi/common/fr.vocab')\n",
    "\n",
    "# Combine vocabularies\n",
    "combined_vocab = list(set(dyula_vocab + french_vocab))\n",
    "\n",
    "# Add special tokens\n",
    "special_tokens = ['<s>', '</s>', '<pad>', '<unk>']\n",
    "combined_vocab = special_tokens + combined_vocab\n",
    "\n",
    "# Write combined vocabulary to file\n",
    "with open('combined_vocab.txt', 'w', encoding='utf-8') as f:\n",
    "    for piece in combined_vocab:\n",
    "        f.write(f\"{piece}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/zindi/transformers/src/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./dyula_french_tokenizer/tokenizer_config.json',\n",
       " './dyula_french_tokenizer/special_tokens_map.json',\n",
       " './dyula_french_tokenizer/vocab.json',\n",
       " './dyula_french_tokenizer/source.spm',\n",
       " './dyula_french_tokenizer/target.spm',\n",
       " './dyula_french_tokenizer/added_tokens.json')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import MarianTokenizer\n",
    "\n",
    "# Create the tokenizer\n",
    "tokenizer = MarianTokenizer.from_pretrained(\n",
    "    \"Helsinki-NLP/opus-mt-af-fr\",  # We're using this as a base and will override its components\n",
    "    src_vocab_file=\"dyu.vocab\",  # Path to the Dyula vocabulary file\n",
    "    tgt_vocab_file=\"fr.vocab\",  # Path to the French vocabulary file\n",
    "    source_spm=\"dyu.model\",  # Path to the Dyula SentencePiece model file\n",
    "    target_spm=\"fr.model\",  # Path to the French SentencePiece model file\n",
    "    source_lang=\"dyu\",\n",
    "    target_lang=\"fr\"\n",
    ")\n",
    "\n",
    "# Adjust special tokens if necessary\n",
    "tokenizer.unk_token = \"<unk>\"\n",
    "tokenizer.pad_token = \"<pad>\"\n",
    "tokenizer.bos_token = \"<s>\"\n",
    "tokenizer.eos_token = \"</s>\"\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained('./dyula_french_tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zindi-z3yfXQo9-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
