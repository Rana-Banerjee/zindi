{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/rana/Projects/zindi\n"
     ]
    }
   ],
   "source": [
    "%cd /home/rana/Projects/zindi\n",
    "import yaml\n",
    "with open('common/config.yaml', 'r') as f:\n",
    "    config = yaml.load(f, Loader=yaml.SafeLoader)\n",
    "checkpoint = config.get('checkpoint')\n",
    "image_name = config.get('image_name')\n",
    "file_path = config.get('file_path')\n",
    "ct_model_path=config.get('ct_model_path')+checkpoint\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    yaml_content = yaml.safe_load(file)\n",
    "    yaml_content['services']['translation_inference_util']['image'] = image_name\n",
    "\n",
    "# Save the updated YAML content back to the file\n",
    "    with open(file_path, 'w') as file:\n",
    "        yaml.dump(yaml_content, file, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf deployment/saved_model/\n",
    "!mkdir deployment/saved_model/\n",
    "!cp -r {ct_model_path} deployment/saved_model/{checkpoint}/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/rana/Projects/zindi/deployment\n"
     ]
    }
   ],
   "source": [
    "%cd deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements.txt\n",
    "# transformers\n",
    "# sacremoses>=0.1.1\n",
    "pyyaml>=6.0.2\n",
    "kserve>=0.13.1\n",
    "ctranslate2==4.3.1\n",
    "# sentencepiece==0.1.99\n",
    "# kserve==0.11.2\n",
    "# torch>=2.4.0\n",
    "# accelerate = \"^0.33.0\"\n",
    "sentencepiece>=0.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"main.py\"\n",
    "with open(filename, \"w\") as file:\n",
    "    file.write(f\"\"\"\n",
    "\\\"\\\"\\\"\n",
    "KServe inference script for NLLB-200 translation model.\n",
    "\\\"\\\"\\\"\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "from typing import List\n",
    "from kserve import (InferOutput, InferRequest, InferResponse, Model, ModelServer, model_server)\n",
    "from kserve.utils.utils import generate_uuid\n",
    "#from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import ctranslate2\n",
    "import sentencepiece as spm\n",
    "\n",
    "# Constants\n",
    "MODEL_DIR = \"./saved_model/{checkpoint}\"\n",
    "\n",
    "class TranslationModel(Model):\n",
    "    \\\"\\\"\\\"\n",
    "    KServe inference implementation of NLLB-200 translation model.\n",
    "    \\\"\\\"\\\"\n",
    "\n",
    "    def __init__(self, name: str):\n",
    "        \\\"\\\"\\\"\n",
    "        Initialize the translation model.\n",
    "        Args:\n",
    "            name (str): Name of the model.\n",
    "        \\\"\\\"\\\"        \n",
    "        super().__init__(name)\n",
    "        self.name = name\n",
    "        self.ready = False\n",
    "        self.model = None\n",
    "        #self.tokenizer = None\n",
    "        self.sp_source_model = None\n",
    "        self.sp_target_model = None\n",
    "        self.load()\n",
    "\n",
    "    def load(self) -> None:\n",
    "        \\\"\\\"\\\"\n",
    "        Load model and tokenizer from disk.\n",
    "        \\\"\\\"\\\"\n",
    "        try:\n",
    "            self.sp_source_model = spm.SentencePieceProcessor(model_file=MODEL_DIR+'/sentencepiece.bpe.model')\n",
    "            self.sp_target_model = spm.SentencePieceProcessor(model_file=MODEL_DIR+'/sentencepiece.bpe.model')\n",
    "            #self.tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)\n",
    "            # self.model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_DIR)\n",
    "            self.model = ctranslate2.Translator(MODEL_DIR)\n",
    "            print('Model and tokenizer loaded')\n",
    "            self.ready = True\n",
    "        except Exception as e:\n",
    "            print('Error loading model: ', e)\n",
    "            self.ready = False\n",
    "\n",
    "    def preprocess(self, payload: InferRequest, *args, **kwargs) -> str:\n",
    "        \\\"\\\"\\\"\n",
    "        Preprocess inference request.\n",
    "\n",
    "        Args:\n",
    "            payload (InferRequest): The input payload containing the text to translate.\n",
    "\n",
    "        Returns:\n",
    "            str: Preprocessed text ready for translation.\n",
    "        \\\"\\\"\\\"\n",
    "        return payload.inputs[0].data[0].lower()\n",
    "\n",
    "    def predict(self, data: str, *args, **kwargs) -> InferResponse:\n",
    "        \\\"\\\"\\\"\n",
    "        Make prediction using the model.\n",
    "        Args:\n",
    "            data (str): Preprocessed input text.\n",
    "\n",
    "        Returns:\n",
    "            InferResponse: KServe inference response containing the translated text.\n",
    "        \\\"\\\"\\\"\n",
    "        source_sentences = [data.strip()]\n",
    "        print(source_sentences)\n",
    "        translation = self._translate(self.model, source_sentences)[0]\n",
    "\n",
    "        return self._create_response(translation)\n",
    "    \n",
    "\n",
    "    # Ctranslate2 translation\n",
    "    def _translate(self, model, text):\n",
    "        tokens = self.sp_source_model.encode(text, out_type=str)\n",
    "        tokens[0].insert(0,\"dyu_Latn\")\n",
    "        tokens[0].append(\"</s>\")\n",
    "        tokens[0].append(\"fra_Latn\")\n",
    "        # tokens = [\"dyu_Latn\"] + [[t] for t in tokens] + [\"</s>\"] + [\"fra_Latn\"]\n",
    "        try:\n",
    "            results = model.translate_batch(tokens)\n",
    "            # The translated results are token strings, so we need to convert them to IDs before decoding\n",
    "            translations = []\n",
    "            for translation in results:\n",
    "                # Convert token strings to IDs before decoding\n",
    "                decoded_text = self.sp_target_model.decode(translation.hypotheses[0]).replace(\"fra_Latn \",\"\")\n",
    "                translations.append(decoded_text)\n",
    "        except Exception as e:\n",
    "            print(f\"Translation error: \", e)\n",
    "            translations = [\"Error: \"+str(e)]  # Return empty string if translation fails\n",
    "        # translations = [\"some thing\"] \n",
    "        return translations\n",
    "\n",
    "    def _create_response(self, translation: str) -> InferResponse:\n",
    "        \\\"\\\"\\\"\n",
    "        Create InferResponse object.\n",
    "\n",
    "        Args:\n",
    "            translation (str): Translated text.\n",
    "\n",
    "        Returns:\n",
    "            InferResponse: KServe inference response object.\n",
    "        \\\"\\\"\\\"\n",
    "        return InferResponse(\n",
    "            model_name=self.name,\n",
    "            infer_outputs=[InferOutput(name=\\\"output-0\\\", shape=[1], datatype=\\\"STR\\\", data=[translation])],\n",
    "            response_id=generate_uuid()\n",
    "        )\n",
    "\n",
    "def parse_arguments() -> argparse.Namespace:\n",
    "    \\\"\\\"\\\"\n",
    "    Parse command-line arguments.\n",
    "\n",
    "    Returns:\n",
    "        argparse.Namespace: Parsed command-line arguments.\n",
    "    \\\"\\\"\\\"\n",
    "    parser = argparse.ArgumentParser(parents=[model_server.parser])\n",
    "    # Check if '--model_name' is already defined\n",
    "    model_name_defined = any('--model_name' in action.option_strings for action in model_server.parser._actions)\n",
    "\n",
    "    if not model_name_defined:\n",
    "        model_server.parser.add_argument(\n",
    "            '--model_name', \n",
    "            default='model', \n",
    "            help='The name that the model is served under.'\n",
    "        )\n",
    "    return parser.parse_args()\n",
    "\n",
    "def main():\n",
    "    \\\"\\\"\\\"\n",
    "    Main function to start the model server.\n",
    "    \\\"\\\"\\\"\n",
    "    args = parse_arguments()\n",
    "    model = TranslationModel(args.model_name)\n",
    "    ModelServer().start([model])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import subprocess\n",
    "\n",
    "def run_command(command_str):\n",
    "    # Prompt for the sudo password\n",
    "    sudo_password = getpass.getpass(\"Enter your sudo password: \")\n",
    "    # Run the command with sudo, passing the password\n",
    "    result = subprocess.run(f\"echo {sudo_password} | sudo -S {command_str}\", shell=True, check=True)\n",
    "    # Check the result\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[sudo] password for rana:  Container translation_inference_ctutil  Stopping\n",
      " Container translation_inference_ctutil  Stopped\n",
      " Container translation_inference_ctutil  Removing\n",
      " Container translation_inference_ctutil  Removed\n",
      " Network deployment_default  Removing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CompletedProcess(args='echo 1234 | sudo -S docker compose down', returncode=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Network deployment_default  Removed\n"
     ]
    }
   ],
   "source": [
    "run_command(\"docker compose down\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[sudo] password for rana: #0 building with \"default\" instance using docker driver\n",
      "\n",
      "#1 [internal] load build definition from Dockerfile\n",
      "#1 transferring dockerfile: 360B done\n",
      "#1 DONE 0.2s\n",
      "\n",
      "#2 [internal] load metadata for docker.io/library/python:3.10.14-slim\n",
      "#2 DONE 1.6s\n",
      "\n",
      "#3 [internal] load .dockerignore\n",
      "#3 transferring context: 2B done\n",
      "#3 DONE 0.2s\n",
      "\n",
      "#4 [1/6] FROM docker.io/library/python:3.10.14-slim@sha256:8666a639a54acc810408e505e2c6b46b50834385701675ee177f578b3d2fdef9\n",
      "#4 DONE 0.0s\n",
      "\n",
      "#5 [internal] load build context\n",
      "#5 transferring context: 5.28kB done\n",
      "#5 DONE 0.2s\n",
      "\n",
      "#6 [2/6] WORKDIR /app\n",
      "#6 CACHED\n",
      "\n",
      "#7 [3/6] COPY ./requirements.txt .\n",
      "#7 CACHED\n",
      "\n",
      "#8 [4/6] RUN pip install --no-cache-dir -r requirements.txt\n",
      "#8 CACHED\n",
      "\n",
      "#9 [5/6] COPY ./saved_model /app/saved_model\n",
      "#9 CACHED\n",
      "\n",
      "#10 [6/6] COPY ./main.py /app/main.py\n",
      "#10 DONE 1.6s\n",
      "\n",
      "#11 exporting to image\n",
      "#11 exporting layers\n",
      "#11 exporting layers 0.7s done\n",
      "#11 writing image sha256:d6043c2aa839c86c22c82bd6d5207435dc4b17985afe181816a5f5c3b6fa6ca1 0.1s done\n",
      "#11 naming to docker.io/library/zindi-image:0.3.0 0.1s done\n",
      "#11 DONE 1.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CompletedProcess(args='echo 1234 | sudo -S docker build -t zindi-image:0.3.0 .', returncode=0)\n"
     ]
    }
   ],
   "source": [
    "run_command(f\"docker build -t {image_name} .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[sudo] password for rana:  Network deployment_default  Creating\n",
      " Network deployment_default  Created\n",
      " Container translation_inference_ctutil  Creating\n",
      " Container translation_inference_ctutil  Created\n",
      " Container translation_inference_ctutil  Starting\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CompletedProcess(args='echo 1234 | sudo -S docker compose up -d', returncode=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Container translation_inference_ctutil  Started\n"
     ]
    }
   ],
   "source": [
    "run_command(f\"docker compose up -d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"model_name\":\"model\",\"model_version\":null,\"id\":\"b08d321d-34f1-461d-8ec0-d5360152d24d\",\"parameters\":null,\"outputs\":[{\"name\":\"output-0\",\"shape\":[1],\"datatype\":\"STR\",\"parameters\":null,\"data\":[\"Je suis sûr que j'aurais aimé.\"]}]}"
     ]
    }
   ],
   "source": [
    "!curl -X POST http://localhost:8080/v2/models/model/infer -H 'Content-Type: application/json' -d @/home/rana/Projects/zindi/common/input.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "translation_inference_ctutil  | 2024-08-19 13:57:20.190 1 kserve INFO [model_server.py:register_model():384] Registering model: model\n",
      "translation_inference_ctutil  | 2024-08-19 13:57:20.190 1 kserve INFO [model_server.py:start():254] Setting max asyncio worker threads as 5\n",
      "translation_inference_ctutil  | 2024-08-19 13:57:20.191 1 kserve INFO [model_server.py:serve():260] Starting uvicorn with 1 workers\n",
      "translation_inference_ctutil  | 2024-08-19 13:57:20.249 uvicorn.error INFO:     Started server process [1]\n",
      "translation_inference_ctutil  | 2024-08-19 13:57:20.250 uvicorn.error INFO:     Waiting for application startup.\n",
      "translation_inference_ctutil  | 2024-08-19 13:57:20.254 1 kserve INFO [server.py:start():63] Starting gRPC server on [::]:8081\n",
      "translation_inference_ctutil  | 2024-08-19 13:57:20.255 uvicorn.error INFO:     Application startup complete.\n",
      "translation_inference_ctutil  | 2024-08-19 13:57:20.255 uvicorn.error INFO:     Uvicorn running on http://0.0.0.0:8080 (Press CTRL+C to quit)\n",
      "translation_inference_ctutil  | 2024-08-19 13:57:24.262 kserve.trace requestId: N.A., preprocess_ms: 0.134944916, explain_ms: 0, predict_ms: 2573.272228241, postprocess_ms: 0.018596649\n",
      "translation_inference_ctutil  | Model and tokenizer loaded\n",
      "translation_inference_ctutil  | ['n dale do a la ko a tun bi na diya n ye']\n",
      "translation_inference_ctutil  | 2024-08-19 13:57:24.263 uvicorn.access INFO:     172.18.0.1:43144 1 - \"POST /v2/models/model/infer HTTP/1.1\" 200 OK\n",
      "translation_inference_ctutil  | 2024-08-19 13:57:24.263 kserve.trace kserve.io.kserve.protocol.rest.v2_endpoints.infer: 2.57680344581604\n",
      "translation_inference_ctutil  | 2024-08-19 13:57:24.264 kserve.trace kserve.io.kserve.protocol.rest.v2_endpoints.infer: 2.5641929999999995\n",
      "CompletedProcess(args='echo 1234 | sudo -S sudo docker compose logs', returncode=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[sudo] password for rana: "
     ]
    }
   ],
   "source": [
    "run_command(f\"sudo docker compose logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ../submission\n",
    "!mkdir ../submission\n",
    "!mkdir ../submission/deployment\n",
    "!cp Dockerfile ../submission/deployment/\n",
    "!cp main.py ../submission/deployment/main.py\n",
    "!cp requirements.txt ../submission/deployment/requirements.txt\n",
    "# !cp image_name.txt ../submission/image_name.txt\n",
    "!cp README.md ../submission/README.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_uri = \"058264459447.dkr.ecr.eu-west-1.amazonaws.com/highwind/f463c7a9-f305-472f-8e49-5f602a7d7882/46c4085f-0ca6-4200-b853-3bf994d3b2d2:latest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/rana/Projects/zindi/submission\n",
      "  adding: image_name.txt (deflated 18%)\n",
      "  adding: deployment/ (stored 0%)\n",
      "  adding: deployment/requirements.txt (deflated 29%)\n",
      "  adding: deployment/Dockerfile (deflated 29%)\n",
      "  adding: deployment/main.py (deflated 68%)\n",
      "  adding: README.md (stored 0%)\n"
     ]
    }
   ],
   "source": [
    "%cd ../submission/\n",
    "!rm -rf image_name.txt\n",
    "with open('image_name.txt', 'w') as f:\n",
    "    f.write(image_uri)\n",
    "!zip -r submission.zip ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zindi-z3yfXQo9-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
