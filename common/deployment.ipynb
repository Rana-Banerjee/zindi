{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_name = \"checkpoint-9\"\n",
    "image_name = 'zindi-image:0.2.5'\n",
    "model=\"/home/rana/Projects/zindi/models/marian/marian_output/\"+checkpoint_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir=\"/home/rana/Projects/zindi/models/marian/ct/\"+checkpoint_name\n",
    "file_path = \"/home/rana/Projects/zindi/deployment/docker-compose.yml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/rana/Projects/zindi/deployment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rana/.cache/pypoetry/virtualenvs/zindi-z3yfXQo9-py3.9/lib/python3.9/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd /home/rana/Projects/zindi/deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf saved_model/\n",
    "!mkdir saved_model/\n",
    "# !cp -r {model} saved_model/\n",
    "!cp -r {output_dir} saved_model/{checkpoint_name}/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml    \n",
    "with open(file_path, 'r') as file:\n",
    "    yaml_content = yaml.safe_load(file)\n",
    "    yaml_content['services']['translation_inference_util']['image'] = image_name\n",
    "\n",
    "# Save the updated YAML content back to the file\n",
    "    with open(file_path, 'w') as file:\n",
    "        yaml.dump(yaml_content, file, default_flow_style=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements.txt\n",
    "# transformers\n",
    "sacremoses>=0.1.1\n",
    "pyyaml>=6.0.2\n",
    "kserve>=0.13.1\n",
    "ctranslate2==4.3.1\n",
    "# sentencepiece==0.1.99\n",
    "# kserve==0.11.2\n",
    "# torch>=2.4.0\n",
    "# accelerate = \"^0.33.0\"\n",
    "sentencepiece>=0.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"main.py\"\n",
    "with open(filename, \"w\") as file:\n",
    "    file.write(f\"\"\"\n",
    "\\\"\\\"\\\"\n",
    "KServe inference script for NLLB-200 translation model.\n",
    "\\\"\\\"\\\"\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "from typing import List\n",
    "from kserve import (InferOutput, InferRequest, InferResponse, Model, ModelServer, model_server)\n",
    "from kserve.utils.utils import generate_uuid\n",
    "#from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import ctranslate2\n",
    "import sentencepiece as spm\n",
    "\n",
    "# Constants\n",
    "MODEL_DIR = \"./saved_model/{checkpoint_name}\"\n",
    "\n",
    "class TranslationModel(Model):\n",
    "    \\\"\\\"\\\"\n",
    "    KServe inference implementation of NLLB-200 translation model.\n",
    "    \\\"\\\"\\\"\n",
    "\n",
    "    def __init__(self, name: str):\n",
    "        \\\"\\\"\\\"\n",
    "        Initialize the translation model.\n",
    "        Args:\n",
    "            name (str): Name of the model.\n",
    "        \\\"\\\"\\\"        \n",
    "        super().__init__(name)\n",
    "        self.name = name\n",
    "        self.ready = False\n",
    "        self.model = None\n",
    "        #self.tokenizer = None\n",
    "        self.sp_source_model = None\n",
    "        self.sp_target_model = None\n",
    "        self.load()\n",
    "\n",
    "    def load(self) -> None:\n",
    "        \\\"\\\"\\\"\n",
    "        Load model and tokenizer from disk.\n",
    "        \\\"\\\"\\\"\n",
    "        try:\n",
    "            self.sp_source_model = spm.SentencePieceProcessor(model_file=MODEL_DIR+'/source.spm')\n",
    "            self.sp_target_model = spm.SentencePieceProcessor(model_file=MODEL_DIR+'/target.spm')\n",
    "            #self.tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)\n",
    "            # self.model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_DIR)\n",
    "            self.model = ctranslate2.Translator(MODEL_DIR)\n",
    "            print('Model and tokenizer loaded')\n",
    "            self.ready = True\n",
    "        except Exception as e:\n",
    "            print('Error loading model: ', e)\n",
    "            self.ready = False\n",
    "\n",
    "    def preprocess(self, payload: InferRequest, *args, **kwargs) -> str:\n",
    "        \\\"\\\"\\\"\n",
    "        Preprocess inference request.\n",
    "\n",
    "        Args:\n",
    "            payload (InferRequest): The input payload containing the text to translate.\n",
    "\n",
    "        Returns:\n",
    "            str: Preprocessed text ready for translation.\n",
    "        \\\"\\\"\\\"\n",
    "        return payload.inputs[0].data[0].lower()\n",
    "\n",
    "    def predict(self, data: str, *args, **kwargs) -> InferResponse:\n",
    "        \\\"\\\"\\\"\n",
    "        Make prediction using the model.\n",
    "        Args:\n",
    "            data (str): Preprocessed input text.\n",
    "\n",
    "        Returns:\n",
    "            InferResponse: KServe inference response containing the translated text.\n",
    "        \\\"\\\"\\\"\n",
    "        source_sentences = [data.strip()]\n",
    "        print(source_sentences)\n",
    "        translation = self._translate(self.model, source_sentences)[0]\n",
    "\n",
    "        return self._create_response(translation)\n",
    "    \n",
    "    # def _translate(self, model, tokenizer, text):\n",
    "    #     inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=150)\n",
    "    #     translated = model.generate(**inputs)\n",
    "    #     return tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "\n",
    "    # Ctranslate2 translation\n",
    "    def _translate(self, model, text):\n",
    "        #tokens = tokenizer.tokenize(text)\n",
    "        tokens = self.sp_source_model.encode(text, out_type=str)\n",
    "        # print(tokens)\n",
    "        tokens = [\"dyu\"] + tokens + [\"</s>\"] + [\"fr\"]\n",
    "        try:\n",
    "            results = model.translate_batch(tokens)\n",
    "            # The translated results are token strings, so we need to convert them to IDs before decoding\n",
    "            translations = []\n",
    "            for translation in results:\n",
    "                # Convert token strings to IDs before decoding\n",
    "                #token_ids = tokenizer.convert_tokens_to_ids(translation.hypotheses[0])\n",
    "                #decoded_text = tokenizer.decode(token_ids)\n",
    "                decoded_text = self.sp_target_model.decode(translation.hypotheses[0])\n",
    "                translations.append(decoded_text)\n",
    "        except Exception as e:\n",
    "            print(f\"Translation error: \", e)\n",
    "            translations = [\"\"]  # Return empty string if translation fails\n",
    "        return translations\n",
    "\n",
    "    def _create_response(self, translation: str) -> InferResponse:\n",
    "        \\\"\\\"\\\"\n",
    "        Create InferResponse object.\n",
    "\n",
    "        Args:\n",
    "            translation (str): Translated text.\n",
    "\n",
    "        Returns:\n",
    "            InferResponse: KServe inference response object.\n",
    "        \\\"\\\"\\\"\n",
    "        return InferResponse(\n",
    "            model_name=self.name,\n",
    "            infer_outputs=[InferOutput(name=\\\"output-0\\\", shape=[1], datatype=\\\"STR\\\", data=[translation])],\n",
    "            response_id=generate_uuid()\n",
    "        )\n",
    "\n",
    "def parse_arguments() -> argparse.Namespace:\n",
    "    \\\"\\\"\\\"\n",
    "    Parse command-line arguments.\n",
    "\n",
    "    Returns:\n",
    "        argparse.Namespace: Parsed command-line arguments.\n",
    "    \\\"\\\"\\\"\n",
    "    parser = argparse.ArgumentParser(parents=[model_server.parser])\n",
    "    # Check if '--model_name' is already defined\n",
    "    model_name_defined = any('--model_name' in action.option_strings for action in model_server.parser._actions)\n",
    "\n",
    "    if not model_name_defined:\n",
    "        model_server.parser.add_argument(\n",
    "            '--model_name', \n",
    "            default='model', \n",
    "            help='The name that the model is served under.'\n",
    "        )\n",
    "    return parser.parse_args()\n",
    "\n",
    "def main():\n",
    "    \\\"\\\"\\\"\n",
    "    Main function to start the model server.\n",
    "    \\\"\\\"\\\"\n",
    "    args = parse_arguments()\n",
    "    model = TranslationModel(args.model_name)\n",
    "    ModelServer().start([model])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import subprocess\n",
    "\n",
    "def run_command(command_str):\n",
    "    # Prompt for the sudo password\n",
    "    sudo_password = getpass.getpass(\"Enter your sudo password: \")\n",
    "    # Run the command with sudo, passing the password\n",
    "    result = subprocess.run(f\"echo {sudo_password} | sudo -S {command_str}\", shell=True, check=True)\n",
    "    # Check the result\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[sudo] password for rana:  Container translation_inference_ctutil  Stopping\n",
      " Container translation_inference_ctutil  Stopped\n",
      " Container translation_inference_ctutil  Removing\n",
      " Container translation_inference_ctutil  Removed\n",
      " Network deployment_default  Removing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CompletedProcess(args='echo 1234 | sudo -S docker compose down', returncode=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Network deployment_default  Removed\n"
     ]
    }
   ],
   "source": [
    "run_command(\"docker compose down\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[sudo] password for rana: #0 building with \"default\" instance using docker driver\n",
      "\n",
      "#1 [internal] load build definition from Dockerfile\n",
      "#1 transferring dockerfile: 360B done\n",
      "#1 DONE 0.2s\n",
      "\n",
      "#2 [internal] load metadata for docker.io/library/python:3.10.14-slim\n",
      "#2 DONE 1.4s\n",
      "\n",
      "#3 [internal] load .dockerignore\n",
      "#3 transferring context: 2B done\n",
      "#3 DONE 0.2s\n",
      "\n",
      "#4 [1/6] FROM docker.io/library/python:3.10.14-slim@sha256:8b3815a0a8f9a554c0f8c40af7dae424c0fd962819c787188ebc2574d909d2df\n",
      "#4 DONE 0.0s\n",
      "\n",
      "#5 [internal] load build context\n",
      "#5 transferring context: 5.72kB done\n",
      "#5 DONE 0.2s\n",
      "\n",
      "#6 [2/6] WORKDIR /app\n",
      "#6 CACHED\n",
      "\n",
      "#7 [3/6] COPY ./requirements.txt .\n",
      "#7 CACHED\n",
      "\n",
      "#8 [4/6] RUN pip install --no-cache-dir -r requirements.txt\n",
      "#8 CACHED\n",
      "\n",
      "#9 [5/6] COPY ./saved_model /app/saved_model\n",
      "#9 CACHED\n",
      "\n",
      "#10 [6/6] COPY ./main.py /app/main.py\n",
      "#10 DONE 1.3s\n",
      "\n",
      "#11 exporting to image\n",
      "#11 exporting layers\n",
      "#11 exporting layers 0.6s done\n",
      "#11 writing image sha256:6443a53733cb759b046327a50b9c25af452374e410cd93931008bc5da134c429 0.0s done\n",
      "#11 naming to docker.io/library/zindi-image:0.2.5 0.1s done\n",
      "#11 DONE 1.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CompletedProcess(args='echo 1234 | sudo -S docker build -t zindi-image:0.2.5 .', returncode=0)\n"
     ]
    }
   ],
   "source": [
    "run_command(f\"docker build -t {image_name} .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[sudo] password for rana:  Network deployment_default  Creating\n",
      " Network deployment_default  Created\n",
      " Container translation_inference_ctutil  Creating\n",
      " Container translation_inference_ctutil  Created\n",
      " Container translation_inference_ctutil  Starting\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CompletedProcess(args='echo 1234 | sudo -S docker compose up -d', returncode=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Container translation_inference_ctutil  Started\n"
     ]
    }
   ],
   "source": [
    "run_command(f\"docker compose up -d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"model_name\":\"model\",\"model_version\":null,\"id\":\"27f4a593-3a91-4cd8-b969-4ae03605c55b\",\"parameters\":null,\"outputs\":[{\"name\":\"output-0\",\"shape\":[1],\"datatype\":\"STR\",\"parameters\":null,\"data\":[\"Une vallée------------- jusqu'à un yeeeee do au cou au cou tun bi à un yeye de yeeee yeye ye ye ye ye yeye yeye yeye ye ye ye ye ye ye ye ye ye ye ye ye ye ye ye ye ye ye ye ye ye ye ye ye ye ye ye ye ye ye ye ye ye ye ye ye ye ye ye ye ye ye ye ye ye ye ye ye ye ye ye ye ye ye ye ye ye y y y y ye y y y y y y y y y y y y y y y y y y y y y y y y y y y y y y y y y y y y y y y y y y y y y y y y y y y y y y y y y y y y y y y y y y y y y y y y\"]}]}"
     ]
    }
   ],
   "source": [
    "!curl -X POST http://localhost:8080/v2/models/model/infer -H 'Content-Type: application/json' -d @./input.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "translation_inference_ctutil  | 2024-08-11 16:47:12.427 1 kserve INFO [model_server.py:register_model():384] Registering model: model\n",
      "translation_inference_ctutil  | 2024-08-11 16:47:12.427 1 kserve INFO [model_server.py:start():254] Setting max asyncio worker threads as 5\n",
      "translation_inference_ctutil  | 2024-08-11 16:47:12.428 1 kserve INFO [model_server.py:serve():260] Starting uvicorn with 1 workers\n",
      "translation_inference_ctutil  | 2024-08-11 16:47:12.508 uvicorn.error INFO:     Started server process [1]\n",
      "translation_inference_ctutil  | 2024-08-11 16:47:12.508 uvicorn.error INFO:     Waiting for application startup.\n",
      "translation_inference_ctutil  | 2024-08-11 16:47:12.511 1 kserve INFO [server.py:start():63] Starting gRPC server on [::]:8081\n",
      "translation_inference_ctutil  | 2024-08-11 16:47:12.512 uvicorn.error INFO:     Application startup complete.\n",
      "translation_inference_ctutil  | 2024-08-11 16:47:12.512 uvicorn.error INFO:     Uvicorn running on http://0.0.0.0:8080 (Press CTRL+C to quit)\n",
      "translation_inference_ctutil  | 2024-08-11 16:47:24.215 kserve.trace requestId: N.A., preprocess_ms: 0.013589859, explain_ms: 0, predict_ms: 10773.061990738, postprocess_ms: 0.035524368\n",
      "translation_inference_ctutil  | Model and tokenizer loaded\n",
      "translation_inference_ctutil  | ['n dale do a la ko a tun bi na diya n ye']\n",
      "translation_inference_ctutil  | [['▁n', '▁dal', 'e', '▁do', '▁a', '▁la', '▁ko', '▁a', '▁t', 'un', '▁bi', '▁na', '▁di', 'ya', '▁n', '▁', 'ye']]\n",
      "translation_inference_ctutil  | 2024-08-11 16:47:24.216 uvicorn.access INFO:     172.18.0.1:45798 1 - \"POST /v2/models/model/infer HTTP/1.1\" 200 OK\n",
      "translation_inference_ctutil  | 2024-08-11 16:47:24.216 kserve.trace kserve.io.kserve.protocol.rest.v2_endpoints.infer: 10.776228666305542\n",
      "translation_inference_ctutil  | 2024-08-11 16:47:24.217 kserve.trace kserve.io.kserve.protocol.rest.v2_endpoints.infer: 10.428483\n",
      "CompletedProcess(args='echo 1234 | sudo -S sudo docker compose logs', returncode=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[sudo] password for rana: "
     ]
    }
   ],
   "source": [
    "run_command(f\"sudo docker compose logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zindi-z3yfXQo9-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
