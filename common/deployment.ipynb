{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/rana/Projects/zindi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rana/.cache/pypoetry/virtualenvs/zindi-z3yfXQo9-py3.9/lib/python3.9/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd /home/rana/Projects/zindi\n",
    "import yaml\n",
    "with open('common/config.yaml', 'r') as f:\n",
    "    config = yaml.load(f, Loader=yaml.SafeLoader)\n",
    "checkpoint = config.get('checkpoint')\n",
    "image_name = config.get('image_name')\n",
    "file_path = config.get('file_path')\n",
    "ct_model_path=config.get('ct_model_path')+checkpoint\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    yaml_content = yaml.safe_load(file)\n",
    "    yaml_content['services']['translation_inference_util']['image'] = image_name\n",
    "\n",
    "# Save the updated YAML content back to the file\n",
    "    with open(file_path, 'w') as file:\n",
    "        yaml.dump(yaml_content, file, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'models/nllb/checkpoint-618'"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ct_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf deployment/saved_model/\n",
    "!mkdir deployment/saved_model/\n",
    "!cp -r {ct_model_path} deployment/saved_model/{checkpoint}/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/rana/Projects/zindi/deployment\n"
     ]
    }
   ],
   "source": [
    "%cd deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements.txt\n",
    "# transformers\n",
    "# sacremoses>=0.1.1\n",
    "pyyaml>=6.0.2\n",
    "kserve>=0.13.1\n",
    "ctranslate2==4.3.1\n",
    "# sentencepiece==0.1.99\n",
    "# kserve==0.11.2\n",
    "# torch>=2.4.0\n",
    "# accelerate = \"^0.33.0\"\n",
    "sentencepiece>=0.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename = \"main.py\"\n",
    "# with open(filename, \"w\") as file:\n",
    "#     file.write(f\"\"\"\n",
    "# \\\"\\\"\\\"\n",
    "# KServe inference script for NLLB-200 translation model.\n",
    "# \\\"\\\"\\\"\n",
    "\n",
    "# import argparse\n",
    "# import os\n",
    "# from typing import List\n",
    "# from kserve import (InferOutput, InferRequest, InferResponse, Model, ModelServer, model_server)\n",
    "# from kserve.utils.utils import generate_uuid\n",
    "# #from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "# import ctranslate2\n",
    "# import sentencepiece as spm\n",
    "# import re\n",
    "# import sys\n",
    "# import unicodedata\n",
    "# from sacremoses import MosesPunctNormalizer\n",
    "\n",
    "# # Constants\n",
    "# MODEL_DIR = \"./saved_model/{checkpoint}\"\n",
    "\n",
    "# def get_non_printing_char_replacer(replace_by: str = \" \"):\n",
    "#     non_printable_map = {{\n",
    "#         ord(c): replace_by\n",
    "#         for c in (chr(i) for i in range(sys.maxunicode + 1))\n",
    "#         # same as \\p{{C}} in perl\n",
    "#         # see https://www.unicode.org/reports/tr44/#General_Category_Values\n",
    "#         if unicodedata.category(c) in {{\"C\", \"Cc\", \"Cf\", \"Cs\", \"Co\", \"Cn\"}}\n",
    "#     }}\n",
    "\n",
    "#     def replace_non_printing_char(line) -> str:\n",
    "#         return line.translate(non_printable_map)\n",
    "\n",
    "#     return replace_non_printing_char\n",
    "\n",
    "# replace_nonprint = get_non_printing_char_replacer(\" \")\n",
    "\n",
    "# class TranslationModel(Model):\n",
    "#     \\\"\\\"\\\"\n",
    "#     KServe inference implementation of NLLB-200 translation model.\n",
    "#     \\\"\\\"\\\"\n",
    "\n",
    "#     def __init__(self, name: str):\n",
    "#         \\\"\\\"\\\"\n",
    "#         Initialize the translation model.\n",
    "#         Args:\n",
    "#             name (str): Name of the model.\n",
    "#         \\\"\\\"\\\"        \n",
    "#         super().__init__(name)\n",
    "#         self.name = name\n",
    "#         self.ready = False\n",
    "#         self.model = None\n",
    "#         self.tokenizer = None\n",
    "#         # self.sp_source_model = None\n",
    "#         # self.sp_target_model = None\n",
    "#         self.mpn=None\n",
    "#         self.load()\n",
    "\n",
    "#     def load(self) -> None:\n",
    "#         \\\"\\\"\\\"\n",
    "#         Load model and tokenizer from disk.\n",
    "#         \\\"\\\"\\\"\n",
    "#         try:\n",
    "#             self.tokenizer = spm.SentencePieceProcessor(model_file=MODEL_DIR+'/sentencepiece.bpe.model')\n",
    "#             self.model = ctranslate2.Translator(MODEL_DIR)\n",
    "#             self.mpn = MosesPunctNormalizer(lang=\"fr\")\n",
    "#             self.mpn.substitutions = [(re.compile(r), sub) for r, sub in self.mpn.substitutions]\n",
    "#             print('Model and tokenizer loaded')\n",
    "#             self.ready = True\n",
    "#         except Exception as e:\n",
    "#             print('Error loading model: ', e)\n",
    "#             self.ready = False\n",
    "\n",
    "#     def preprocess(self, payload: InferRequest, *args, **kwargs) -> str:\n",
    "#         \\\"\\\"\\\"\n",
    "#         Preprocess inference request.\n",
    "\n",
    "#         Args:\n",
    "#             payload (InferRequest): The input payload containing the text to translate.\n",
    "\n",
    "#         Returns:\n",
    "#             str: Preprocessed text ready for translation.\n",
    "#         \\\"\\\"\\\"\n",
    "\n",
    "#         text = payload.inputs[0].data[0]\n",
    "#         clean = self.mpn.normalize(text)\n",
    "#         clean = replace_nonprint(clean)\n",
    "#         clean = unicodedata.normalize(\"NFKC\", clean)\n",
    "#         return clean\n",
    "\n",
    "#     def predict(self, data: str, *args, **kwargs) -> InferResponse:\n",
    "#         \\\"\\\"\\\"\n",
    "#         Make prediction using the model.\n",
    "#         Args:\n",
    "#             data (str): Preprocessed input text.\n",
    "\n",
    "#         Returns:\n",
    "#             InferResponse: KServe inference response containing the translated text.\n",
    "#         \\\"\\\"\\\"\n",
    "#         source_sentences = data.strip()\n",
    "#         source_sentences = self.mpn.normalize(source_sentences)\n",
    "#         source_sentences = replace_nonprint(source_sentences)\n",
    "#         source_sentences = unicodedata.normalize(\"NFKC\", source_sentences)\n",
    "#         translation = self._translate(self.model, source_sentences)\n",
    "\n",
    "#         return self._create_response(translation)\n",
    "    \n",
    "\n",
    "#     # Ctranslate2 translation\n",
    "#     def _translate(self, model, text):\n",
    "#         target_prefix = [['fra_Latn']] * len(text)\n",
    "#         source_sents_subworded = [[\"dyu_Latn\"] + self.tokenizer.encode_as_pieces(sent) + [\"</s>\"] for sent in [text]]\n",
    "#         try:\n",
    "#             translations= self.model.translate_batch(\n",
    "#                     source_sents_subworded,\n",
    "#                     batch_type=\"tokens\",\n",
    "#                     max_batch_size=256,\n",
    "#                     beam_size=1,\n",
    "#                     target_prefix=[[\"fra_Latn\"]],\n",
    "#                     return_scores=False,\n",
    "#                     return_attention=False,\n",
    "#                     return_alternatives=False,\n",
    "#                 )\n",
    "#             translation = translations[0].hypotheses[0]\n",
    "#             if \"fra_Latn\" in translation:\n",
    "#                 translation.remove(\"fra_Latn\")\n",
    "#             trans = self.tokenizer.decode(translation)\n",
    "#         except Exception as e:\n",
    "#             trans = [\"Error: \"+str(e)]  # Return empty string if translation fails\n",
    "#         return trans\n",
    "\n",
    "#     def _create_response(self, translation: str) -> InferResponse:\n",
    "#         \\\"\\\"\\\"\n",
    "#         Create InferResponse object.\n",
    "\n",
    "#         Args:\n",
    "#             translation (str): Translated text.\n",
    "\n",
    "#         Returns:\n",
    "#             InferResponse: KServe inference response object.\n",
    "#         \\\"\\\"\\\"\n",
    "#         return InferResponse(\n",
    "#             model_name=self.name,\n",
    "#             infer_outputs=[InferOutput(name=\\\"output-0\\\", shape=[1], datatype=\\\"STR\\\", data=[translation])],\n",
    "#             response_id=generate_uuid()\n",
    "#         )\n",
    "\n",
    "# def parse_arguments() -> argparse.Namespace:\n",
    "#     \\\"\\\"\\\"\n",
    "#     Parse command-line arguments.\n",
    "\n",
    "#     Returns:\n",
    "#         argparse.Namespace: Parsed command-line arguments.\n",
    "#     \\\"\\\"\\\"\n",
    "#     parser = argparse.ArgumentParser(parents=[model_server.parser])\n",
    "#     # Check if '--model_name' is already defined\n",
    "#     model_name_defined = any('--model_name' in action.option_strings for action in model_server.parser._actions)\n",
    "\n",
    "#     if not model_name_defined:\n",
    "#         model_server.parser.add_argument(\n",
    "#             '--model_name', \n",
    "#             default='model', \n",
    "#             help='The name that the model is served under.'\n",
    "#         )\n",
    "#     return parser.parse_args()\n",
    "\n",
    "# def main():\n",
    "#     \\\"\\\"\\\"\n",
    "#     Main function to start the model server.\n",
    "#     \\\"\\\"\\\"\n",
    "#     args = parse_arguments()\n",
    "#     model = TranslationModel(args.model_name)\n",
    "#     ModelServer().start([model])\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n",
    "\n",
    "# \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"main.py\"\n",
    "with open(filename, \"w\") as file:\n",
    "    file.write(f\"\"\"\n",
    "\\\"\\\"\\\"\n",
    "KServe inference script for NLLB-200 translation model.\n",
    "\\\"\\\"\\\"\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "from typing import List\n",
    "from kserve import (InferOutput, InferRequest, InferResponse, Model, ModelServer, model_server)\n",
    "from kserve.utils.utils import generate_uuid\n",
    "import ctranslate2\n",
    "import sentencepiece as spm\n",
    "\n",
    "# Constants\n",
    "MODEL_DIR = \"./saved_model/{checkpoint}\"\n",
    "\n",
    "class TranslationModel(Model):\n",
    "    \\\"\\\"\\\"\n",
    "    KServe inference implementation of NLLB-200 translation model.\n",
    "    \\\"\\\"\\\"\n",
    "\n",
    "    def __init__(self, name: str):\n",
    "        \\\"\\\"\\\"\n",
    "        Initialize the translation model.\n",
    "\n",
    "        Args:\n",
    "            name (str): Name of the model.\n",
    "        \\\"\\\"\\\"\n",
    "        super().__init__(name)\n",
    "        self.name = name\n",
    "        self.ready = False\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self.mpn = None\n",
    "        self.load()\n",
    "\n",
    "    def load(self) -> None:\n",
    "        \\\"\\\"\\\"\n",
    "        Load model and tokenizer from disk.\n",
    "        \\\"\\\"\\\"\n",
    "        try:\n",
    "            self.tokenizer = spm.SentencePieceProcessor(model_file=os.path.join(MODEL_DIR, 'sentencepiece.bpe.model'))\n",
    "            self.model = ctranslate2.Translator(\n",
    "                MODEL_DIR,\n",
    "                device=\"cpu\",\n",
    "                device_index=0,\n",
    "                compute_type=\"int8\",\n",
    "                intra_threads=os.cpu_count(),\n",
    "                inter_threads=1,\n",
    "            )\n",
    "            print('Model and tokenizer loaded')\n",
    "            self.ready = True\n",
    "        except Exception as e:\n",
    "            print('Error loading model:', e)\n",
    "            self.ready = False\n",
    "\n",
    "    def preprocess(self, payload: InferRequest, *args, **kwargs) -> str:\n",
    "        \\\"\\\"\\\"\n",
    "        Preprocess inference request.\n",
    "\n",
    "        Args:\n",
    "            payload (InferRequest): The input payload containing the text to translate.\n",
    "\n",
    "        Returns:\n",
    "            str: Preprocessed text ready for translation.\n",
    "        \\\"\\\"\\\"\n",
    "        text = payload.inputs[0].data[0]\n",
    "        return text.strip()\n",
    "\n",
    "    def predict(self, data: str, *args, **kwargs) -> InferResponse:\n",
    "        \\\"\\\"\\\"\n",
    "        Make prediction using the model.\n",
    "\n",
    "        Args:\n",
    "            data (str): Preprocessed input text.\n",
    "\n",
    "        Returns:\n",
    "            InferResponse: KServe inference response containing the translated text.\n",
    "        \\\"\\\"\\\"\n",
    "        translation = self._translate(self.model, data)\n",
    "        return self._create_response(translation)\n",
    "\n",
    "    def _translate(self, model, text):\n",
    "        \\\"\\\"\\\"\n",
    "        Translate the input text using the ctranslate2 library.\n",
    "\n",
    "        Args:\n",
    "            model (ctranslate2.Translator): The translation model.\n",
    "            text (str): The input text to be translated.\n",
    "\n",
    "        Returns:\n",
    "            str: The translated text.\n",
    "        \\\"\\\"\\\"\n",
    "        target_prefix = [['fra_Latn']] * len([text])\n",
    "        source_sents_subworded = [[\"dyu_Latn\"] + self.tokenizer.encode_as_pieces(sent) + [\"</s>\"] for sent in [text]]\n",
    "        try:\n",
    "            translations = self.model.translate_batch(\n",
    "                source_sents_subworded,\n",
    "                batch_type=\"tokens\",\n",
    "                max_batch_size=256,\n",
    "                beam_size=1,\n",
    "                target_prefix=target_prefix,\n",
    "                return_scores=False,\n",
    "                return_attention=False,\n",
    "                return_alternatives=False,\n",
    "            )\n",
    "            translation = translations[0].hypotheses[0]\n",
    "            if \"fra_Latn\" in translation:\n",
    "                translation.remove(\"fra_Latn\")\n",
    "            trans = self.tokenizer.decode(translation)\n",
    "        except Exception as e:\n",
    "            trans = [\"Error: \" + str(e)]  # Return error message if translation fails\n",
    "        return trans\n",
    "\n",
    "    def _create_response(self, translation: str) -> InferResponse:\n",
    "        \\\"\\\"\\\"\n",
    "        Create InferResponse object.\n",
    "\n",
    "        Args:\n",
    "            translation (str): Translated text.\n",
    "\n",
    "        Returns:\n",
    "            InferResponse: KServe inference response object.\n",
    "        \\\"\\\"\\\"\n",
    "        return InferResponse(\n",
    "            model_name=self.name,\n",
    "            infer_outputs=[InferOutput(name=\"output-0\", shape=[1], datatype=\"STR\", data=[translation])],\n",
    "            response_id=generate_uuid()\n",
    "        )\n",
    "\n",
    "def parse_arguments() -> argparse.Namespace:\n",
    "    \\\"\\\"\\\"\n",
    "    Parse command-line arguments.\n",
    "\n",
    "    Returns:\n",
    "        argparse.Namespace: Parsed command-line arguments.\n",
    "    \\\"\\\"\\\"\n",
    "    parser = argparse.ArgumentParser(parents=[model_server.parser])\n",
    "    # Check if '--model_name' is already defined\n",
    "    model_name_defined = any('--model_name' in action.option_strings for action in model_server.parser._actions)\n",
    "\n",
    "    if not model_name_defined:\n",
    "        model_server.parser.add_argument(\n",
    "            '--model_name',\n",
    "            default='model',\n",
    "            help='The name that the model is served under.'\n",
    "        )\n",
    "    return parser.parse_args()\n",
    "\n",
    "def main():\n",
    "    \\\"\\\"\\\"\n",
    "    Main function to start the model server.\n",
    "    \\\"\\\"\\\"\n",
    "    args = parse_arguments()\n",
    "    model = TranslationModel(args.model_name)\n",
    "    ModelServer().start([model])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import subprocess\n",
    "\n",
    "def run_command(command_str):\n",
    "    # Prompt for the sudo password\n",
    "    sudo_password = getpass.getpass(\"Enter your sudo password: \")\n",
    "    # Run the command with sudo, passing the password\n",
    "    result = subprocess.run(f\"echo {sudo_password} | sudo -S {command_str}\", shell=True, check=True)\n",
    "    # Check the result\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[sudo] password for rana:  Container translation_inference_ctutil  Stopping\n",
      " Container translation_inference_ctutil  Stopped\n",
      " Container translation_inference_ctutil  Removing\n",
      " Container translation_inference_ctutil  Removed\n",
      " Network deployment_default  Removing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CompletedProcess(args='echo 1234 | sudo -S docker compose down', returncode=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Network deployment_default  Removed\n"
     ]
    }
   ],
   "source": [
    "run_command(\"docker compose down\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[sudo] password for rana: #0 building with \"default\" instance using docker driver\n",
      "\n",
      "#1 [internal] load build definition from Dockerfile\n",
      "#1 transferring dockerfile: 360B done\n",
      "#1 DONE 0.2s\n",
      "\n",
      "#2 [internal] load metadata for docker.io/library/python:3.10.14-slim\n",
      "#2 DONE 2.8s\n",
      "\n",
      "#3 [internal] load .dockerignore\n",
      "#3 transferring context: 2B done\n",
      "#3 DONE 0.2s\n",
      "\n",
      "#4 [1/6] FROM docker.io/library/python:3.10.14-slim@sha256:8666a639a54acc810408e505e2c6b46b50834385701675ee177f578b3d2fdef9\n",
      "#4 DONE 0.0s\n",
      "\n",
      "#5 [internal] load build context\n",
      "#5 transferring context: 466.17MB 4.8s\n",
      "#5 transferring context: 633.53MB 6.8s done\n",
      "#5 DONE 7.0s\n",
      "\n",
      "#6 [2/6] WORKDIR /app\n",
      "#6 CACHED\n",
      "\n",
      "#7 [3/6] COPY ./requirements.txt .\n",
      "#7 CACHED\n",
      "\n",
      "#8 [4/6] RUN pip install --no-cache-dir -r requirements.txt\n",
      "#8 CACHED\n",
      "\n",
      "#9 [5/6] COPY ./saved_model /app/saved_model\n",
      "#9 DONE 16.1s\n",
      "\n",
      "#10 [6/6] COPY ./main.py /app/main.py\n",
      "#10 DONE 2.1s\n",
      "\n",
      "#11 exporting to image\n",
      "#11 exporting layers\n",
      "#11 exporting layers 4.3s done\n",
      "#11 writing image sha256:6ad0e983426744328358e59165065ec80ee5dba5059e53f958ed19eaf77b9fbc 0.1s done\n",
      "#11 naming to docker.io/library/zindi-image:0.3.1 0.1s done\n",
      "#11 DONE 4.7s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CompletedProcess(args='echo 1234 | sudo -S docker build -t zindi-image:0.3.1 .', returncode=0)\n"
     ]
    }
   ],
   "source": [
    "run_command(f\"docker build -t {image_name} .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[sudo] password for rana:  Network deployment_default  Creating\n",
      " Network deployment_default  Created\n",
      " Container translation_inference_ctutil  Creating\n",
      " Container translation_inference_ctutil  Created\n",
      " Container translation_inference_ctutil  Starting\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CompletedProcess(args='echo 1234 | sudo -S docker compose up -d', returncode=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Container translation_inference_ctutil  Started\n"
     ]
    }
   ],
   "source": [
    "run_command(f\"docker compose up -d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"model_name\":\"model\",\"model_version\":null,\"id\":\"c60e699e-80fe-43e0-a90e-2e4b527611a8\",\"parameters\":null,\"outputs\":[{\"name\":\"output-0\",\"shape\":[1],\"datatype\":\"STR\",\"parameters\":null,\"data\":[\"Jean et son père connus.\"]}]}"
     ]
    }
   ],
   "source": [
    "!curl -X POST http://localhost:8080/v2/models/model/infer -H 'Content-Type: application/json' -d @/home/rana/Projects/zindi/common/input.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[sudo] password for rana: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "translation_inference_ctutil  | 2024-08-21 12:04:17.471 1 kserve INFO [model_server.py:register_model():384] Registering model: model\n",
      "translation_inference_ctutil  | 2024-08-21 12:04:17.473 1 kserve INFO [model_server.py:start():254] Setting max asyncio worker threads as 5\n",
      "translation_inference_ctutil  | 2024-08-21 12:04:17.476 1 kserve INFO [model_server.py:serve():260] Starting uvicorn with 1 workers\n",
      "translation_inference_ctutil  | 2024-08-21 12:04:17.640 uvicorn.error INFO:     Started server process [1]\n",
      "translation_inference_ctutil  | 2024-08-21 12:04:17.640 uvicorn.error INFO:     Waiting for application startup.\n",
      "translation_inference_ctutil  | 2024-08-21 12:04:17.645 1 kserve INFO [server.py:start():63] Starting gRPC server on [::]:8081\n",
      "translation_inference_ctutil  | 2024-08-21 12:04:17.646 uvicorn.error INFO:     Application startup complete.\n",
      "translation_inference_ctutil  | 2024-08-21 12:04:17.647 uvicorn.error INFO:     Uvicorn running on http://0.0.0.0:8080 (Press CTRL+C to quit)\n",
      "translation_inference_ctutil  | 2024-08-21 12:04:21.183 kserve.trace requestId: N.A., preprocess_ms: 0.031709671, explain_ms: 0, predict_ms: 3180.800914764, postprocess_ms: 0.037908554\n",
      "translation_inference_ctutil  | Model and tokenizer loaded\n",
      "translation_inference_ctutil  | 2024-08-21 12:04:21.185 uvicorn.access INFO:     172.18.0.1:45578 1 - \"POST /v2/models/model/infer HTTP/1.1\" 200 OK\n",
      "translation_inference_ctutil  | 2024-08-21 12:04:21.185 kserve.trace kserve.io.kserve.protocol.rest.v2_endpoints.infer: 3.1855885982513428\n",
      "translation_inference_ctutil  | 2024-08-21 12:04:21.186 kserve.trace kserve.io.kserve.protocol.rest.v2_endpoints.infer: 3.1499029999999983\n",
      "translation_inference_ctutil  | 2024-08-21 12:04:34.499 kserve.trace requestId: N.A., preprocess_ms: 0.037193298, explain_ms: 0, predict_ms: 3237.173080444, postprocess_ms: 0.026702881\n",
      "translation_inference_ctutil  | 2024-08-21 12:04:34.500 uvicorn.access INFO:     172.18.0.1:42332 1 - \"POST /v2/models/model/infer HTTP/1.1\" 200 OK\n",
      "translation_inference_ctutil  | 2024-08-21 12:04:34.501 kserve.trace kserve.io.kserve.protocol.rest.v2_endpoints.infer: 3.241964817047119\n",
      "translation_inference_ctutil  | 2024-08-21 12:04:34.501 kserve.trace kserve.io.kserve.protocol.rest.v2_endpoints.infer: 3.1443350000000017\n",
      "CompletedProcess(args='echo 1234 | sudo -S sudo docker compose logs', returncode=0)\n"
     ]
    }
   ],
   "source": [
    "run_command(f\"sudo docker compose logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ../submission\n",
    "!mkdir ../submission\n",
    "!mkdir ../submission/deployment\n",
    "!cp Dockerfile ../submission/deployment/\n",
    "!cp main.py ../submission/deployment/main.py\n",
    "!cp requirements.txt ../submission/deployment/requirements.txt\n",
    "# !cp image_name.txt ../submission/image_name.txt\n",
    "!cp README.md ../submission/README.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_uri = \"058264459447.dkr.ecr.eu-west-1.amazonaws.com/highwind/f463c7a9-f305-472f-8e49-5f602a7d7882/875c2cfb-bc5b-4234-8826-3326639bea62:latest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/rana/Projects/zindi/submission\n",
      "  adding: image_name.txt (deflated 16%)\n",
      "  adding: deployment/ (stored 0%)\n",
      "  adding: deployment/requirements.txt (deflated 29%)\n",
      "  adding: deployment/Dockerfile (deflated 29%)\n",
      "  adding: deployment/main.py (deflated 67%)\n",
      "  adding: README.md (stored 0%)\n"
     ]
    }
   ],
   "source": [
    "%cd ../submission/\n",
    "!rm -rf image_name.txt\n",
    "with open('image_name.txt', 'w') as f:\n",
    "    f.write(image_uri)\n",
    "!zip -r submission.zip ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zindi-z3yfXQo9-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
