{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.cache/pypoetry/virtualenvs/zindi-LtLKIbXv-py3.9/lib/python3.9/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/zindi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.cache/pypoetry/virtualenvs/zindi-LtLKIbXv-py3.9/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# %cd /home/rana/Projects/zindi\n",
    "%cd /root/zindi/\n",
    "import yaml\n",
    "import json\n",
    "import os   \n",
    "import shutil\n",
    "import evaluate\n",
    "import numpy as np\n",
    "metric = evaluate.load(\"bleu\")\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "with open('common/config.yaml', 'r') as f:\n",
    "    config = yaml.load(f, Loader=yaml.SafeLoader)\n",
    "\n",
    "def delete_file_if_exists(file_path):\n",
    "    if os.path.exists(file_path):\n",
    "        os.remove(file_path)\n",
    "        print(f\"File {file_path} has been deleted.\")\n",
    "    else:\n",
    "        print(f\"File {file_path} does not exist.\")\n",
    "\n",
    "\n",
    "def copy_file_or_directory(source, destination):\n",
    "    try:\n",
    "        if os.path.isfile(source):\n",
    "            # If source is a file, copy it directly\n",
    "            shutil.copy2(source, destination)\n",
    "            print(f\"File copied successfully from {source} to {destination}\")\n",
    "        elif os.path.isdir(source):\n",
    "            # If source is a directory, copy all files within it\n",
    "            if not os.path.exists(destination):\n",
    "                os.makedirs(destination)\n",
    "            for item in os.listdir(source):\n",
    "                s = os.path.join(source, item)\n",
    "                d = os.path.join(destination, item)\n",
    "                if os.path.isfile(s):\n",
    "                    shutil.copy2(s, d)\n",
    "                    print(f\"File copied successfully from {s} to {d}\")\n",
    "        else:\n",
    "            print(f\"Source {source} is neither a file nor a directory.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Source not found: {source}\")\n",
    "    except PermissionError:\n",
    "        print(\"Permission denied. Check file permissions.\")\n",
    "    except shutil.SameFileError:\n",
    "        print(\"Source and destination are the same file.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "def create_or_clean_directory(path):\n",
    "    if os.path.exists(path):\n",
    "        # Path exists, so clean it\n",
    "        for item in os.listdir(path):\n",
    "            item_path = os.path.join(path, item)\n",
    "            if os.path.isfile(item_path):\n",
    "                os.unlink(item_path)\n",
    "            elif os.path.isdir(item_path):\n",
    "                shutil.rmtree(item_path)\n",
    "        print(f\"Cleaned existing directory: {path}\")\n",
    "    else:\n",
    "        # Path doesn't exist, so create it\n",
    "        os.makedirs(path)\n",
    "        print(f\"Created new directory: {path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [[label.strip()] for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "def safe_bleu_compute(bleu, predictions, references):\n",
    "    try:\n",
    "        return bleu.compute(predictions=predictions, references=references)\n",
    "    except ZeroDivisionError:\n",
    "        return {\"bleu\": 0.0, \"precisions\": [0.0, 0.0, 0.0, 0.0], \"brevity_penalty\": 0.0, \"length_ratio\": 0.0, \"translation_length\": 0, \"reference_length\": 0}\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "    print(decoded_preds[0], decoded_labels[0])\n",
    "    result = safe_bleu_compute(metric, predictions=decoded_preds, references=decoded_labels)\n",
    "    # result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    # print(result)\n",
    "    result = {\"bleu\": result[\"bleu\"]}\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    return result\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [example[\"dyu\"] for example in examples[\"translation\"]]\n",
    "    targets = [example[\"fr\"] for example in examples[\"translation\"]]\n",
    "\n",
    "    # Tokenize source and target\n",
    "    model_inputs = tokenizer(inputs, max_length=48, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    # Tokenize target separately with explicit padding token\n",
    "    labels = tokenizer(targets, max_length=48, padding=\"max_length\", truncation=True)\n",
    "\n",
    "    # Replace padding token with -100 for loss calculation\n",
    "    labels[\"input_ids\"] = [[(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]]\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "\n",
    "\n",
    "\n",
    "    # model_inputs = tokenizer(inputs, text_target=targets, max_length=48, truncation=True, padding=\"max_length\")\n",
    "    # labels = tokenizer(text_target=targets, max_length=48, padding=\"max_length\", truncation=True)\n",
    "    # labels[\"input_ids\"] = [[(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]]\n",
    "    # model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    # Check for None values in input_ids and labels\n",
    "    # if None in model_inputs[\"input_ids\"] or None in model_inputs[\"labels\"]:\n",
    "    #     print(\"Warning: None values found in tokenized output\")\n",
    "    #     # Remove examples with None values\n",
    "    #     valid_indices = [i for i, (inp, lab) in enumerate(zip(model_inputs[\"input_ids\"], model_inputs[\"labels\"]))\n",
    "    #                      if inp is not None and lab is not None]\n",
    "    #     for key in model_inputs.keys():\n",
    "    #         model_inputs[key] = [model_inputs[key][i] for i in valid_indices]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#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\n",
    "# !huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Device specific params\n",
    "import os\n",
    "os.environ['model_name_or_path'] = \"models/marian/marian_output/base_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned existing directory: models/marian/marian_output/base_model\n",
      "File copied successfully from /root/zindi/models/marian/marian_output/checkpoint-380/config.json to models/marian/marian_output/base_model/config.json\n",
      "File copied successfully from /root/zindi/models/marian/marian_output/checkpoint-380/generation_config.json to models/marian/marian_output/base_model/generation_config.json\n",
      "File copied successfully from /root/zindi/models/marian/marian_output/checkpoint-380/model.safetensors to models/marian/marian_output/base_model/model.safetensors\n",
      "File copied successfully from /root/zindi/models/marian/marian_output/checkpoint-380/optimizer.pt to models/marian/marian_output/base_model/optimizer.pt\n",
      "File copied successfully from /root/zindi/models/marian/marian_output/checkpoint-380/rng_state.pth to models/marian/marian_output/base_model/rng_state.pth\n",
      "File copied successfully from /root/zindi/models/marian/marian_output/checkpoint-380/scheduler.pt to models/marian/marian_output/base_model/scheduler.pt\n",
      "File copied successfully from /root/zindi/models/marian/marian_output/checkpoint-380/source.spm to models/marian/marian_output/base_model/source.spm\n",
      "File copied successfully from /root/zindi/models/marian/marian_output/checkpoint-380/special_tokens_map.json to models/marian/marian_output/base_model/special_tokens_map.json\n",
      "File copied successfully from /root/zindi/models/marian/marian_output/checkpoint-380/target.spm to models/marian/marian_output/base_model/target.spm\n",
      "File copied successfully from /root/zindi/models/marian/marian_output/checkpoint-380/tokenizer_config.json to models/marian/marian_output/base_model/tokenizer_config.json\n",
      "File copied successfully from /root/zindi/models/marian/marian_output/checkpoint-380/trainer_state.json to models/marian/marian_output/base_model/trainer_state.json\n",
      "File copied successfully from /root/zindi/models/marian/marian_output/checkpoint-380/training_args.bin to models/marian/marian_output/base_model/training_args.bin\n",
      "File copied successfully from /root/zindi/models/marian/marian_output/checkpoint-380/vocab.json to models/marian/marian_output/base_model/vocab.json\n",
      "File models/marian/marian_output/base_model/generation_config.json has been deleted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 48, 'bad_words_ids': [[59421]], 'forced_eos_token_id': 0}\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "from transformers import GenerationConfig, MarianConfig, AutoTokenizer, AutoConfig\n",
    "# first = True\n",
    "first = False\n",
    "\n",
    "# base_model_path = \"models/marian/marian_output/base_model\"\n",
    "# custom_tokenizer=\"tokenizer_custom/nllb\"\n",
    "\n",
    "create_or_clean_directory(os.environ['model_name_or_path'])\n",
    "if first:\n",
    "    # Download the model\n",
    "    # Load & save model:\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(config.get('model_name_or_path'))\n",
    "    model.save_pretrained(os.environ['model_name_or_path'])\n",
    "else:\n",
    "    # Copy checkpoint to base model path\n",
    "    copy_file_or_directory(config.get('model_name_or_path'), os.environ['model_name_or_path'])\n",
    "\n",
    "# Overwrite Generation Config\n",
    "\n",
    "delete_file_if_exists(os.environ['model_name_or_path']+'/generation_config.json')\n",
    "generation_config = GenerationConfig(\n",
    "bad_words_ids = [[59421]],\n",
    "# bos_token_id = 0,\n",
    "decoder_start_token_id = 59421,\n",
    "eos_token_id= 0,\n",
    "forced_eos_token_id= 0,\n",
    "max_length= 48,\n",
    "pad_token_id= 59421,\n",
    ")\n",
    "generation_config.save_pretrained(os.environ['model_name_or_path'] )\n",
    "\n",
    "\n",
    "#Overwrite Model Config\n",
    "mconfig = AutoConfig.from_pretrained(config.get(\"model_name_or_path\"))\n",
    "mconfig.dropout=0.5\n",
    "mconfig.max_length=48\n",
    "# mconfig.num_beams=1\n",
    "# # mconfig.vocab_size = 32000\n",
    "mconfig.bad_words_ids = [[59421]]\n",
    "# mconfig.bos_token_id = 0\n",
    "mconfig.decoder_start_token_id = 59421\n",
    "mconfig.eos_token_id=0\n",
    "mconfig.forced_eos_token_id=0\n",
    "# # mconfig.decoder_vocab_size = 32000\n",
    "mconfig.pad_token_id=59421\n",
    "# mconfig.save_pretrained(os.environ['model_name_or_path'] )\n",
    "\n",
    "\n",
    "#Overwrite Toeknizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"tokenizer_custom/marian_v2\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n",
    "# tokenizer.source_lang='dyu_Latn'\n",
    "# tokenizer.target_lang='fra_Latn'\n",
    "# tokenizer.src_lang='dyu_Latn'\n",
    "# tokenizer.tgt_lang='fra_Latn'\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(config.get('model_name_or_path'))\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"tokenizer_custom/marian_v2\")\n",
    "tokenizer.source_lang='af'\n",
    "tokenizer.target_lang='fr'\n",
    "tokenizer.src_lang='af'\n",
    "tokenizer.tgt_lang='fr'\n",
    "\n",
    "tokenizer.save_pretrained(os.environ['model_name_or_path'])  \n",
    "\n",
    "# Reload model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(os.environ['model_name_or_path'],config=mconfig)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.save_pretrained(os.environ['model_name_or_path']) \n",
    "# model.resize_token_embeddings(len(tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=os.environ['model_name_or_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 8065/8065 [00:00<00:00, 8173.35 examples/s]\n",
      "Map: 100%|██████████| 1471/1471 [00:00<00:00, 8269.24 examples/s]\n",
      "Map: 100%|██████████| 1393/1393 [00:00<00:00, 7954.66 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset,concatenate_datasets\n",
    "zindi_ds = load_dataset(\"uvci/Koumankan_mt_dyu_fr\")\n",
    "tokenized_zds = zindi_ds.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=zindi_ds[\"train\"].column_names  # Remove original columns\n",
    ")\n",
    "# concat_ds = concatenate_datasets([tokenized_zds['train'], tokenized_zds['test']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='876' max='800000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   876/800000 1:09:24 < 1057:45:19, 0.21 it/s, Epoch 216.05/200000]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.712000</td>\n",
       "      <td>4.175342</td>\n",
       "      <td>0.052600</td>\n",
       "      <td>15.243400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.671600</td>\n",
       "      <td>4.208504</td>\n",
       "      <td>0.051700</td>\n",
       "      <td>15.191700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.657300</td>\n",
       "      <td>4.233708</td>\n",
       "      <td>0.051700</td>\n",
       "      <td>15.285500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.645300</td>\n",
       "      <td>4.253683</td>\n",
       "      <td>0.053100</td>\n",
       "      <td>15.325600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.636300</td>\n",
       "      <td>4.267724</td>\n",
       "      <td>0.051400</td>\n",
       "      <td>15.352800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.624700</td>\n",
       "      <td>4.283550</td>\n",
       "      <td>0.051900</td>\n",
       "      <td>15.331100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.617700</td>\n",
       "      <td>4.301023</td>\n",
       "      <td>0.051300</td>\n",
       "      <td>15.420800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.609800</td>\n",
       "      <td>4.321321</td>\n",
       "      <td>0.050200</td>\n",
       "      <td>15.510500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.602500</td>\n",
       "      <td>4.329663</td>\n",
       "      <td>0.050100</td>\n",
       "      <td>15.583300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.597300</td>\n",
       "      <td>4.346901</td>\n",
       "      <td>0.050500</td>\n",
       "      <td>15.502400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.590100</td>\n",
       "      <td>4.359325</td>\n",
       "      <td>0.050700</td>\n",
       "      <td>15.431000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.579500</td>\n",
       "      <td>4.375281</td>\n",
       "      <td>0.051800</td>\n",
       "      <td>15.416000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.577100</td>\n",
       "      <td>4.386560</td>\n",
       "      <td>0.053100</td>\n",
       "      <td>15.541100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.568400</td>\n",
       "      <td>4.400327</td>\n",
       "      <td>0.050700</td>\n",
       "      <td>15.690000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.560500</td>\n",
       "      <td>4.410974</td>\n",
       "      <td>0.052500</td>\n",
       "      <td>15.560200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.558300</td>\n",
       "      <td>4.426604</td>\n",
       "      <td>0.052100</td>\n",
       "      <td>15.660800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.549800</td>\n",
       "      <td>4.434326</td>\n",
       "      <td>0.052800</td>\n",
       "      <td>15.645100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.544500</td>\n",
       "      <td>4.450103</td>\n",
       "      <td>0.051800</td>\n",
       "      <td>15.683900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.538300</td>\n",
       "      <td>4.466775</td>\n",
       "      <td>0.052700</td>\n",
       "      <td>15.698200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.531700</td>\n",
       "      <td>4.477864</td>\n",
       "      <td>0.053500</td>\n",
       "      <td>15.666900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.526600</td>\n",
       "      <td>4.488503</td>\n",
       "      <td>0.051100</td>\n",
       "      <td>15.739000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.522400</td>\n",
       "      <td>4.496532</td>\n",
       "      <td>0.053300</td>\n",
       "      <td>15.672300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.516000</td>\n",
       "      <td>4.510801</td>\n",
       "      <td>0.052700</td>\n",
       "      <td>15.705600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.511300</td>\n",
       "      <td>4.525867</td>\n",
       "      <td>0.051800</td>\n",
       "      <td>15.663500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.508000</td>\n",
       "      <td>4.534135</td>\n",
       "      <td>0.053100</td>\n",
       "      <td>15.609800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.500300</td>\n",
       "      <td>4.545705</td>\n",
       "      <td>0.053700</td>\n",
       "      <td>15.838200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.495600</td>\n",
       "      <td>4.554310</td>\n",
       "      <td>0.052500</td>\n",
       "      <td>15.847700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.490100</td>\n",
       "      <td>4.563453</td>\n",
       "      <td>0.053200</td>\n",
       "      <td>15.857200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.486700</td>\n",
       "      <td>4.578422</td>\n",
       "      <td>0.053000</td>\n",
       "      <td>15.775000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.480300</td>\n",
       "      <td>4.589092</td>\n",
       "      <td>0.053500</td>\n",
       "      <td>15.814400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>0.475500</td>\n",
       "      <td>4.601227</td>\n",
       "      <td>0.054000</td>\n",
       "      <td>15.802900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.468900</td>\n",
       "      <td>4.611670</td>\n",
       "      <td>0.054100</td>\n",
       "      <td>15.812400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>0.466400</td>\n",
       "      <td>4.623120</td>\n",
       "      <td>0.052900</td>\n",
       "      <td>15.775700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>0.461100</td>\n",
       "      <td>4.634084</td>\n",
       "      <td>0.052300</td>\n",
       "      <td>15.891900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.456300</td>\n",
       "      <td>4.646074</td>\n",
       "      <td>0.052100</td>\n",
       "      <td>15.927300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.452600</td>\n",
       "      <td>4.656498</td>\n",
       "      <td>0.052100</td>\n",
       "      <td>15.896700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>0.448700</td>\n",
       "      <td>4.667428</td>\n",
       "      <td>0.053700</td>\n",
       "      <td>15.716500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>0.445300</td>\n",
       "      <td>4.679980</td>\n",
       "      <td>0.056200</td>\n",
       "      <td>15.854500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>0.439600</td>\n",
       "      <td>4.689167</td>\n",
       "      <td>0.054100</td>\n",
       "      <td>15.913000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.436000</td>\n",
       "      <td>4.696681</td>\n",
       "      <td>0.054000</td>\n",
       "      <td>15.901400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>0.429400</td>\n",
       "      <td>4.707317</td>\n",
       "      <td>0.053800</td>\n",
       "      <td>16.006800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>0.425500</td>\n",
       "      <td>4.720235</td>\n",
       "      <td>0.054900</td>\n",
       "      <td>15.952400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>0.423200</td>\n",
       "      <td>4.728521</td>\n",
       "      <td>0.054200</td>\n",
       "      <td>15.973500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment tappelles-tu? ['Tu portes un nom de fantaisie.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 48, 'bad_words_ids': [[59421]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment tappelles-tu? ['Tu portes un nom de fantaisie.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 48, 'bad_words_ids': [[59421]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment tappelles-tu? ['Tu portes un nom de fantaisie.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 48, 'bad_words_ids': [[59421]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment tappelles-tu? ['Tu portes un nom de fantaisie.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 48, 'bad_words_ids': [[59421]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment tappelles-tu? ['Tu portes un nom de fantaisie.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 48, 'bad_words_ids': [[59421]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment tappelles-tu? ['Tu portes un nom de fantaisie.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 48, 'bad_words_ids': [[59421]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment tappelles-tu? ['Tu portes un nom de fantaisie.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 48, 'bad_words_ids': [[59421]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment tappelles-tu? ['Tu portes un nom de fantaisie.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 48, 'bad_words_ids': [[59421]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment tappelles-tu? ['Tu portes un nom de fantaisie.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 48, 'bad_words_ids': [[59421]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment tappelles-tu? ['Tu portes un nom de fantaisie.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 48, 'bad_words_ids': [[59421]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment tappelles-tu? ['Tu portes un nom de fantaisie.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 48, 'bad_words_ids': [[59421]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment tappelles-tu? ['Tu portes un nom de fantaisie.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 48, 'bad_words_ids': [[59421]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment tappelles-tu? ['Tu portes un nom de fantaisie.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 48, 'bad_words_ids': [[59421]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment tappelles-tu? ['Tu portes un nom de fantaisie.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 48, 'bad_words_ids': [[59421]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment tappelles-tu? ['Tu portes un nom de fantaisie.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 48, 'bad_words_ids': [[59421]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment tappelles-tu? ['Tu portes un nom de fantaisie.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 48, 'bad_words_ids': [[59421]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment tappelles-tu? ['Tu portes un nom de fantaisie.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 48, 'bad_words_ids': [[59421]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment tappelles-tu? ['Tu portes un nom de fantaisie.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 48, 'bad_words_ids': [[59421]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment tappelles-tu? ['Tu portes un nom de fantaisie.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 48, 'bad_words_ids': [[59421]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment tappelles-tu? ['Tu portes un nom de fantaisie.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 48, 'bad_words_ids': [[59421]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment tappelles-tu? ['Tu portes un nom de fantaisie.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 48, 'bad_words_ids': [[59421]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment tappelles-tu? ['Tu portes un nom de fantaisie.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 48, 'bad_words_ids': [[59421]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment tappelles-tu? ['Tu portes un nom de fantaisie.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 48, 'bad_words_ids': [[59421]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment tappelles-tu? ['Tu portes un nom de fantaisie.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 48, 'bad_words_ids': [[59421]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment tappelles-tu? ['Tu portes un nom de fantaisie.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 48, 'bad_words_ids': [[59421]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment tappelles-tu? ['Tu portes un nom de fantaisie.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 48, 'bad_words_ids': [[59421]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment tappelles-tu? ['Tu portes un nom de fantaisie.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 48, 'bad_words_ids': [[59421]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment tappelles-tu? ['Tu portes un nom de fantaisie.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 48, 'bad_words_ids': [[59421]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment tappelles-tu? ['Tu portes un nom de fantaisie.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 48, 'bad_words_ids': [[59421]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment tappelles-tu? ['Tu portes un nom de fantaisie.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 48, 'bad_words_ids': [[59421]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment tappelles-tu? ['Tu portes un nom de fantaisie.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 48, 'bad_words_ids': [[59421]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment tappelles-tu? ['Tu portes un nom de fantaisie.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 48, 'bad_words_ids': [[59421]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment tappelles-tu? ['Tu portes un nom de fantaisie.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 48, 'bad_words_ids': [[59421]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment tappelles-tu? ['Tu portes un nom de fantaisie.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 48, 'bad_words_ids': [[59421]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment tappelles-tu? ['Tu portes un nom de fantaisie.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 48, 'bad_words_ids': [[59421]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment tappelles-tu? ['Tu portes un nom de fantaisie.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 48, 'bad_words_ids': [[59421]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment tappelles-tu? ['Tu portes un nom de fantaisie.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 48, 'bad_words_ids': [[59421]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment tappelles-tu? ['Tu portes un nom de fantaisie.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 48, 'bad_words_ids': [[59421]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment tappelles-tu? ['Tu portes un nom de fantaisie.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 48, 'bad_words_ids': [[59421]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment tappelles-tu? ['Tu portes un nom de fantaisie.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 48, 'bad_words_ids': [[59421]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment tappelles-tu? ['Tu portes un nom de fantaisie.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 48, 'bad_words_ids': [[59421]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment tappelles-tu? ['Tu portes un nom de fantaisie.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 48, 'bad_words_ids': [[59421]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment tappelles-tu? ['Tu portes un nom de fantaisie.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 48, 'bad_words_ids': [[59421]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 46\u001b[0m\n\u001b[1;32m      2\u001b[0m training_args \u001b[38;5;241m=\u001b[39m Seq2SeqTrainingArguments(\n\u001b[1;32m      3\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels/marian/marian_output\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m     eval_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msteps\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m     run_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMTest11\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     34\u001b[0m )\n\u001b[1;32m     36\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Seq2SeqTrainer(\n\u001b[1;32m     37\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     38\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     43\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[1;32m     44\u001b[0m )\n\u001b[0;32m---> 46\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/zindi/transformers/src/transformers/trainer.py:1938\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1936\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1937\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1938\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1939\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1940\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1941\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1942\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1943\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/zindi/transformers/src/transformers/trainer.py:2279\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2276\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2278\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2279\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2282\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2283\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2284\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2285\u001b[0m ):\n\u001b[1;32m   2286\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2287\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/zindi/transformers/src/transformers/trainer.py:3349\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3347\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   3348\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3349\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3351\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/zindi-LtLKIbXv-py3.9/lib/python3.9/site-packages/accelerate/accelerator.py:2159\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2157\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[1;32m   2158\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2159\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/zindi-LtLKIbXv-py3.9/lib/python3.9/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/zindi-LtLKIbXv-py3.9/lib/python3.9/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/zindi-LtLKIbXv-py3.9/lib/python3.9/site-packages/torch/autograd/graph.py:768\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    766\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"models/marian/marian_output\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps = 20,\n",
    "    learning_rate=5e-4,\n",
    "    per_device_train_batch_size=100,\n",
    "    per_device_eval_batch_size=100,\n",
    "    # weight_decay=0.01,\n",
    "    num_train_epochs=200000,\n",
    "    predict_with_generate=True,\n",
    "    # fp16=True,\n",
    "    # push_to_hub=False,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    gradient_accumulation_steps=20,\n",
    "    logging_dir= \"models/marian/marian_output/logs\",\n",
    "    logging_steps = 20,\n",
    "    save_strategy = 'steps',\n",
    "    save_steps = 20,\n",
    "    save_total_limit = 10,\n",
    "    seed = 42,\n",
    "    dataloader_drop_last = False,\n",
    "    # label_smoothing_factor: float = 0.0,\n",
    "    optim = 'adafactor',\n",
    "    # resume_from_checkpoint: Optional[str] = None,\n",
    "    # fp16_backend: str = 'auto',\n",
    "    # batch_eval_metrics: bool = False,\n",
    "    # eval_on_start=True,\n",
    "    # generation_max_length= 50,\n",
    "    # generation_num_beams=1,\n",
    "    generation_config = \"models/marian/marian_output/base_model/generation_config.json\",\n",
    "    run_name=\"MTest12\"\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_zds[\"train\"],\n",
    "    eval_dataset=tokenized_zds[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ['dataset_name'] = \"uvci/Koumankan_mt_dyu_fr\"\n",
    "# os.environ['generation_config'] = os.environ['model_name_or_path']+'/generation_config.json'\n",
    "# os.environ['source_lang']=\"dyu_Latn\"\n",
    "# os.environ['target_lang']=\"fra_Latn\"\n",
    "# os.environ['dataset_config_name']= \"default\"\n",
    "# os.environ['per_device_eval_batch_size']=\"1024\"\n",
    "# os.environ['per_device_train_batch_size']=\"1024\"\n",
    "# # use_cpu=False\n",
    "# os.environ['save_steps']=\"100\"\n",
    "# os.environ['eval_steps']=\"100\"\n",
    "# os.environ['num_train_epochs']=\"10000\"\n",
    "# os.environ['logging_steps']=\"10\"\n",
    "# os.environ['save_total_limit']=\"10\"\n",
    "# os.environ['overwrite_output_dir']=\"True\"\n",
    "# os.environ['run_name']=\"marian-1\"\n",
    "# os.environ['output_dir']=\"models/marian/marian_output\"\n",
    "# os.environ['logging_dir']=\"models/marian/logs\"\n",
    "# os.environ['predict_with_generate']=\"True\"\n",
    "# os.environ['dataloader_drop_last']=\"True\"\n",
    "# os.environ['jit_mode_eval']=\"False\"\n",
    "# # os.environ['do_eval']=\"True\"\n",
    "# os.environ['do_predict']=\"False\"\n",
    "# os.environ['do_train']=\"True\"\n",
    "\n",
    "# ### Config\n",
    "# os.environ['label_smoothing_factor']=\"0.00001\"\n",
    "# os.environ['learning_rate']=\"5e-04\"\n",
    "# os.environ['gradient_accumulation_steps']=\"4\"\n",
    "# os.environ['generation_max_length']=\"128\"\n",
    "# os.environ['generation_num_beams']=\"2\"\n",
    "# os.environ['max_source_length']= \"128\"\n",
    "# os.environ['warmup_steps']=\"10\"\n",
    "# os.environ['weight_decay']=\"0.00001\"\n",
    "# os.environ['seed']=\"42\"\n",
    "# os.environ['fp16']=\"False\"\n",
    "# os.environ['fp16_backend']=\"auto\"\n",
    "# os.environ['fp16_full_eval']=\"False\"\n",
    "# os.environ['full_determinism']=\"True\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/17/2024 16:11:17 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "08/17/2024 16:11:17 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n",
      "_n_gpu=1,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=True,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_on_start=False,\n",
      "eval_steps=100,\n",
      "eval_strategy=steps,\n",
      "eval_use_gather_object=False,\n",
      "evaluation_strategy=None,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=True,\n",
      "generation_config=models/marian/marian_output/base_model/generation_config.json,\n",
      "generation_max_length=128,\n",
      "generation_num_beams=2,\n",
      "gradient_accumulation_steps=4,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=1e-05,\n",
      "learning_rate=0.0005,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=models/marian/logs,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=10,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=10000.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=models/marian/marian_output,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=720,\n",
      "per_device_train_batch_size=720,\n",
      "predict_with_generate=True,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=marian-1,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=100,\n",
      "save_strategy=steps,\n",
      "save_total_limit=10,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "sortish_sampler=False,\n",
      "split_batches=None,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torch_empty_cache_steps=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=10,\n",
      "weight_decay=1e-05,\n",
      ")\n",
      "Overwrite dataset info from restored data version if exists.\n",
      "08/17/2024 16:11:17 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
      "Loading Dataset info from /root/.cache/huggingface/datasets/uvci___koumankan_mt_dyu_fr/default/0.0.0/5207d8690e8c37ec07a1954dc7441c348f8242fa\n",
      "08/17/2024 16:11:17 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/uvci___koumankan_mt_dyu_fr/default/0.0.0/5207d8690e8c37ec07a1954dc7441c348f8242fa\n",
      "Found cached dataset koumankan_mt_dyu_fr (/root/.cache/huggingface/datasets/uvci___koumankan_mt_dyu_fr/default/0.0.0/5207d8690e8c37ec07a1954dc7441c348f8242fa)\n",
      "08/17/2024 16:11:17 - INFO - datasets.builder - Found cached dataset koumankan_mt_dyu_fr (/root/.cache/huggingface/datasets/uvci___koumankan_mt_dyu_fr/default/0.0.0/5207d8690e8c37ec07a1954dc7441c348f8242fa)\n",
      "Loading Dataset info from /root/.cache/huggingface/datasets/uvci___koumankan_mt_dyu_fr/default/0.0.0/5207d8690e8c37ec07a1954dc7441c348f8242fa\n",
      "08/17/2024 16:11:17 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/uvci___koumankan_mt_dyu_fr/default/0.0.0/5207d8690e8c37ec07a1954dc7441c348f8242fa\n",
      "[INFO|configuration_utils.py:731] 2024-08-17 16:11:17,911 >> loading configuration file models/marian/marian_output/base_model/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-08-17 16:11:17,915 >> Model config MarianConfig {\n",
      "  \"_name_or_path\": \"models/marian/marian_output/base_model\",\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.2,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.2,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      31999\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classif_dropout\": 0.2,\n",
      "  \"classifier_dropout\": 0.2,\n",
      "  \"d_model\": 64,\n",
      "  \"decoder_attention_heads\": 4,\n",
      "  \"decoder_ffn_dim\": 1024,\n",
      "  \"decoder_layerdrop\": 0.2,\n",
      "  \"decoder_layers\": 4,\n",
      "  \"decoder_start_token_id\": 31999,\n",
      "  \"decoder_vocab_size\": 32000,\n",
      "  \"dropout\": 0.5,\n",
      "  \"encoder_attention_heads\": 4,\n",
      "  \"encoder_ffn_dim\": 1024,\n",
      "  \"encoder_layerdrop\": 0.2,\n",
      "  \"encoder_layers\": 4,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 128,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 2,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 31999,\n",
      "  \"scale_embedding\": true,\n",
      "  \"share_encoder_decoder_embeddings\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.45.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2267] 2024-08-17 16:11:17,916 >> loading file source.spm\n",
      "[INFO|tokenization_utils_base.py:2267] 2024-08-17 16:11:17,916 >> loading file target.spm\n",
      "[INFO|tokenization_utils_base.py:2267] 2024-08-17 16:11:17,916 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:2267] 2024-08-17 16:11:17,916 >> loading file target_vocab.json\n",
      "[INFO|tokenization_utils_base.py:2267] 2024-08-17 16:11:17,916 >> loading file tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2267] 2024-08-17 16:11:17,916 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2267] 2024-08-17 16:11:17,916 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2267] 2024-08-17 16:11:17,916 >> loading file tokenizer.json\n",
      "[INFO|modeling_utils.py:3676] 2024-08-17 16:11:18,315 >> loading weights file models/marian/marian_output/base_model/model.safetensors\n",
      "[INFO|configuration_utils.py:1038] 2024-08-17 16:11:18,320 >> Generate config GenerationConfig {\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      31999\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 1,\n",
      "  \"decoder_start_token_id\": 31999,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"num_beams\": 2,\n",
      "  \"pad_token_id\": 31999\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:4508] 2024-08-17 16:11:18,470 >> All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "[INFO|modeling_utils.py:4516] 2024-08-17 16:11:18,470 >> All the weights of MarianMTModel were initialized from the model checkpoint at models/marian/marian_output/base_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "[INFO|configuration_utils.py:991] 2024-08-17 16:11:18,471 >> loading configuration file models/marian/marian_output/base_model/generation_config.json\n",
      "[INFO|configuration_utils.py:1038] 2024-08-17 16:11:18,471 >> Generate config GenerationConfig {\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      31999\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 1,\n",
      "  \"decoder_start_token_id\": 31999,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"num_beams\": 2,\n",
      "  \"pad_token_id\": 31999\n",
      "}\n",
      "\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/uvci___koumankan_mt_dyu_fr/default/0.0.0/5207d8690e8c37ec07a1954dc7441c348f8242fa/cache-6a6945058669b6c0.arrow\n",
      "08/17/2024 16:11:18 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/uvci___koumankan_mt_dyu_fr/default/0.0.0/5207d8690e8c37ec07a1954dc7441c348f8242fa/cache-6a6945058669b6c0.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/uvci___koumankan_mt_dyu_fr/default/0.0.0/5207d8690e8c37ec07a1954dc7441c348f8242fa/cache-7207420cab42d583.arrow\n",
      "08/17/2024 16:11:19 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/uvci___koumankan_mt_dyu_fr/default/0.0.0/5207d8690e8c37ec07a1954dc7441c348f8242fa/cache-7207420cab42d583.arrow\n",
      "[INFO|configuration_utils.py:991] 2024-08-17 16:11:19,535 >> loading configuration file models/marian/marian_output/base_model/generation_config.json\n",
      "[INFO|configuration_utils.py:1038] 2024-08-17 16:11:19,536 >> Generate config GenerationConfig {\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      31999\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 1,\n",
      "  \"decoder_start_token_id\": 31999,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"num_beams\": 2,\n",
      "  \"pad_token_id\": 31999\n",
      "}\n",
      "\n",
      "[INFO|trainer.py:2150] 2024-08-17 16:11:20,090 >> ***** Running training *****\n",
      "[INFO|trainer.py:2151] 2024-08-17 16:11:20,090 >>   Num examples = 8,065\n",
      "[INFO|trainer.py:2152] 2024-08-17 16:11:20,090 >>   Num Epochs = 10,000\n",
      "[INFO|trainer.py:2153] 2024-08-17 16:11:20,090 >>   Instantaneous batch size per device = 720\n",
      "[INFO|trainer.py:2156] 2024-08-17 16:11:20,090 >>   Total train batch size (w. parallel, distributed & accumulation) = 2,880\n",
      "[INFO|trainer.py:2157] 2024-08-17 16:11:20,090 >>   Gradient Accumulation steps = 4\n",
      "[INFO|trainer.py:2158] 2024-08-17 16:11:20,090 >>   Total optimization steps = 20,000\n",
      "[INFO|trainer.py:2159] 2024-08-17 16:11:20,090 >>   Number of trainable parameters = 3,307,520\n",
      "{'loss': 10.3432, 'grad_norm': 0.6874338388442993, 'learning_rate': 0.0005, 'epoch': 3.64}\n",
      "{'loss': 9.9917, 'grad_norm': 0.9947864413261414, 'learning_rate': 0.0004997498749374688, 'epoch': 7.27}\n",
      "{'loss': 9.537, 'grad_norm': 1.0433158874511719, 'learning_rate': 0.0004994997498749375, 'epoch': 10.91}\n",
      "{'loss': 9.069, 'grad_norm': 1.0841349363327026, 'learning_rate': 0.0004992496248124062, 'epoch': 14.55}\n",
      "{'loss': 8.6439, 'grad_norm': 1.063642144203186, 'learning_rate': 0.0004989994997498749, 'epoch': 18.18}\n",
      "  0%|                                      | 53/20000 [00:47<4:57:00,  1.12it/s]"
     ]
    }
   ],
   "source": [
    "# !python transformers/examples/pytorch/translation/run_translation.py \\\n",
    "# --per_device_train_batch_size $$per_device_train_batch_size \\\n",
    "# --per_device_eval_batch_size $$per_device_eval_batch_size \\\n",
    "# --save_steps $$save_steps \\\n",
    "# --num_train_epochs $$num_train_epochs \\\n",
    "# --logging_steps $$logging_steps \\\n",
    "# --label_smoothing_factor $$label_smoothing_factor \\\n",
    "# --learning_rate $$learning_rate \\\n",
    "# --run_name $$run_name \\\n",
    "# --output_dir $$output_dir \\\n",
    "# --logging_dir $$logging_dir \\\n",
    "# --eval_steps $$eval_steps \\\n",
    "# --gradient_accumulation_steps $$gradient_accumulation_steps \\\n",
    "# --model_name_or_path  $$model_name_or_path  \\\n",
    "# --dataset_name  $$dataset_name  \\\n",
    "# --generation_max_length $$generation_max_length \\\n",
    "# --generation_num_beams $$generation_num_beams \\\n",
    "# --source_lang $$source_lang \\\n",
    "# --target_lang $$target_lang \\\n",
    "# --dataset_config_name $$dataset_config_name \\\n",
    "# --predict_with_generate $$predict_with_generate \\\n",
    "# --max_source_length $$max_source_length \\\n",
    "# --dataloader_drop_last $$dataloader_drop_last \\\n",
    "# --warmup_steps $$warmup_steps \\\n",
    "# --weight_decay $$weight_decay \\\n",
    "# --save_total_limit $$save_total_limit \\\n",
    "# --seed $$seed \\\n",
    "# --overwrite_output_dir $$overwrite_output_dir \\\n",
    "# --jit_mode_eval $$jit_mode_eval \\\n",
    "# --do_train $$do_train \\\n",
    "# --fp16 $$fp16 \\\n",
    "# --fp16_backend $$fp16_backend \\\n",
    "# --fp16_full_eval $$fp16_full_eval \\\n",
    "# --full_determinism $$full_determinism \\\n",
    "# --predict_with_generate true \\\n",
    "# --do_eval $$do_eval \\\n",
    "# --do_predict $$do_predict \\\n",
    "# --eval_strategy steps \\\n",
    "# --generation_config $$generation_config \n",
    "# # --resume_from_checkpoint {resume_from_checkpoint} \n",
    "# # --use_cpu {use_cpu} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zindi-z3yfXQo9-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
