{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.cache/pypoetry/virtualenvs/zindi-LtLKIbXv-py3.9/lib/python3.9/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/zindi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.cache/pypoetry/virtualenvs/zindi-LtLKIbXv-py3.9/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# %cd /home/rana/Projects/zindi\n",
    "%cd /root/zindi/\n",
    "import yaml\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "with open('common/config.yaml', 'r') as f:\n",
    "    config = yaml.load(f, Loader=yaml.SafeLoader)\n",
    "\n",
    "def delete_file_if_exists(file_path):\n",
    "    if os.path.exists(file_path):\n",
    "        os.remove(file_path)\n",
    "        print(f\"File {file_path} has been deleted.\")\n",
    "    else:\n",
    "        print(f\"File {file_path} does not exist.\")\n",
    "\n",
    "\n",
    "def copy_file_or_directory(source, destination):\n",
    "    try:\n",
    "        if os.path.isfile(source):\n",
    "            # If source is a file, copy it directly\n",
    "            shutil.copy2(source, destination)\n",
    "            print(f\"File copied successfully from {source} to {destination}\")\n",
    "        elif os.path.isdir(source):\n",
    "            # If source is a directory, copy all files within it\n",
    "            if not os.path.exists(destination):\n",
    "                os.makedirs(destination)\n",
    "            for item in os.listdir(source):\n",
    "                s = os.path.join(source, item)\n",
    "                d = os.path.join(destination, item)\n",
    "                if os.path.isfile(s):\n",
    "                    shutil.copy2(s, d)\n",
    "                    print(f\"File copied successfully from {s} to {d}\")\n",
    "        else:\n",
    "            print(f\"Source {source} is neither a file nor a directory.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Source not found: {source}\")\n",
    "    except PermissionError:\n",
    "        print(\"Permission denied. Check file permissions.\")\n",
    "    except shutil.SameFileError:\n",
    "        print(\"Source and destination are the same file.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "def create_or_clean_directory(path):\n",
    "    if os.path.exists(path):\n",
    "        # Path exists, so clean it\n",
    "        for item in os.listdir(path):\n",
    "            item_path = os.path.join(path, item)\n",
    "            if os.path.isfile(item_path):\n",
    "                os.unlink(item_path)\n",
    "            elif os.path.isdir(item_path):\n",
    "                shutil.rmtree(item_path)\n",
    "        print(f\"Cleaned existing directory: {path}\")\n",
    "    else:\n",
    "        # Path doesn't exist, so create it\n",
    "        os.makedirs(path)\n",
    "        print(f\"Created new directory: {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#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\n",
    "# !huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Device specific params\n",
    "os.environ['model_name_or_path'] = \"models/marian/marian_output/base_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created new directory: models/marian/marian_output/base_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59421]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File models/marian/marian_output/base_model/generation_config.json has been deleted.\n",
      "File copied successfully from tokenizer_custom/combined_pad_V1/source.spm to models/marian/marian_output/base_model/source.spm\n",
      "File copied successfully from tokenizer_custom/combined_pad_V1/special_tokens_map.json to models/marian/marian_output/base_model/special_tokens_map.json\n",
      "File copied successfully from tokenizer_custom/combined_pad_V1/target.spm to models/marian/marian_output/base_model/target.spm\n",
      "File copied successfully from tokenizer_custom/combined_pad_V1/tokenizer_config.json to models/marian/marian_output/base_model/tokenizer_config.json\n",
      "File copied successfully from tokenizer_custom/combined_pad_V1/vocab.json to models/marian/marian_output/base_model/vocab.json\n"
     ]
    }
   ],
   "source": [
    "first = True\n",
    "# first = False\n",
    "\n",
    "# base_model_path = \"models/marian/marian_output/base_model\"\n",
    "custom_tokenizer=\"tokenizer_custom/combined_pad_V1\"\n",
    "\n",
    "create_or_clean_directory(os.environ['model_name_or_path'])\n",
    "if first:\n",
    "    # Download the model\n",
    "    # Load model:\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(config.get('model_name_or_path'))\n",
    "    model.save_pretrained(os.environ['model_name_or_path'])\n",
    "    delete_file_if_exists(os.environ['model_name_or_path']+'/generation_config.json')\n",
    "    copy_file_or_directory(custom_tokenizer, os.environ['model_name_or_path'])\n",
    "else:\n",
    "    # Copy checkpoint to base model path\n",
    "    copy_file_or_directory(config.get('model_name_or_path'), os.environ['model_name_or_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MarianMTModel were not initialized from the model checkpoint at models/marian/marian_output/base_model and are newly initialized because the shapes did not match:\n",
      "- final_logits_bias: found shape torch.Size([1, 59422]) in the checkpoint and torch.Size([1, 32000]) in the model instantiated\n",
      "- model.shared.weight: found shape torch.Size([59422, 512]) in the checkpoint and torch.Size([32000, 512]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 128, 'num_beams': 2, 'bad_words_ids': [[31999]], 'forced_eos_token_id': 2}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "378"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "# Modify the model config\n",
    "config_path = os.environ['model_name_or_path']+'/config.json'\n",
    "with open(config_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "    # data['decoder_attention_heads']=4\n",
    "    # data['decoder_ffn_dim']=1024\n",
    "    # data['decoder_layers']=6\n",
    "    data['dropout']=0.3\n",
    "    # data['encoder_attention_heads']=4\n",
    "    # data['encoder_ffn_dim']=1024\n",
    "    # data['encoder_layers']=6\n",
    "    # data['max_position_embeddings']=512\n",
    "    # data['num_hidden_layers']=6\n",
    "    # data['torch_dtype']=\"float32\"\n",
    "\n",
    "    data['max_length']=128\n",
    "    data['num_beams']=2\n",
    "    data[\"bos_token_id\"]= 1\n",
    "    data[\"eos_token_id\"]= 2\n",
    "    data[\"forced_eos_token_id\"]= 2\n",
    "    data[\"bad_words_ids\"]= [[31999]]\n",
    "    data[\"decoder_start_token_id\"]= 31999\n",
    "    data['decoder_vocab_size']=32000\n",
    "    data[\"pad_token_id\"]= 31999\n",
    "    data['vocab_size']=32000\n",
    "\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(data, f, indent=2)    \n",
    "\n",
    "# Load model with updated config and save it\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(os.environ['model_name_or_path'], ignore_mismatched_sizes=True)\n",
    "# Save model\n",
    "model.save_pretrained(os.environ['model_name_or_path'])\n",
    "\n",
    "del model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['dataset_name'] = \"uvci/Koumankan_mt_dyu_fr\"\n",
    "os.environ['generation_config'] = os.environ['model_name_or_path']+'/generation_config.json'\n",
    "os.environ['source_lang']=\"dyu\"\n",
    "os.environ['target_lang']=\"fr\"\n",
    "os.environ['dataset_config_name']= \"default\"\n",
    "os.environ['per_device_eval_batch_size']=\"480\"\n",
    "os.environ['per_device_train_batch_size']=\"480\"\n",
    "# use_cpu=False\n",
    "os.environ['save_steps']=\"200\"\n",
    "os.environ['eval_steps']=\"200\"\n",
    "os.environ['num_train_epochs']=\"10000\"\n",
    "os.environ['logging_steps']=\"10\"\n",
    "os.environ['save_total_limit']=\"10\"\n",
    "os.environ['overwrite_output_dir']=\"True\"\n",
    "os.environ['run_name']=\"marian-1\"\n",
    "os.environ['output_dir']=\"models/marian/marian_output\"\n",
    "os.environ['logging_dir']=\"models/marian/logs\"\n",
    "os.environ['predict_with_generate']=\"True\"\n",
    "os.environ['dataloader_drop_last']=\"True\"\n",
    "os.environ['jit_mode_eval']=\"False\"\n",
    "# os.environ['do_eval']=\"True\"\n",
    "os.environ['do_predict']=\"False\"\n",
    "os.environ['do_train']=\"True\"\n",
    "\n",
    "### Config\n",
    "os.environ['label_smoothing_factor']=\"0.00001\"\n",
    "os.environ['learning_rate']=\"1e-04\"\n",
    "os.environ['gradient_accumulation_steps']=\"4\"\n",
    "os.environ['generation_max_length']=\"128\"\n",
    "os.environ['generation_num_beams']=\"2\"\n",
    "os.environ['max_source_length']= \"128\"\n",
    "os.environ['warmup_steps']=\"10\"\n",
    "os.environ['weight_decay']=\"0.00001\"\n",
    "os.environ['seed']=\"42\"\n",
    "os.environ['fp16']=\"False\"\n",
    "os.environ['fp16_backend']=\"auto\"\n",
    "os.environ['fp16_full_eval']=\"False\"\n",
    "os.environ['full_determinism']=\"True\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import GenerationConfig\n",
    "\n",
    "# # Create a custom generation config\n",
    "# custom_gen_config = GenerationConfig(\n",
    "#     bad_words_ids=[[31999]],\n",
    "#     bos_token_id=1,\n",
    "#     decoder_start_token_id=31999,\n",
    "#     eos_token_id=2,\n",
    "#     forced_eos_token_id=2,\n",
    "#     pad_token_id=31999,\n",
    "#     num_beams=1,\n",
    "#     max_length=128\n",
    "#     # Add any other parameters you want to override\n",
    "# )\n",
    "\n",
    "# custom_gen_config.save_pretrained(base_model_path, \"generation_config.json\")\n",
    "# os.environ['generation_config']=base_model_path+\"/generation_config.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/16/2024 05:24:46 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "08/16/2024 05:24:46 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n",
      "_n_gpu=1,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=True,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_on_start=False,\n",
      "eval_steps=100,\n",
      "eval_strategy=steps,\n",
      "eval_use_gather_object=False,\n",
      "evaluation_strategy=None,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=True,\n",
      "generation_config=None,\n",
      "generation_max_length=128,\n",
      "generation_num_beams=2,\n",
      "gradient_accumulation_steps=4,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=1e-05,\n",
      "learning_rate=0.0001,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=models/marian/logs,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=10,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=10000.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=models/marian/marian_output,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=480,\n",
      "predict_with_generate=True,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=marian-1,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=100,\n",
      "save_strategy=steps,\n",
      "save_total_limit=10,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "sortish_sampler=False,\n",
      "split_batches=None,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torch_empty_cache_steps=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=10,\n",
      "weight_decay=1e-05,\n",
      ")\n",
      "Overwrite dataset info from restored data version if exists.\n",
      "08/16/2024 05:24:48 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
      "Loading Dataset info from /root/.cache/huggingface/datasets/uvci___koumankan_mt_dyu_fr/default/0.0.0/5207d8690e8c37ec07a1954dc7441c348f8242fa\n",
      "08/16/2024 05:24:48 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/uvci___koumankan_mt_dyu_fr/default/0.0.0/5207d8690e8c37ec07a1954dc7441c348f8242fa\n",
      "Found cached dataset koumankan_mt_dyu_fr (/root/.cache/huggingface/datasets/uvci___koumankan_mt_dyu_fr/default/0.0.0/5207d8690e8c37ec07a1954dc7441c348f8242fa)\n",
      "08/16/2024 05:24:48 - INFO - datasets.builder - Found cached dataset koumankan_mt_dyu_fr (/root/.cache/huggingface/datasets/uvci___koumankan_mt_dyu_fr/default/0.0.0/5207d8690e8c37ec07a1954dc7441c348f8242fa)\n",
      "Loading Dataset info from /root/.cache/huggingface/datasets/uvci___koumankan_mt_dyu_fr/default/0.0.0/5207d8690e8c37ec07a1954dc7441c348f8242fa\n",
      "08/16/2024 05:24:48 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/uvci___koumankan_mt_dyu_fr/default/0.0.0/5207d8690e8c37ec07a1954dc7441c348f8242fa\n",
      "[INFO|configuration_utils.py:731] 2024-08-16 05:24:48,910 >> loading configuration file models/marian/marian_output/base_model/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-08-16 05:24:48,916 >> Model config MarianConfig {\n",
      "  \"_name_or_path\": \"models/marian/marian_output/base_model\",\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      31999\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 31999,\n",
      "  \"decoder_vocab_size\": 32000,\n",
      "  \"dropout\": 0.3,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 128,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 2,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 31999,\n",
      "  \"scale_embedding\": true,\n",
      "  \"share_encoder_decoder_embeddings\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.45.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2267] 2024-08-16 05:24:48,918 >> loading file source.spm\n",
      "[INFO|tokenization_utils_base.py:2267] 2024-08-16 05:24:48,918 >> loading file target.spm\n",
      "[INFO|tokenization_utils_base.py:2267] 2024-08-16 05:24:48,918 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:2267] 2024-08-16 05:24:48,918 >> loading file target_vocab.json\n",
      "[INFO|tokenization_utils_base.py:2267] 2024-08-16 05:24:48,918 >> loading file tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2267] 2024-08-16 05:24:48,918 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2267] 2024-08-16 05:24:48,918 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2267] 2024-08-16 05:24:48,918 >> loading file tokenizer.json\n",
      "[INFO|modeling_utils.py:3654] 2024-08-16 05:24:49,684 >> loading weights file models/marian/marian_output/base_model/model.safetensors\n",
      "[INFO|configuration_utils.py:1038] 2024-08-16 05:24:49,701 >> Generate config GenerationConfig {\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      31999\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 1,\n",
      "  \"decoder_start_token_id\": 31999,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"num_beams\": 2,\n",
      "  \"pad_token_id\": 31999\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:4489] 2024-08-16 05:24:51,766 >> All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "[INFO|modeling_utils.py:4497] 2024-08-16 05:24:51,766 >> All the weights of MarianMTModel were initialized from the model checkpoint at models/marian/marian_output/base_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "[INFO|configuration_utils.py:991] 2024-08-16 05:24:51,769 >> loading configuration file models/marian/marian_output/base_model/generation_config.json\n",
      "[INFO|configuration_utils.py:1038] 2024-08-16 05:24:51,769 >> Generate config GenerationConfig {\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      31999\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 1,\n",
      "  \"decoder_start_token_id\": 31999,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"num_beams\": 2,\n",
      "  \"pad_token_id\": 31999\n",
      "}\n",
      "\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/uvci___koumankan_mt_dyu_fr/default/0.0.0/5207d8690e8c37ec07a1954dc7441c348f8242fa/cache-a41a5e5f24ce3af3.arrow\n",
      "08/16/2024 05:24:52 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/uvci___koumankan_mt_dyu_fr/default/0.0.0/5207d8690e8c37ec07a1954dc7441c348f8242fa/cache-a41a5e5f24ce3af3.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/uvci___koumankan_mt_dyu_fr/default/0.0.0/5207d8690e8c37ec07a1954dc7441c348f8242fa/cache-d90c5ed89d411922.arrow\n",
      "08/16/2024 05:24:53 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/uvci___koumankan_mt_dyu_fr/default/0.0.0/5207d8690e8c37ec07a1954dc7441c348f8242fa/cache-d90c5ed89d411922.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/uvci___koumankan_mt_dyu_fr/default/0.0.0/5207d8690e8c37ec07a1954dc7441c348f8242fa/cache-8f2fd3fd6db6cf8f.arrow\n",
      "08/16/2024 05:24:54 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/uvci___koumankan_mt_dyu_fr/default/0.0.0/5207d8690e8c37ec07a1954dc7441c348f8242fa/cache-8f2fd3fd6db6cf8f.arrow\n",
      "[INFO|trainer.py:2160] 2024-08-16 05:24:55,998 >> ***** Running training *****\n",
      "[INFO|trainer.py:2161] 2024-08-16 05:24:55,998 >>   Num examples = 8,065\n",
      "[INFO|trainer.py:2162] 2024-08-16 05:24:55,998 >>   Num Epochs = 10,000\n",
      "[INFO|trainer.py:2163] 2024-08-16 05:24:55,999 >>   Instantaneous batch size per device = 480\n",
      "[INFO|trainer.py:2166] 2024-08-16 05:24:55,999 >>   Total train batch size (w. parallel, distributed & accumulation) = 1,920\n",
      "[INFO|trainer.py:2167] 2024-08-16 05:24:55,999 >>   Gradient Accumulation steps = 4\n",
      "[INFO|trainer.py:2168] 2024-08-16 05:24:55,999 >>   Total optimization steps = 40,000\n",
      "[INFO|trainer.py:2169] 2024-08-16 05:24:56,000 >>   Number of trainable parameters = 60,522,496\n",
      "{'loss': 10.8091, 'grad_norm': 7.197111129760742, 'learning_rate': 0.0001, 'epoch': 2.5}\n",
      "{'loss': 8.3484, 'grad_norm': 2.733583927154541, 'learning_rate': 9.997499374843712e-05, 'epoch': 5.0}\n",
      "{'loss': 7.0883, 'grad_norm': 1.5675228834152222, 'learning_rate': 9.994998749687422e-05, 'epoch': 7.5}\n",
      "{'loss': 6.5856, 'grad_norm': 1.0522485971450806, 'learning_rate': 9.992498124531134e-05, 'epoch': 10.0}\n",
      "{'loss': 6.4365, 'grad_norm': 2.60375714302063, 'learning_rate': 9.989997499374844e-05, 'epoch': 12.5}\n",
      "{'loss': 6.3395, 'grad_norm': 1.8658686876296997, 'learning_rate': 9.987496874218555e-05, 'epoch': 15.0}\n",
      "{'loss': 6.253, 'grad_norm': 1.2608591318130493, 'learning_rate': 9.984996249062266e-05, 'epoch': 17.5}\n",
      "{'loss': 6.1567, 'grad_norm': 1.2823671102523804, 'learning_rate': 9.982495623905978e-05, 'epoch': 20.0}\n",
      "{'loss': 6.0599, 'grad_norm': 1.27088463306427, 'learning_rate': 9.979994998749688e-05, 'epoch': 22.5}\n",
      "{'loss': 5.9874, 'grad_norm': 1.5477792024612427, 'learning_rate': 9.977494373593399e-05, 'epoch': 25.0}\n",
      "  0%|                                    | 100/40000 [03:58<26:30:08,  2.39s/it][INFO|trainer.py:3864] 2024-08-16 05:28:54,034 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3866] 2024-08-16 05:28:54,034 >>   Num examples = 1471\n",
      "[INFO|trainer.py:3869] 2024-08-16 05:28:54,034 >>   Batch size = 8\n",
      "[INFO|configuration_utils.py:1038] 2024-08-16 05:28:54,040 >> Generate config GenerationConfig {\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      31999\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 1,\n",
      "  \"decoder_start_token_id\": 31999,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"num_beams\": 2,\n",
      "  \"pad_token_id\": 31999\n",
      "}\n",
      "\n",
      "\n",
      "  0%|                                                   | 0/183 [00:00<?, ?it/s]\u001b[A\n",
      "  1%|▍                                          | 2/183 [00:02<03:53,  1.29s/it]\u001b[A\n",
      "  2%|▋                                          | 3/183 [00:04<05:13,  1.74s/it]\u001b[A\n",
      "  2%|▉                                          | 4/183 [00:07<06:08,  2.06s/it]\u001b[A\n",
      "  3%|█▏                                         | 5/183 [00:10<06:38,  2.24s/it]\u001b[A\n",
      "  3%|█▍                                         | 6/183 [00:12<06:57,  2.36s/it]\u001b[A\n",
      "  4%|█▋                                         | 7/183 [00:15<07:07,  2.43s/it]\u001b[A\n",
      "  4%|█▉                                         | 8/183 [00:17<07:14,  2.48s/it]\u001b[A\n",
      "  5%|██                                         | 9/183 [00:20<07:27,  2.57s/it]\u001b[A\n",
      "  5%|██▎                                       | 10/183 [00:23<07:36,  2.64s/it]\u001b[A\n",
      "  6%|██▌                                       | 11/183 [00:26<07:30,  2.62s/it]\u001b[A\n",
      "  7%|██▊                                       | 12/183 [00:28<07:34,  2.66s/it]\u001b[A\n",
      "  7%|██▉                                       | 13/183 [00:31<07:31,  2.66s/it]\u001b[A\n",
      "  8%|███▏                                      | 14/183 [00:33<07:19,  2.60s/it]\u001b[A\n",
      "  8%|███▍                                      | 15/183 [00:36<07:32,  2.70s/it]\u001b[A\n",
      "  9%|███▋                                      | 16/183 [00:39<07:40,  2.76s/it]\u001b[A\n",
      "  9%|███▉                                      | 17/183 [00:42<07:18,  2.64s/it]\u001b[A\n",
      " 10%|████▏                                     | 18/183 [00:44<07:11,  2.61s/it]\u001b[A\n",
      " 10%|████▎                                     | 19/183 [00:47<07:08,  2.61s/it]\u001b[A\n",
      " 11%|████▌                                     | 20/183 [00:49<07:01,  2.59s/it]\u001b[A\n",
      " 11%|████▊                                     | 21/183 [00:52<07:05,  2.62s/it]\u001b[A\n",
      " 12%|█████                                     | 22/183 [00:55<07:00,  2.61s/it]\u001b[A\n",
      " 13%|█████▎                                    | 23/183 [00:57<06:54,  2.59s/it]\u001b[A\n",
      " 13%|█████▌                                    | 24/183 [01:00<06:54,  2.60s/it]\u001b[A\n",
      " 14%|█████▋                                    | 25/183 [01:02<06:48,  2.59s/it]\u001b[A\n",
      " 14%|█████▉                                    | 26/183 [01:05<06:43,  2.57s/it]\u001b[A\n",
      " 15%|██████▏                                   | 27/183 [01:07<06:42,  2.58s/it]\u001b[A\n",
      " 15%|██████▍                                   | 28/183 [01:10<06:54,  2.68s/it]\u001b[A\n",
      " 16%|██████▋                                   | 29/183 [01:13<06:50,  2.66s/it]\u001b[A\n",
      " 16%|██████▉                                   | 30/183 [01:17<07:58,  3.12s/it]\u001b[A\n",
      " 17%|███████                                   | 31/183 [01:20<07:36,  3.00s/it]\u001b[A\n",
      " 17%|███████▎                                  | 32/183 [01:22<07:00,  2.79s/it]\u001b[A\n",
      " 18%|███████▌                                  | 33/183 [01:25<06:51,  2.74s/it]\u001b[A\n",
      " 19%|███████▊                                  | 34/183 [01:29<07:51,  3.17s/it]\u001b[A\n",
      " 19%|████████                                  | 35/183 [01:32<07:29,  3.03s/it]\u001b[A\n",
      " 20%|████████▎                                 | 36/183 [01:36<08:13,  3.36s/it]\u001b[A\n",
      " 20%|████████▍                                 | 37/183 [01:38<07:35,  3.12s/it]\u001b[A\n",
      " 21%|████████▋                                 | 38/183 [01:41<07:07,  2.95s/it]\u001b[A\n",
      " 21%|████████▉                                 | 39/183 [01:44<06:51,  2.86s/it]\u001b[A\n",
      " 22%|█████████▏                                | 40/183 [01:46<06:37,  2.78s/it]\u001b[A\n",
      " 22%|█████████▍                                | 41/183 [01:50<07:30,  3.17s/it]\u001b[A\n",
      " 23%|█████████▋                                | 42/183 [01:53<06:50,  2.91s/it]\u001b[A\n",
      " 23%|█████████▊                                | 43/183 [01:56<06:55,  2.97s/it]\u001b[A\n",
      " 24%|██████████                                | 44/183 [02:00<07:40,  3.31s/it]\u001b[A\n",
      " 25%|██████████▎                               | 45/183 [02:02<07:04,  3.07s/it]\u001b[A\n",
      " 25%|██████████▌                               | 46/183 [02:04<06:07,  2.69s/it]\u001b[A\n",
      " 26%|██████████▊                               | 47/183 [02:06<05:40,  2.50s/it]\u001b[A\n",
      " 26%|███████████                               | 48/183 [02:09<05:41,  2.53s/it]\u001b[A\n",
      " 27%|███████████▏                              | 49/183 [02:11<05:30,  2.47s/it]\u001b[A\n",
      " 27%|███████████▍                              | 50/183 [02:14<05:42,  2.58s/it]\u001b[A\n",
      " 28%|███████████▋                              | 51/183 [02:16<05:13,  2.37s/it]\u001b[A\n",
      " 28%|███████████▉                              | 52/183 [02:19<05:31,  2.53s/it]\u001b[A\n",
      " 29%|████████████▏                             | 53/183 [02:23<06:37,  3.06s/it]\u001b[A\n",
      " 30%|████████████▍                             | 54/183 [02:27<07:18,  3.40s/it]\u001b[A\n",
      " 30%|████████████▌                             | 55/183 [02:31<07:46,  3.65s/it]\u001b[A\n",
      " 31%|████████████▊                             | 56/183 [02:34<06:52,  3.25s/it]\u001b[A\n",
      " 31%|█████████████                             | 57/183 [02:36<06:23,  3.05s/it]\u001b[A\n",
      " 32%|█████████████▎                            | 58/183 [02:39<06:07,  2.94s/it]\u001b[A\n",
      " 32%|█████████████▌                            | 59/183 [02:41<05:40,  2.75s/it]\u001b[A\n",
      " 33%|█████████████▊                            | 60/183 [02:44<05:41,  2.77s/it]\u001b[A\n",
      " 33%|██████████████                            | 61/183 [02:47<05:47,  2.85s/it]\u001b[A\n",
      " 34%|██████████████▏                           | 62/183 [02:51<06:33,  3.25s/it]\u001b[A\n",
      " 34%|██████████████▍                           | 63/183 [02:56<07:05,  3.55s/it]\u001b[A\n",
      " 35%|██████████████▋                           | 64/183 [02:58<06:16,  3.17s/it]\u001b[A\n",
      " 36%|██████████████▉                           | 65/183 [03:02<06:48,  3.46s/it]\u001b[A\n",
      " 36%|███████████████▏                          | 66/183 [03:06<07:08,  3.66s/it]\u001b[A\n",
      " 37%|███████████████▍                          | 67/183 [03:10<07:21,  3.81s/it]\u001b[A\n",
      " 37%|███████████████▌                          | 68/183 [03:14<07:29,  3.91s/it]\u001b[A\n",
      " 38%|███████████████▊                          | 69/183 [03:18<06:57,  3.67s/it]\u001b[A\n",
      " 38%|████████████████                          | 70/183 [03:20<06:00,  3.19s/it]\u001b[A\n",
      " 39%|████████████████▎                         | 71/183 [03:22<05:19,  2.85s/it]\u001b[A\n",
      " 39%|████████████████▌                         | 72/183 [03:24<05:15,  2.84s/it]\u001b[A\n",
      " 40%|████████████████▊                         | 73/183 [03:27<05:04,  2.76s/it]\u001b[A\n",
      " 40%|████████████████▉                         | 74/183 [03:30<04:57,  2.73s/it]\u001b[A\n",
      " 41%|█████████████████▏                        | 75/183 [03:32<04:54,  2.73s/it]\u001b[A\n",
      " 42%|█████████████████▍                        | 76/183 [03:35<04:38,  2.60s/it]\u001b[A\n",
      " 42%|█████████████████▋                        | 77/183 [03:37<04:38,  2.63s/it]\u001b[A\n",
      " 43%|█████████████████▉                        | 78/183 [03:40<04:45,  2.72s/it]\u001b[A\n",
      " 43%|██████████████████▏                       | 79/183 [03:43<04:28,  2.59s/it]\u001b[A\n",
      " 44%|██████████████████▎                       | 80/183 [03:45<04:24,  2.57s/it]\u001b[A\n",
      " 44%|██████████████████▌                       | 81/183 [03:48<04:15,  2.50s/it]\u001b[A\n",
      " 45%|██████████████████▊                       | 82/183 [03:50<04:16,  2.54s/it]\u001b[A\n",
      " 45%|███████████████████                       | 83/183 [03:53<04:18,  2.58s/it]\u001b[A\n",
      " 46%|███████████████████▎                      | 84/183 [03:55<04:08,  2.51s/it]\u001b[A\n",
      " 46%|███████████████████▌                      | 85/183 [03:58<04:13,  2.59s/it]\u001b[A\n",
      " 47%|███████████████████▋                      | 86/183 [04:00<04:05,  2.53s/it]\u001b[A\n",
      " 48%|███████████████████▉                      | 87/183 [04:03<04:04,  2.55s/it]\u001b[A\n",
      " 48%|████████████████████▏                     | 88/183 [04:06<04:04,  2.57s/it]\u001b[A\n",
      " 49%|████████████████████▍                     | 89/183 [04:08<04:02,  2.58s/it]\u001b[A\n",
      " 49%|████████████████████▋                     | 90/183 [04:11<03:59,  2.58s/it]\u001b[A\n",
      " 50%|████████████████████▉                     | 91/183 [04:13<03:58,  2.59s/it]\u001b[A\n",
      " 50%|█████████████████████                     | 92/183 [04:16<03:54,  2.58s/it]\u001b[A\n",
      " 51%|█████████████████████▎                    | 93/183 [04:18<03:52,  2.59s/it]\u001b[A\n",
      " 51%|█████████████████████▌                    | 94/183 [04:21<03:50,  2.59s/it]\u001b[A\n",
      " 52%|█████████████████████▊                    | 95/183 [04:24<03:49,  2.60s/it]\u001b[A\n",
      " 52%|██████████████████████                    | 96/183 [04:26<03:46,  2.60s/it]\u001b[A\n",
      " 53%|██████████████████████▎                   | 97/183 [04:28<03:31,  2.46s/it]\u001b[A\n",
      " 54%|██████████████████████▍                   | 98/183 [04:31<03:27,  2.44s/it]\u001b[A\n",
      " 54%|██████████████████████▋                   | 99/183 [04:34<03:32,  2.53s/it]\u001b[A\n",
      " 55%|██████████████████████▍                  | 100/183 [04:36<03:30,  2.54s/it]\u001b[A\n",
      " 55%|██████████████████████▋                  | 101/183 [04:40<04:07,  3.02s/it]\u001b[A\n",
      " 56%|██████████████████████▊                  | 102/183 [04:43<03:58,  2.94s/it]\u001b[A\n",
      " 56%|███████████████████████                  | 103/183 [04:47<04:23,  3.30s/it]\u001b[A\n",
      " 57%|███████████████████████▎                 | 104/183 [04:50<03:57,  3.01s/it]\u001b[A\n",
      " 57%|███████████████████████▌                 | 105/183 [04:54<04:21,  3.36s/it]\u001b[A\n",
      " 58%|███████████████████████▋                 | 106/183 [04:58<04:35,  3.58s/it]\u001b[A\n",
      " 58%|███████████████████████▉                 | 107/183 [05:00<04:10,  3.30s/it]\u001b[A\n",
      " 59%|████████████████████████▏                | 108/183 [05:03<03:43,  2.98s/it]\u001b[A\n",
      " 60%|████████████████████████▍                | 109/183 [05:05<03:25,  2.78s/it]\u001b[A\n",
      " 60%|████████████████████████▋                | 110/183 [05:08<03:18,  2.73s/it]\u001b[A\n",
      " 61%|████████████████████████▊                | 111/183 [05:10<03:07,  2.60s/it]\u001b[A\n",
      " 61%|█████████████████████████                | 112/183 [05:12<03:03,  2.59s/it]\u001b[A\n",
      " 62%|█████████████████████████▎               | 113/183 [05:17<03:33,  3.05s/it]\u001b[A\n",
      " 62%|█████████████████████████▌               | 114/183 [05:21<03:53,  3.38s/it]\u001b[A\n",
      " 63%|█████████████████████████▊               | 115/183 [05:25<04:05,  3.61s/it]\u001b[A\n",
      " 63%|█████████████████████████▉               | 116/183 [05:29<04:12,  3.77s/it]\u001b[A\n",
      " 64%|██████████████████████████▏              | 117/183 [05:31<03:39,  3.33s/it]\u001b[A\n",
      " 64%|██████████████████████████▍              | 118/183 [05:34<03:23,  3.12s/it]\u001b[A\n",
      " 65%|██████████████████████████▋              | 119/183 [05:38<03:40,  3.45s/it]\u001b[A\n",
      " 66%|██████████████████████████▉              | 120/183 [05:42<03:50,  3.66s/it]\u001b[A\n",
      " 66%|███████████████████████████              | 121/183 [05:45<03:22,  3.26s/it]\u001b[A\n",
      " 67%|███████████████████████████▎             | 122/183 [05:47<03:05,  3.04s/it]\u001b[A\n",
      " 67%|███████████████████████████▌             | 123/183 [05:51<03:22,  3.38s/it]\u001b[A\n",
      " 68%|███████████████████████████▊             | 124/183 [05:53<02:52,  2.92s/it]\u001b[A\n",
      " 68%|████████████████████████████             | 125/183 [05:55<02:38,  2.74s/it]\u001b[A\n",
      " 69%|████████████████████████████▏            | 126/183 [05:58<02:28,  2.60s/it]\u001b[A\n",
      " 69%|████████████████████████████▍            | 127/183 [06:00<02:25,  2.61s/it]\u001b[A\n",
      " 70%|████████████████████████████▋            | 128/183 [06:03<02:18,  2.52s/it]\u001b[A\n",
      " 70%|████████████████████████████▉            | 129/183 [06:07<02:41,  2.99s/it]\u001b[A\n",
      " 71%|█████████████████████████████▏           | 130/183 [06:11<02:57,  3.36s/it]\u001b[A\n",
      " 72%|█████████████████████████████▎           | 131/183 [06:15<03:07,  3.61s/it]\u001b[A\n",
      " 72%|█████████████████████████████▌           | 132/183 [06:18<02:44,  3.22s/it]\u001b[A\n",
      " 73%|█████████████████████████████▊           | 133/183 [06:20<02:36,  3.13s/it]\u001b[A\n",
      " 73%|██████████████████████████████           | 134/183 [06:23<02:29,  3.04s/it]\u001b[A\n",
      " 74%|██████████████████████████████▏          | 135/183 [06:26<02:18,  2.89s/it]\u001b[A\n",
      " 74%|██████████████████████████████▍          | 136/183 [06:29<02:15,  2.89s/it]\u001b[A\n",
      " 75%|██████████████████████████████▋          | 137/183 [06:31<02:05,  2.72s/it]\u001b[A\n",
      " 75%|██████████████████████████████▉          | 138/183 [06:33<01:54,  2.54s/it]\u001b[A\n",
      " 76%|███████████████████████████████▏         | 139/183 [06:36<01:52,  2.55s/it]\u001b[A\n",
      " 77%|███████████████████████████████▎         | 140/183 [06:38<01:50,  2.56s/it]\u001b[A\n",
      " 77%|███████████████████████████████▌         | 141/183 [06:41<01:47,  2.56s/it]\u001b[A\n",
      " 78%|███████████████████████████████▊         | 142/183 [06:43<01:45,  2.58s/it]\u001b[A\n",
      " 78%|████████████████████████████████         | 143/183 [06:46<01:42,  2.57s/it]\u001b[A\n",
      " 79%|████████████████████████████████▎        | 144/183 [06:48<01:37,  2.49s/it]\u001b[A\n",
      " 79%|████████████████████████████████▍        | 145/183 [06:51<01:35,  2.52s/it]\u001b[A\n",
      " 80%|████████████████████████████████▋        | 146/183 [06:55<01:50,  3.00s/it]\u001b[A\n",
      " 80%|████████████████████████████████▉        | 147/183 [06:59<02:00,  3.34s/it]\u001b[A\n",
      " 81%|█████████████████████████████████▏       | 148/183 [07:03<02:05,  3.58s/it]\u001b[A\n",
      " 81%|█████████████████████████████████▍       | 149/183 [07:07<02:07,  3.76s/it]\u001b[A\n",
      " 82%|█████████████████████████████████▌       | 150/183 [07:10<01:52,  3.42s/it]\u001b[A\n",
      " 83%|█████████████████████████████████▊       | 151/183 [07:13<01:41,  3.18s/it]\u001b[A\n",
      " 83%|██████████████████████████████████       | 152/183 [07:15<01:30,  2.92s/it]\u001b[A\n",
      " 84%|██████████████████████████████████▎      | 153/183 [07:18<01:25,  2.84s/it]\u001b[A\n",
      " 84%|██████████████████████████████████▌      | 154/183 [07:20<01:19,  2.75s/it]\u001b[A\n",
      " 85%|██████████████████████████████████▋      | 155/183 [07:22<01:10,  2.50s/it]\u001b[A\n",
      " 85%|██████████████████████████████████▉      | 156/183 [07:24<01:05,  2.44s/it]\u001b[A\n",
      " 86%|███████████████████████████████████▏     | 157/183 [07:27<01:03,  2.44s/it]\u001b[A\n",
      " 86%|███████████████████████████████████▍     | 158/183 [07:30<01:04,  2.58s/it]\u001b[A\n",
      " 87%|███████████████████████████████████▌     | 159/183 [07:32<01:01,  2.57s/it]\u001b[A\n",
      " 87%|███████████████████████████████████▊     | 160/183 [07:35<00:57,  2.50s/it]\u001b[A\n",
      " 88%|████████████████████████████████████     | 161/183 [07:37<00:52,  2.37s/it]\u001b[A\n",
      " 89%|████████████████████████████████████▎    | 162/183 [07:41<01:00,  2.90s/it]\u001b[A\n",
      " 89%|████████████████████████████████████▌    | 163/183 [07:43<00:54,  2.72s/it]\u001b[A\n",
      " 90%|████████████████████████████████████▋    | 164/183 [07:45<00:49,  2.59s/it]\u001b[A\n",
      " 90%|████████████████████████████████████▉    | 165/183 [07:48<00:45,  2.52s/it]\u001b[A\n",
      " 91%|█████████████████████████████████████▏   | 166/183 [07:50<00:43,  2.56s/it]\u001b[A\n",
      " 91%|█████████████████████████████████████▍   | 167/183 [07:53<00:41,  2.58s/it]\u001b[A\n",
      " 92%|█████████████████████████████████████▋   | 168/183 [07:56<00:38,  2.59s/it]\u001b[A\n",
      " 92%|█████████████████████████████████████▊   | 169/183 [08:00<00:42,  3.06s/it]\u001b[A\n",
      " 93%|██████████████████████████████████████   | 170/183 [08:03<00:38,  3.00s/it]\u001b[A\n",
      " 93%|██████████████████████████████████████▎  | 171/183 [08:05<00:34,  2.84s/it]\u001b[A\n",
      " 94%|██████████████████████████████████████▌  | 172/183 [08:08<00:29,  2.68s/it]\u001b[A\n",
      " 95%|██████████████████████████████████████▊  | 173/183 [08:10<00:26,  2.64s/it]\u001b[A\n",
      " 95%|██████████████████████████████████████▉  | 174/183 [08:14<00:28,  3.12s/it]\u001b[A\n",
      " 96%|███████████████████████████████████████▏ | 175/183 [08:17<00:23,  2.96s/it]\u001b[A\n",
      " 96%|███████████████████████████████████████▍ | 176/183 [08:21<00:23,  3.33s/it]\u001b[A\n",
      " 97%|███████████████████████████████████████▋ | 177/183 [08:24<00:19,  3.17s/it]\u001b[A\n",
      " 97%|███████████████████████████████████████▉ | 178/183 [08:27<00:15,  3.03s/it]\u001b[A\n",
      " 98%|████████████████████████████████████████ | 179/183 [08:29<00:11,  2.90s/it]\u001b[A\n",
      " 98%|████████████████████████████████████████▎| 180/183 [08:31<00:08,  2.72s/it]\u001b[A\n",
      " 99%|████████████████████████████████████████▌| 181/183 [08:34<00:05,  2.67s/it]\u001b[A\n",
      " 99%|████████████████████████████████████████▊| 182/183 [08:37<00:02,  2.65s/it]\u001b[A\n",
      "100%|█████████████████████████████████████████| 183/183 [08:39<00:00,  2.66s/it]\u001b[A\n",
      "{'eval_loss': 6.020317554473877, 'eval_bleu': 0.0247, 'eval_gen_len': 54.1598, 'eval_runtime': 531.8008, 'eval_samples_per_second': 2.766, 'eval_steps_per_second': 0.346, 'epoch': 25.0}\n",
      "\n",
      "  0%|                                    | 100/40000 [12:49<26:30:08,  2.39s/it]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:3548] 2024-08-16 05:37:45,835 >> Saving model checkpoint to models/marian/marian_output/checkpoint-100\n",
      "[WARNING|configuration_utils.py:448] 2024-08-16 05:37:45,836 >> Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 128, 'num_beams': 2, 'bad_words_ids': [[31999]], 'forced_eos_token_id': 2}\n",
      "[INFO|configuration_utils.py:472] 2024-08-16 05:37:45,838 >> Configuration saved in models/marian/marian_output/checkpoint-100/config.json\n",
      "[INFO|configuration_utils.py:807] 2024-08-16 05:37:45,839 >> Configuration saved in models/marian/marian_output/checkpoint-100/generation_config.json\n",
      "[INFO|modeling_utils.py:2778] 2024-08-16 05:37:46,316 >> Model weights saved in models/marian/marian_output/checkpoint-100/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2684] 2024-08-16 05:37:46,317 >> tokenizer config file saved in models/marian/marian_output/checkpoint-100/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2693] 2024-08-16 05:37:46,317 >> Special tokens file saved in models/marian/marian_output/checkpoint-100/special_tokens_map.json\n",
      "{'loss': 5.9048, 'grad_norm': 2.276027202606201, 'learning_rate': 9.97499374843711e-05, 'epoch': 27.5}\n",
      "{'loss': 5.8199, 'grad_norm': 2.5293049812316895, 'learning_rate': 9.97249312328082e-05, 'epoch': 30.0}\n",
      "{'loss': 5.7115, 'grad_norm': 1.1548680067062378, 'learning_rate': 9.969992498124532e-05, 'epoch': 32.5}\n",
      "{'loss': 5.5834, 'grad_norm': 1.340080976486206, 'learning_rate': 9.967491872968243e-05, 'epoch': 35.0}\n",
      "{'loss': 5.4605, 'grad_norm': 1.8851202726364136, 'learning_rate': 9.964991247811953e-05, 'epoch': 37.5}\n",
      "{'loss': 5.338, 'grad_norm': 1.066258430480957, 'learning_rate': 9.962490622655665e-05, 'epoch': 40.0}\n",
      "{'loss': 5.2178, 'grad_norm': 2.0635523796081543, 'learning_rate': 9.959989997499375e-05, 'epoch': 42.5}\n",
      "{'loss': 5.108, 'grad_norm': 1.3573788404464722, 'learning_rate': 9.957489372343087e-05, 'epoch': 45.0}\n",
      "{'loss': 4.9925, 'grad_norm': 0.7882304191589355, 'learning_rate': 9.954988747186797e-05, 'epoch': 47.5}\n",
      "{'loss': 4.8803, 'grad_norm': 1.457580327987671, 'learning_rate': 9.952488122030508e-05, 'epoch': 50.0}\n",
      "  0%|▏                                   | 200/40000 [16:47<25:59:46,  2.35s/it][INFO|trainer.py:3864] 2024-08-16 05:41:43,070 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3866] 2024-08-16 05:41:43,070 >>   Num examples = 1471\n",
      "[INFO|trainer.py:3869] 2024-08-16 05:41:43,070 >>   Batch size = 8\n",
      "\n",
      "  0%|                                                   | 0/183 [00:00<?, ?it/s]\u001b[A\n",
      "  1%|▍                                          | 2/183 [00:00<01:00,  3.00it/s]\u001b[A\n",
      "  2%|▋                                          | 3/183 [00:01<01:31,  1.97it/s]\u001b[A\n",
      "  2%|▉                                          | 4/183 [00:01<01:29,  2.01it/s]\u001b[A\n",
      "  3%|█▏                                         | 5/183 [00:02<01:32,  1.92it/s]\u001b[A\n",
      "  3%|█▍                                         | 6/183 [00:03<01:35,  1.84it/s]\u001b[A\n",
      "  4%|█▋                                         | 7/183 [00:03<01:44,  1.69it/s]\u001b[A\n",
      "  4%|█▉                                         | 8/183 [00:04<01:46,  1.65it/s]\u001b[A\n",
      "  5%|██                                         | 9/183 [00:05<01:56,  1.49it/s]\u001b[A\n",
      "  5%|██▎                                       | 10/183 [00:06<02:20,  1.23it/s]\u001b[A\n",
      "  6%|██▌                                       | 11/183 [00:06<02:10,  1.31it/s]\u001b[A\n",
      "  7%|██▊                                       | 12/183 [00:07<02:08,  1.33it/s]\u001b[A\n",
      "  7%|██▉                                       | 13/183 [00:08<02:13,  1.27it/s]\u001b[A\n",
      "  8%|███▏                                      | 14/183 [00:09<02:17,  1.23it/s]\u001b[A\n",
      "  8%|███▍                                      | 15/183 [00:10<02:15,  1.24it/s]\u001b[A\n",
      "  9%|███▋                                      | 16/183 [00:10<02:03,  1.35it/s]\u001b[A\n",
      "  9%|███▉                                      | 17/183 [00:11<01:56,  1.43it/s]\u001b[A\n",
      " 10%|████▏                                     | 18/183 [00:11<01:48,  1.52it/s]\u001b[A\n",
      " 10%|████▎                                     | 19/183 [00:12<01:46,  1.54it/s]\u001b[A\n",
      " 11%|████▌                                     | 20/183 [00:13<01:45,  1.54it/s]\u001b[A\n",
      " 11%|████▊                                     | 21/183 [00:13<01:46,  1.53it/s]\u001b[A\n",
      " 12%|█████                                     | 22/183 [00:14<01:54,  1.40it/s]\u001b[A\n",
      " 13%|█████▎                                    | 23/183 [00:15<01:46,  1.50it/s]\u001b[A\n",
      " 13%|█████▌                                    | 24/183 [00:15<01:44,  1.53it/s]\u001b[A\n",
      " 14%|█████▋                                    | 25/183 [00:16<01:50,  1.43it/s]\u001b[A\n",
      " 14%|█████▉                                    | 26/183 [00:20<04:00,  1.53s/it]\u001b[A\n",
      " 15%|██████▏                                   | 27/183 [00:20<03:07,  1.20s/it]\u001b[A\n",
      " 15%|██████▍                                   | 28/183 [00:21<02:59,  1.16s/it]\u001b[A\n",
      " 16%|██████▋                                   | 29/183 [00:22<02:35,  1.01s/it]\u001b[A\n",
      " 16%|██████▉                                   | 30/183 [00:25<04:31,  1.77s/it]\u001b[A\n",
      " 17%|███████                                   | 31/183 [00:27<03:56,  1.56s/it]\u001b[A\n",
      " 17%|███████▎                                  | 32/183 [00:28<03:36,  1.43s/it]\u001b[A\n",
      " 18%|███████▌                                  | 33/183 [00:28<02:59,  1.20s/it]\u001b[A\n",
      " 19%|███████▊                                  | 34/183 [00:29<02:32,  1.02s/it]\u001b[A\n",
      " 19%|████████                                  | 35/183 [00:32<04:21,  1.76s/it]\u001b[A\n",
      " 20%|████████▎                                 | 36/183 [00:33<03:45,  1.53s/it]\u001b[A\n",
      " 20%|████████▍                                 | 37/183 [00:34<03:11,  1.31s/it]\u001b[A\n",
      " 21%|████████▋                                 | 38/183 [00:35<02:49,  1.17s/it]\u001b[A\n",
      " 21%|████████▉                                 | 39/183 [00:36<02:24,  1.01s/it]\u001b[A\n",
      " 22%|█████████▏                                | 40/183 [00:36<02:08,  1.11it/s]\u001b[A\n",
      " 22%|█████████▍                                | 41/183 [00:37<02:01,  1.17it/s]\u001b[A\n",
      " 23%|█████████▋                                | 42/183 [00:38<01:59,  1.18it/s]\u001b[A\n",
      " 23%|█████████▊                                | 43/183 [00:39<02:01,  1.15it/s]\u001b[A\n",
      " 24%|██████████                                | 44/183 [00:40<01:57,  1.18it/s]\u001b[A\n",
      " 25%|██████████▎                               | 45/183 [00:41<02:03,  1.11it/s]\u001b[A\n",
      " 25%|██████████▌                               | 46/183 [00:42<02:09,  1.06it/s]\u001b[A\n",
      " 26%|██████████▊                               | 47/183 [00:42<01:54,  1.18it/s]\u001b[A\n",
      " 26%|███████████                               | 48/183 [00:43<01:48,  1.24it/s]\u001b[A\n",
      " 27%|███████████▏                              | 49/183 [00:44<01:39,  1.35it/s]\u001b[A\n",
      " 27%|███████████▍                              | 50/183 [00:44<01:42,  1.29it/s]\u001b[A\n",
      " 28%|███████████▋                              | 51/183 [00:45<01:49,  1.21it/s]\u001b[A\n",
      " 28%|███████████▉                              | 52/183 [00:46<01:44,  1.25it/s]\u001b[A\n",
      " 29%|████████████▏                             | 53/183 [00:47<01:52,  1.16it/s]\u001b[A\n",
      " 30%|████████████▍                             | 54/183 [00:48<01:47,  1.20it/s]\u001b[A\n",
      " 30%|████████████▌                             | 55/183 [00:48<01:37,  1.31it/s]\u001b[A\n",
      " 31%|████████████▊                             | 56/183 [00:49<01:37,  1.31it/s]\u001b[A\n",
      " 31%|█████████████                             | 57/183 [00:50<01:36,  1.30it/s]\u001b[A\n",
      " 32%|█████████████▎                            | 58/183 [00:52<02:06,  1.01s/it]\u001b[A\n",
      " 32%|█████████████▌                            | 59/183 [00:53<02:02,  1.01it/s]\u001b[A\n",
      " 33%|█████████████▊                            | 60/183 [00:54<02:03,  1.01s/it]\u001b[A\n",
      " 33%|██████████████                            | 61/183 [00:54<01:58,  1.03it/s]\u001b[A^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/zindi/transformers/examples/pytorch/translation/run_translation.py\", line 697, in <module>\n",
      "    main()\n",
      "  File \"/root/zindi/transformers/examples/pytorch/translation/run_translation.py\", line 612, in main\n",
      "    train_result = trainer.train(resume_from_checkpoint=checkpoint)\n",
      "  File \"/root/zindi/transformers/src/transformers/trainer.py\", line 1964, in train\n",
      "    return inner_training_loop(\n",
      "  File \"/root/zindi/transformers/src/transformers/trainer.py\", line 2382, in _inner_training_loop\n",
      "    self._maybe_log_save_evaluate(tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\n",
      "  File \"/root/zindi/transformers/src/transformers/trainer.py\", line 2830, in _maybe_log_save_evaluate\n",
      "    metrics = self._evaluate(trial, ignore_keys_for_eval)\n",
      "  File \"/root/zindi/transformers/src/transformers/trainer.py\", line 2787, in _evaluate\n",
      "    metrics = self.evaluate(ignore_keys=ignore_keys_for_eval)\n",
      "  File \"/root/zindi/transformers/src/transformers/trainer_seq2seq.py\", line 180, in evaluate\n",
      "    return super().evaluate(eval_dataset, ignore_keys=ignore_keys, metric_key_prefix=metric_key_prefix)\n",
      "  File \"/root/zindi/transformers/src/transformers/trainer.py\", line 3711, in evaluate\n",
      "    output = eval_loop(\n",
      "  File \"/root/zindi/transformers/src/transformers/trainer.py\", line 3902, in evaluation_loop\n",
      "    losses, logits, labels = self.prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)\n",
      "  File \"/root/zindi/transformers/src/transformers/trainer_seq2seq.py\", line 310, in prediction_step\n",
      "    generated_tokens = self.model.generate(**generation_inputs, **gen_kwargs)\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/zindi-LtLKIbXv-py3.9/lib/python3.9/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/root/zindi/transformers/src/transformers/generation/utils.py\", line 2063, in generate\n",
      "    result = self._beam_search(\n",
      "  File \"/root/zindi/transformers/src/transformers/generation/utils.py\", line 3238, in _beam_search\n",
      "    outputs = self(**model_inputs, return_dict=True)\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/zindi-LtLKIbXv-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/zindi-LtLKIbXv-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/root/zindi/transformers/src/transformers/models/marian/modeling_marian.py\", line 1399, in forward\n",
      "    outputs = self.model(\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/zindi-LtLKIbXv-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/zindi-LtLKIbXv-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/root/zindi/transformers/src/transformers/models/marian/modeling_marian.py\", line 1194, in forward\n",
      "    decoder_outputs = self.decoder(\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/zindi-LtLKIbXv-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/zindi-LtLKIbXv-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/root/zindi/transformers/src/transformers/models/marian/modeling_marian.py\", line 994, in forward\n",
      "    layer_outputs = decoder_layer(\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/zindi-LtLKIbXv-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/zindi-LtLKIbXv-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/root/zindi/transformers/src/transformers/models/marian/modeling_marian.py\", line 423, in forward\n",
      "    hidden_states, cross_attn_weights, cross_attn_present_key_value = self.encoder_attn(\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/zindi-LtLKIbXv-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/zindi-LtLKIbXv-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/root/zindi/transformers/src/transformers/models/marian/modeling_marian.py\", line 220, in forward\n",
      "    attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/zindi-LtLKIbXv-py3.9/lib/python3.9/site-packages/torch/nn/functional.py\", line 1888, in softmax\n",
      "    ret = input.softmax(dim)\n",
      "KeyboardInterrupt\n",
      "  0%|▏                                   | 200/40000 [17:43<58:47:20,  5.32s/it]\n",
      "\n",
      "                                                                                \u001b[A"
     ]
    }
   ],
   "source": [
    "!python transformers/examples/pytorch/translation/run_translation.py \\\n",
    "--per_device_train_batch_size $$per_device_train_batch_size \\\n",
    "--per_device_eval_batch_size $$per_device_eval_batch_size \\\n",
    "--save_steps $$save_steps \\\n",
    "--num_train_epochs $$num_train_epochs \\\n",
    "--logging_steps $$logging_steps \\\n",
    "--label_smoothing_factor $$label_smoothing_factor \\\n",
    "--learning_rate $$learning_rate \\\n",
    "--run_name $$run_name \\\n",
    "--output_dir $$output_dir \\\n",
    "--logging_dir $$logging_dir \\\n",
    "--eval_steps $$eval_steps \\\n",
    "--gradient_accumulation_steps $$gradient_accumulation_steps \\\n",
    "--model_name_or_path  $$model_name_or_path  \\\n",
    "--dataset_name  $$dataset_name  \\\n",
    "--generation_max_length $$generation_max_length \\\n",
    "--generation_num_beams $$generation_num_beams \\\n",
    "--source_lang $$source_lang \\\n",
    "--target_lang $$target_lang \\\n",
    "--dataset_config_name $$dataset_config_name \\\n",
    "--predict_with_generate $$predict_with_generate \\\n",
    "--max_source_length $$max_source_length \\\n",
    "--dataloader_drop_last $$dataloader_drop_last \\\n",
    "--warmup_steps $$warmup_steps \\\n",
    "--weight_decay $$weight_decay \\\n",
    "--save_total_limit $$save_total_limit \\\n",
    "--seed $$seed \\\n",
    "--overwrite_output_dir $$overwrite_output_dir \\\n",
    "--jit_mode_eval $$jit_mode_eval \\\n",
    "--do_train $$do_train \\\n",
    "--fp16 $$fp16 \\\n",
    "--fp16_backend $$fp16_backend \\\n",
    "--fp16_full_eval $$fp16_full_eval \\\n",
    "--full_determinism $$full_determinism \\\n",
    "--predict_with_generate true \\\n",
    "--do_eval true \\\n",
    "--do_predict true \\\n",
    "--eval_strategy steps\n",
    "# --generation_config $$generation_config \n",
    "# --resume_from_checkpoint {resume_from_checkpoint} \n",
    "# --use_cpu {use_cpu} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zindi-z3yfXQo9-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
