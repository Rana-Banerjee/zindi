{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/rana/Projects/zindi/common\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rana/.cache/pypoetry/virtualenvs/zindi-z3yfXQo9-py3.9/lib/python3.9/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd /home/rana/Projects/zindi/common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #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\n",
    "# !huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Device specific params\n",
    "per_device_eval_batch_size=1\n",
    "per_device_train_batch_size=1\n",
    "use_cpu=True\n",
    "save_steps=1\n",
    "num_train_epochs=1000.0\n",
    "logging_steps=1\n",
    "label_smoothing_factor=0.00001\n",
    "learning_rate=5e-05\n",
    "# resume_from_checkpoint=None\n",
    "run_name=\"marian-1\"\n",
    "output_dir=\"../models/marian/marian_output\"\n",
    "logging_dir=\"../models/marian/marian-logs\"\n",
    "eval_steps=1\n",
    "gradient_accumulation_steps=8\n",
    "model_name_or_path = \"Helsinki-NLP/opus-mt-af-fr\"\n",
    "dataset_name = \"uvci/Koumankan_mt_dyu_fr\"\n",
    "generation_max_length=150\n",
    "generation_num_beams=1\n",
    "source_lang=\"dyu\"\n",
    "target_lang=\"fr\"\n",
    "dataset_config_name= \"default\"\n",
    "predict_with_generate=\"True\"\n",
    "max_source_length= 150\n",
    "dataloader_drop_last=\"True\"\n",
    "warmup_steps=10\n",
    "weight_decay=0.00001\n",
    "save_total_limit=3\n",
    "seed=42\n",
    "overwrite_output_dir=\"True\"\n",
    "jit_mode_eval=\"False\"\n",
    "do_eval=\"True\"\n",
    "do_predict=\"False\"\n",
    "do_train=\"True\"\n",
    "fp16=\"True\"\n",
    "fp16_backend=\"auto\"\n",
    "fp16_full_eval=\"True\"\n",
    "full_determinism=\"True\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Seq2SeqTrainingArguments(\n",
    "# _n_gpu=0,\n",
    "# accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
    "# adafactor=False,\n",
    "# adam_beta1=0.9,\n",
    "# adam_beta2=0.999,\n",
    "# adam_epsilon=1e-08,\n",
    "# auto_find_batch_size=False,\n",
    "# batch_eval_metrics=False,\n",
    "# bf16=False,\n",
    "# bf16_full_eval=False,\n",
    "# data_seed=None,\n",
    "# dataloader_num_workers=0,\n",
    "# dataloader_persistent_workers=False,\n",
    "# dataloader_pin_memory=True,\n",
    "# dataloader_prefetch_factor=None,\n",
    "# ddp_backend=None,\n",
    "# ddp_broadcast_buffers=None,\n",
    "# ddp_bucket_cap_mb=None,\n",
    "# ddp_find_unused_parameters=None,\n",
    "# ddp_timeout=1800,\n",
    "# debug=[],\n",
    "# deepspeed=None,\n",
    "# disable_tqdm=False,\n",
    "# dispatch_batches=None,\n",
    "# eval_accumulation_steps=None,\n",
    "# eval_delay=0,\n",
    "# eval_do_concat_batches=True,\n",
    "# eval_on_start=False,\n",
    "# eval_strategy=no,\n",
    "# eval_use_gather_object=False,\n",
    "# evaluation_strategy=None,\n",
    "# fp16_opt_level=O1,\n",
    "# fsdp=[],\n",
    "# fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
    "# fsdp_min_num_params=0,\n",
    "# fsdp_transformer_layer_cls_to_wrap=None,\n",
    "# generation_config=None,\n",
    "# gradient_checkpointing=False,\n",
    "# gradient_checkpointing_kwargs=None,\n",
    "# greater_is_better=None,\n",
    "# group_by_length=False,\n",
    "# half_precision_backend=auto,\n",
    "# hub_always_push=False,\n",
    "# hub_model_id=None,\n",
    "# hub_private_repo=False,\n",
    "# hub_strategy=every_save,\n",
    "# hub_token=<HUB_TOKEN>,\n",
    "# ignore_data_skip=False,\n",
    "# include_inputs_for_metrics=False,\n",
    "# include_num_input_tokens_seen=False,\n",
    "# include_tokens_per_second=False,\n",
    "# label_names=None,\n",
    "# length_column_name=length,\n",
    "# load_best_model_at_end=False,\n",
    "# local_rank=0,\n",
    "# log_level=passive,\n",
    "# log_level_replica=warning,\n",
    "# log_on_each_node=True,\n",
    "# logging_first_step=False,\n",
    "# logging_nan_inf_filter=True,\n",
    "# logging_strategy=steps,\n",
    "# lr_scheduler_kwargs={},\n",
    "# lr_scheduler_type=linear,\n",
    "# max_grad_norm=1.0,\n",
    "# max_steps=-1,\n",
    "# metric_for_best_model=None,\n",
    "# mp_parameters=,\n",
    "# neftune_noise_alpha=None,\n",
    "# no_cuda=False,\n",
    "# optim=adamw_torch,\n",
    "# optim_args=None,\n",
    "# optim_target_modules=None,\n",
    "# past_index=-1,\n",
    "# predict_with_generate=True,\n",
    "# prediction_loss_only=False,\n",
    "# push_to_hub=False,\n",
    "# push_to_hub_model_id=None,\n",
    "# push_to_hub_organization=None,\n",
    "# push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
    "# ray_scope=last,\n",
    "# remove_unused_columns=True,\n",
    "# report_to=[],\n",
    "# restore_callback_states_from_checkpoint=False,\n",
    "# save_on_each_node=False,\n",
    "# save_only_model=False,\n",
    "# save_safetensors=True,\n",
    "# save_strategy=steps,\n",
    "# skip_memory_metrics=True,\n",
    "# sortish_sampler=False,\n",
    "# split_batches=None,\n",
    "# tf32=None,\n",
    "# torch_compile=False,\n",
    "# torch_compile_backend=None,\n",
    "# torch_compile_mode=None,\n",
    "# torch_empty_cache_steps=None,\n",
    "# torchdynamo=None,\n",
    "# tpu_metrics_debug=False,\n",
    "# tpu_num_cores=None,\n",
    "# use_ipex=False,\n",
    "# use_legacy_prediction_loop=False,\n",
    "# use_mps_device=False,\n",
    "# warmup_ratio=0.0,\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rana/.cache/pypoetry/virtualenvs/zindi-z3yfXQo9-py3.9/lib/python3.9/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/rana/Projects/zindi/transformers\n",
      "08/10/2024 18:13:57 - WARNING - __main__ - Process rank: 0, device: cpu, n_gpu: 0, distributed training: False, 16-bits training: True\n",
      "08/10/2024 18:13:57 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n",
      "_n_gpu=0,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=True,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=False,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_on_start=False,\n",
      "eval_steps=1.0,\n",
      "eval_strategy=no,\n",
      "eval_use_gather_object=False,\n",
      "evaluation_strategy=None,\n",
      "fp16=True,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=True,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=True,\n",
      "generation_config=None,\n",
      "generation_max_length=150,\n",
      "generation_num_beams=1,\n",
      "gradient_accumulation_steps=8,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=1e-05,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/marian/marian-logs,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=1.0,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1000.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=../models/marian/marian_output,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=1,\n",
      "predict_with_generate=True,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=marian-1,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=1.0,\n",
      "save_strategy=steps,\n",
      "save_total_limit=3,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "sortish_sampler=False,\n",
      "split_batches=None,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torch_empty_cache_steps=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=True,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=10,\n",
      "weight_decay=1e-05,\n",
      ")\n",
      "Overwrite dataset info from restored data version if exists.\n",
      "08/10/2024 18:14:18 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
      "Loading Dataset info from /home/rana/.cache/huggingface/datasets/uvci___koumankan_mt_dyu_fr/default/0.0.0/5207d8690e8c37ec07a1954dc7441c348f8242fa\n",
      "08/10/2024 18:14:18 - INFO - datasets.info - Loading Dataset info from /home/rana/.cache/huggingface/datasets/uvci___koumankan_mt_dyu_fr/default/0.0.0/5207d8690e8c37ec07a1954dc7441c348f8242fa\n",
      "Found cached dataset koumankan_mt_dyu_fr (/home/rana/.cache/huggingface/datasets/uvci___koumankan_mt_dyu_fr/default/0.0.0/5207d8690e8c37ec07a1954dc7441c348f8242fa)\n",
      "08/10/2024 18:14:18 - INFO - datasets.builder - Found cached dataset koumankan_mt_dyu_fr (/home/rana/.cache/huggingface/datasets/uvci___koumankan_mt_dyu_fr/default/0.0.0/5207d8690e8c37ec07a1954dc7441c348f8242fa)\n",
      "Loading Dataset info from /home/rana/.cache/huggingface/datasets/uvci___koumankan_mt_dyu_fr/default/0.0.0/5207d8690e8c37ec07a1954dc7441c348f8242fa\n",
      "08/10/2024 18:14:18 - INFO - datasets.info - Loading Dataset info from /home/rana/.cache/huggingface/datasets/uvci___koumankan_mt_dyu_fr/default/0.0.0/5207d8690e8c37ec07a1954dc7441c348f8242fa\n",
      "[INFO|configuration_utils.py:733] 2024-08-10 18:14:18,493 >> loading configuration file config.json from cache at /home/rana/.cache/huggingface/hub/models--Helsinki-NLP--opus-mt-af-fr/snapshots/6cc04f6929b1a149e10f2cf565e5233f35e45ec3/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-08-10 18:14:18,501 >> Model config MarianConfig {\n",
      "  \"_name_or_path\": \"Helsinki-NLP/opus-mt-af-fr\",\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      59421\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 59421,\n",
      "  \"decoder_vocab_size\": 59422,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 59421,\n",
      "  \"scale_embedding\": true,\n",
      "  \"share_encoder_decoder_embeddings\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"transformers_version\": \"4.45.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 59422\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:733] 2024-08-10 18:14:18,883 >> loading configuration file config.json from cache at /home/rana/.cache/huggingface/hub/models--Helsinki-NLP--opus-mt-af-fr/snapshots/6cc04f6929b1a149e10f2cf565e5233f35e45ec3/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-08-10 18:14:18,885 >> Model config MarianConfig {\n",
      "  \"_name_or_path\": \"Helsinki-NLP/opus-mt-af-fr\",\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      59421\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 59421,\n",
      "  \"decoder_vocab_size\": 59422,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 59421,\n",
      "  \"scale_embedding\": true,\n",
      "  \"share_encoder_decoder_embeddings\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"transformers_version\": \"4.45.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 59422\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2269] 2024-08-10 18:14:18,888 >> loading file source.spm from cache at /home/rana/.cache/huggingface/hub/models--Helsinki-NLP--opus-mt-af-fr/snapshots/6cc04f6929b1a149e10f2cf565e5233f35e45ec3/source.spm\n",
      "[INFO|tokenization_utils_base.py:2269] 2024-08-10 18:14:18,888 >> loading file target.spm from cache at /home/rana/.cache/huggingface/hub/models--Helsinki-NLP--opus-mt-af-fr/snapshots/6cc04f6929b1a149e10f2cf565e5233f35e45ec3/target.spm\n",
      "[INFO|tokenization_utils_base.py:2269] 2024-08-10 18:14:18,888 >> loading file vocab.json from cache at /home/rana/.cache/huggingface/hub/models--Helsinki-NLP--opus-mt-af-fr/snapshots/6cc04f6929b1a149e10f2cf565e5233f35e45ec3/vocab.json\n",
      "[INFO|tokenization_utils_base.py:2269] 2024-08-10 18:14:18,888 >> loading file target_vocab.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2269] 2024-08-10 18:14:18,888 >> loading file tokenizer_config.json from cache at /home/rana/.cache/huggingface/hub/models--Helsinki-NLP--opus-mt-af-fr/snapshots/6cc04f6929b1a149e10f2cf565e5233f35e45ec3/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2269] 2024-08-10 18:14:18,888 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2269] 2024-08-10 18:14:18,888 >> loading file special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2269] 2024-08-10 18:14:18,888 >> loading file tokenizer.json from cache at None\n",
      "[INFO|configuration_utils.py:733] 2024-08-10 18:14:18,889 >> loading configuration file config.json from cache at /home/rana/.cache/huggingface/hub/models--Helsinki-NLP--opus-mt-af-fr/snapshots/6cc04f6929b1a149e10f2cf565e5233f35e45ec3/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-08-10 18:14:18,890 >> Model config MarianConfig {\n",
      "  \"_name_or_path\": \"Helsinki-NLP/opus-mt-af-fr\",\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      59421\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 59421,\n",
      "  \"decoder_vocab_size\": 59422,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 59421,\n",
      "  \"scale_embedding\": true,\n",
      "  \"share_encoder_decoder_embeddings\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"transformers_version\": \"4.45.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 59422\n",
      "}\n",
      "\n",
      "/home/rana/Projects/zindi/transformers/src/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
      "/home/rana/Projects/zindi/transformers/src/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "[INFO|modeling_utils.py:3657] 2024-08-10 18:14:22,882 >> loading weights file pytorch_model.bin from cache at /home/rana/.cache/huggingface/hub/models--Helsinki-NLP--opus-mt-af-fr/snapshots/6cc04f6929b1a149e10f2cf565e5233f35e45ec3/pytorch_model.bin\n",
      "[INFO|configuration_utils.py:1038] 2024-08-10 18:14:24,320 >> Generate config GenerationConfig {\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      59421\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 59421,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"max_length\": 512,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 59421\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:4489] 2024-08-10 18:14:25,571 >> All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "[INFO|modeling_utils.py:4497] 2024-08-10 18:14:25,571 >> All the weights of MarianMTModel were initialized from the model checkpoint at Helsinki-NLP/opus-mt-af-fr.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "[INFO|configuration_utils.py:993] 2024-08-10 18:14:26,887 >> loading configuration file generation_config.json from cache at /home/rana/.cache/huggingface/hub/models--Helsinki-NLP--opus-mt-af-fr/snapshots/6cc04f6929b1a149e10f2cf565e5233f35e45ec3/generation_config.json\n",
      "[INFO|configuration_utils.py:1038] 2024-08-10 18:14:26,887 >> Generate config GenerationConfig {\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      59421\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 59421,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"max_length\": 512,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 59421,\n",
      "  \"renormalize_logits\": true\n",
      "}\n",
      "\n",
      "Loading cached processed dataset at /home/rana/.cache/huggingface/datasets/uvci___koumankan_mt_dyu_fr/default/0.0.0/5207d8690e8c37ec07a1954dc7441c348f8242fa/cache-76da34be6e4b97b5.arrow\n",
      "08/10/2024 18:14:27 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/rana/.cache/huggingface/datasets/uvci___koumankan_mt_dyu_fr/default/0.0.0/5207d8690e8c37ec07a1954dc7441c348f8242fa/cache-76da34be6e4b97b5.arrow\n",
      "Loading cached processed dataset at /home/rana/.cache/huggingface/datasets/uvci___koumankan_mt_dyu_fr/default/0.0.0/5207d8690e8c37ec07a1954dc7441c348f8242fa/cache-34205d99a086edb4.arrow\n",
      "08/10/2024 18:14:28 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/rana/.cache/huggingface/datasets/uvci___koumankan_mt_dyu_fr/default/0.0.0/5207d8690e8c37ec07a1954dc7441c348f8242fa/cache-34205d99a086edb4.arrow\n",
      "[INFO|trainer.py:655] 2024-08-10 18:14:31,284 >> Using auto half precision backend\n",
      "[INFO|trainer.py:2141] 2024-08-10 18:14:32,249 >> ***** Running training *****\n",
      "[INFO|trainer.py:2142] 2024-08-10 18:14:32,250 >>   Num examples = 8,065\n",
      "[INFO|trainer.py:2143] 2024-08-10 18:14:32,250 >>   Num Epochs = 1,000\n",
      "[INFO|trainer.py:2144] 2024-08-10 18:14:32,250 >>   Instantaneous batch size per device = 1\n",
      "[INFO|trainer.py:2147] 2024-08-10 18:14:32,250 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "[INFO|trainer.py:2148] 2024-08-10 18:14:32,250 >>   Gradient Accumulation steps = 8\n",
      "[INFO|trainer.py:2149] 2024-08-10 18:14:32,250 >>   Total optimization steps = 1,008,000\n",
      "[INFO|trainer.py:2150] 2024-08-10 18:14:32,251 >>   Number of trainable parameters = 74,562,560\n",
      "{'loss': 6.7695, 'grad_norm': 25.714614868164062, 'learning_rate': 5e-06, 'epoch': 0.0}\n",
      "  0%|                                  | 1/1008000 [00:05<1641:09:18,  5.86s/it][INFO|trainer.py:3510] 2024-08-10 18:14:38,116 >> Saving model checkpoint to ../models/marian/marian_output/checkpoint-1\n",
      "[WARNING|configuration_utils.py:448] 2024-08-10 18:14:38,116 >> Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59421]], 'forced_eos_token_id': 0}\n",
      "[INFO|configuration_utils.py:472] 2024-08-10 18:14:38,118 >> Configuration saved in ../models/marian/marian_output/checkpoint-1/config.json\n",
      "[INFO|configuration_utils.py:807] 2024-08-10 18:14:38,119 >> Configuration saved in ../models/marian/marian_output/checkpoint-1/generation_config.json\n",
      "[INFO|modeling_utils.py:2778] 2024-08-10 18:14:38,520 >> Model weights saved in ../models/marian/marian_output/checkpoint-1/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2684] 2024-08-10 18:14:38,521 >> tokenizer config file saved in ../models/marian/marian_output/checkpoint-1/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2693] 2024-08-10 18:14:38,521 >> Special tokens file saved in ../models/marian/marian_output/checkpoint-1/special_tokens_map.json\n",
      "{'loss': 6.7893, 'grad_norm': 26.04947853088379, 'learning_rate': 1e-05, 'epoch': 0.0}\n",
      "  0%|                                  | 2/1008000 [00:13<1902:48:52,  6.80s/it][INFO|trainer.py:3510] 2024-08-10 18:14:45,565 >> Saving model checkpoint to ../models/marian/marian_output/checkpoint-2\n",
      "[WARNING|configuration_utils.py:448] 2024-08-10 18:14:45,566 >> Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59421]], 'forced_eos_token_id': 0}\n",
      "[INFO|configuration_utils.py:472] 2024-08-10 18:14:45,568 >> Configuration saved in ../models/marian/marian_output/checkpoint-2/config.json\n",
      "[INFO|configuration_utils.py:807] 2024-08-10 18:14:45,569 >> Configuration saved in ../models/marian/marian_output/checkpoint-2/generation_config.json\n",
      "[INFO|modeling_utils.py:2778] 2024-08-10 18:14:45,952 >> Model weights saved in ../models/marian/marian_output/checkpoint-2/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2684] 2024-08-10 18:14:45,953 >> tokenizer config file saved in ../models/marian/marian_output/checkpoint-2/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2693] 2024-08-10 18:14:45,953 >> Special tokens file saved in ../models/marian/marian_output/checkpoint-2/special_tokens_map.json\n",
      "{'loss': 6.0887, 'grad_norm': 28.24888038635254, 'learning_rate': 1.5e-05, 'epoch': 0.0}\n",
      "  0%|                                  | 3/1008000 [00:20<1992:11:44,  7.12s/it][INFO|trainer.py:3510] 2024-08-10 18:14:53,060 >> Saving model checkpoint to ../models/marian/marian_output/checkpoint-3\n",
      "[WARNING|configuration_utils.py:448] 2024-08-10 18:14:53,061 >> Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59421]], 'forced_eos_token_id': 0}\n",
      "[INFO|configuration_utils.py:472] 2024-08-10 18:14:53,064 >> Configuration saved in ../models/marian/marian_output/checkpoint-3/config.json\n",
      "[INFO|configuration_utils.py:807] 2024-08-10 18:14:53,065 >> Configuration saved in ../models/marian/marian_output/checkpoint-3/generation_config.json\n",
      "[INFO|modeling_utils.py:2778] 2024-08-10 18:14:53,460 >> Model weights saved in ../models/marian/marian_output/checkpoint-3/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2684] 2024-08-10 18:14:53,461 >> tokenizer config file saved in ../models/marian/marian_output/checkpoint-3/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2693] 2024-08-10 18:14:53,461 >> Special tokens file saved in ../models/marian/marian_output/checkpoint-3/special_tokens_map.json\n",
      "{'loss': 5.8108, 'grad_norm': 26.02781867980957, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "  0%|                                  | 4/1008000 [00:28<2075:11:14,  7.41s/it][INFO|trainer.py:3510] 2024-08-10 18:15:00,926 >> Saving model checkpoint to ../models/marian/marian_output/checkpoint-4\n",
      "[WARNING|configuration_utils.py:448] 2024-08-10 18:15:00,926 >> Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59421]], 'forced_eos_token_id': 0}\n",
      "[INFO|configuration_utils.py:472] 2024-08-10 18:15:00,928 >> Configuration saved in ../models/marian/marian_output/checkpoint-4/config.json\n",
      "[INFO|configuration_utils.py:807] 2024-08-10 18:15:00,928 >> Configuration saved in ../models/marian/marian_output/checkpoint-4/generation_config.json\n",
      "[INFO|modeling_utils.py:2778] 2024-08-10 18:15:01,318 >> Model weights saved in ../models/marian/marian_output/checkpoint-4/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2684] 2024-08-10 18:15:01,319 >> tokenizer config file saved in ../models/marian/marian_output/checkpoint-4/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2693] 2024-08-10 18:15:01,320 >> Special tokens file saved in ../models/marian/marian_output/checkpoint-4/special_tokens_map.json\n",
      "[INFO|trainer.py:3602] 2024-08-10 18:15:03,321 >> Deleting older checkpoint [../models/marian/marian_output/checkpoint-1] due to args.save_total_limit\n",
      "{'loss': 5.1441, 'grad_norm': 21.914535522460938, 'learning_rate': 2.5e-05, 'epoch': 0.0}\n",
      "  0%|                                  | 5/1008000 [00:37<2170:33:17,  7.75s/it][INFO|trainer.py:3510] 2024-08-10 18:15:09,294 >> Saving model checkpoint to ../models/marian/marian_output/checkpoint-5\n",
      "[WARNING|configuration_utils.py:448] 2024-08-10 18:15:09,295 >> Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59421]], 'forced_eos_token_id': 0}\n",
      "[INFO|configuration_utils.py:472] 2024-08-10 18:15:09,297 >> Configuration saved in ../models/marian/marian_output/checkpoint-5/config.json\n",
      "[INFO|configuration_utils.py:807] 2024-08-10 18:15:09,298 >> Configuration saved in ../models/marian/marian_output/checkpoint-5/generation_config.json\n",
      "[INFO|modeling_utils.py:2778] 2024-08-10 18:15:09,741 >> Model weights saved in ../models/marian/marian_output/checkpoint-5/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2684] 2024-08-10 18:15:09,742 >> tokenizer config file saved in ../models/marian/marian_output/checkpoint-5/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2693] 2024-08-10 18:15:09,742 >> Special tokens file saved in ../models/marian/marian_output/checkpoint-5/special_tokens_map.json\n",
      "[INFO|trainer.py:3602] 2024-08-10 18:15:11,789 >> Deleting older checkpoint [../models/marian/marian_output/checkpoint-2] due to args.save_total_limit\n",
      "{'loss': 6.09, 'grad_norm': 22.444427490234375, 'learning_rate': 3e-05, 'epoch': 0.01}\n",
      "  0%|                                  | 6/1008000 [00:45<2264:40:11,  8.09s/it][INFO|trainer.py:3510] 2024-08-10 18:15:18,022 >> Saving model checkpoint to ../models/marian/marian_output/checkpoint-6\n",
      "[WARNING|configuration_utils.py:448] 2024-08-10 18:15:18,023 >> Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59421]], 'forced_eos_token_id': 0}\n",
      "[INFO|configuration_utils.py:472] 2024-08-10 18:15:18,024 >> Configuration saved in ../models/marian/marian_output/checkpoint-6/config.json\n",
      "[INFO|configuration_utils.py:807] 2024-08-10 18:15:18,025 >> Configuration saved in ../models/marian/marian_output/checkpoint-6/generation_config.json\n",
      "[INFO|modeling_utils.py:2778] 2024-08-10 18:15:18,409 >> Model weights saved in ../models/marian/marian_output/checkpoint-6/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2684] 2024-08-10 18:15:18,410 >> tokenizer config file saved in ../models/marian/marian_output/checkpoint-6/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2693] 2024-08-10 18:15:18,410 >> Special tokens file saved in ../models/marian/marian_output/checkpoint-6/special_tokens_map.json\n",
      "[INFO|trainer.py:3602] 2024-08-10 18:15:20,187 >> Deleting older checkpoint [../models/marian/marian_output/checkpoint-3] due to args.save_total_limit\n",
      "{'loss': 5.4698, 'grad_norm': 23.968250274658203, 'learning_rate': 3.5e-05, 'epoch': 0.01}\n",
      "  0%|                                  | 7/1008000 [00:53<2276:04:05,  8.13s/it][INFO|trainer.py:3510] 2024-08-10 18:15:26,235 >> Saving model checkpoint to ../models/marian/marian_output/checkpoint-7\n",
      "[WARNING|configuration_utils.py:448] 2024-08-10 18:15:26,236 >> Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59421]], 'forced_eos_token_id': 0}\n",
      "[INFO|configuration_utils.py:472] 2024-08-10 18:15:26,237 >> Configuration saved in ../models/marian/marian_output/checkpoint-7/config.json\n",
      "[INFO|configuration_utils.py:807] 2024-08-10 18:15:26,238 >> Configuration saved in ../models/marian/marian_output/checkpoint-7/generation_config.json\n",
      "[INFO|modeling_utils.py:2778] 2024-08-10 18:15:26,659 >> Model weights saved in ../models/marian/marian_output/checkpoint-7/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2684] 2024-08-10 18:15:26,660 >> tokenizer config file saved in ../models/marian/marian_output/checkpoint-7/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2693] 2024-08-10 18:15:26,661 >> Special tokens file saved in ../models/marian/marian_output/checkpoint-7/special_tokens_map.json\n",
      "[INFO|trainer.py:3602] 2024-08-10 18:15:28,769 >> Deleting older checkpoint [../models/marian/marian_output/checkpoint-4] due to args.save_total_limit\n",
      "{'loss': 5.2043, 'grad_norm': 21.832223892211914, 'learning_rate': 4e-05, 'epoch': 0.01}\n",
      "  0%|                                  | 8/1008000 [01:03<2356:18:52,  8.42s/it][INFO|trainer.py:3510] 2024-08-10 18:15:35,399 >> Saving model checkpoint to ../models/marian/marian_output/checkpoint-8\n",
      "[WARNING|configuration_utils.py:448] 2024-08-10 18:15:35,400 >> Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59421]], 'forced_eos_token_id': 0}\n",
      "[INFO|configuration_utils.py:472] 2024-08-10 18:15:35,402 >> Configuration saved in ../models/marian/marian_output/checkpoint-8/config.json\n",
      "[INFO|configuration_utils.py:807] 2024-08-10 18:15:35,403 >> Configuration saved in ../models/marian/marian_output/checkpoint-8/generation_config.json\n",
      "[INFO|modeling_utils.py:2778] 2024-08-10 18:15:35,910 >> Model weights saved in ../models/marian/marian_output/checkpoint-8/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2684] 2024-08-10 18:15:35,912 >> tokenizer config file saved in ../models/marian/marian_output/checkpoint-8/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2693] 2024-08-10 18:15:35,913 >> Special tokens file saved in ../models/marian/marian_output/checkpoint-8/special_tokens_map.json\n",
      "[INFO|trainer.py:3602] 2024-08-10 18:15:38,330 >> Deleting older checkpoint [../models/marian/marian_output/checkpoint-5] due to args.save_total_limit\n",
      "{'loss': 5.4051, 'grad_norm': 19.028871536254883, 'learning_rate': 4.5e-05, 'epoch': 0.01}\n",
      "  0%|                                  | 9/1008000 [01:12<2447:39:29,  8.74s/it][INFO|trainer.py:3510] 2024-08-10 18:15:44,724 >> Saving model checkpoint to ../models/marian/marian_output/checkpoint-9\n",
      "[WARNING|configuration_utils.py:448] 2024-08-10 18:15:44,725 >> Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59421]], 'forced_eos_token_id': 0}\n",
      "[INFO|configuration_utils.py:472] 2024-08-10 18:15:44,726 >> Configuration saved in ../models/marian/marian_output/checkpoint-9/config.json\n",
      "[INFO|configuration_utils.py:807] 2024-08-10 18:15:44,727 >> Configuration saved in ../models/marian/marian_output/checkpoint-9/generation_config.json\n",
      "[INFO|modeling_utils.py:2778] 2024-08-10 18:15:45,254 >> Model weights saved in ../models/marian/marian_output/checkpoint-9/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2684] 2024-08-10 18:15:45,255 >> tokenizer config file saved in ../models/marian/marian_output/checkpoint-9/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2693] 2024-08-10 18:15:45,255 >> Special tokens file saved in ../models/marian/marian_output/checkpoint-9/special_tokens_map.json\n",
      "[INFO|trainer.py:3602] 2024-08-10 18:15:47,911 >> Deleting older checkpoint [../models/marian/marian_output/checkpoint-6] due to args.save_total_limit\n",
      "{'loss': 5.3605, 'grad_norm': 17.875436782836914, 'learning_rate': 5e-05, 'epoch': 0.01}\n",
      "  0%|                                 | 10/1008000 [01:22<2522:22:04,  9.01s/it][INFO|trainer.py:3510] 2024-08-10 18:15:54,330 >> Saving model checkpoint to ../models/marian/marian_output/checkpoint-10\n",
      "[WARNING|configuration_utils.py:448] 2024-08-10 18:15:54,331 >> Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59421]], 'forced_eos_token_id': 0}\n",
      "[INFO|configuration_utils.py:472] 2024-08-10 18:15:54,333 >> Configuration saved in ../models/marian/marian_output/checkpoint-10/config.json\n",
      "[INFO|configuration_utils.py:807] 2024-08-10 18:15:54,334 >> Configuration saved in ../models/marian/marian_output/checkpoint-10/generation_config.json\n",
      "[INFO|modeling_utils.py:2778] 2024-08-10 18:15:54,754 >> Model weights saved in ../models/marian/marian_output/checkpoint-10/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2684] 2024-08-10 18:15:54,755 >> tokenizer config file saved in ../models/marian/marian_output/checkpoint-10/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2693] 2024-08-10 18:15:54,755 >> Special tokens file saved in ../models/marian/marian_output/checkpoint-10/special_tokens_map.json\n",
      "[INFO|trainer.py:3602] 2024-08-10 18:15:57,074 >> Deleting older checkpoint [../models/marian/marian_output/checkpoint-7] due to args.save_total_limit\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "%cd ../transformers\n",
    "!python examples/pytorch/translation/run_translation.py \\\n",
    "--per_device_train_batch_size {per_device_train_batch_size} \\\n",
    "--use_cpu {use_cpu} \\\n",
    "--save_steps {save_steps} \\\n",
    "--num_train_epochs {num_train_epochs} \\\n",
    "--logging_steps {logging_steps} \\\n",
    "--label_smoothing_factor {label_smoothing_factor} \\\n",
    "--learning_rate {learning_rate} \\\n",
    "--run_name {run_name} \\\n",
    "--output_dir {output_dir} \\\n",
    "--logging_dir {logging_dir} \\\n",
    "--eval_steps {eval_steps} \\\n",
    "--gradient_accumulation_steps {gradient_accumulation_steps} \\\n",
    "--model_name_or_path  {model_name_or_path } \\\n",
    "--dataset_name  {dataset_name } \\\n",
    "--generation_max_length {generation_max_length} \\\n",
    "--generation_num_beams {generation_num_beams} \\\n",
    "--source_lang {source_lang} \\\n",
    "--target_lang {target_lang} \\\n",
    "--dataset_config_name {dataset_config_name} \\\n",
    "--predict_with_generate {predict_with_generate} \\\n",
    "--max_source_length {max_source_length} \\\n",
    "--dataloader_drop_last {dataloader_drop_last} \\\n",
    "--warmup_steps {warmup_steps} \\\n",
    "--weight_decay {weight_decay} \\\n",
    "--save_total_limit {save_total_limit} \\\n",
    "--seed {seed} \\\n",
    "--overwrite_output_dir {overwrite_output_dir} \\\n",
    "--jit_mode_eval {jit_mode_eval} \\\n",
    "--do_eval {do_eval} \\\n",
    "--do_predict {do_predict} \\\n",
    "--do_train {do_train} \\\n",
    "--fp16 {fp16} \\\n",
    "--fp16_backend {fp16_backend} \\\n",
    "--fp16_full_eval {fp16_full_eval} \\\n",
    "--full_determinism {full_determinism}\n",
    "# --resume_from_checkpoint {resume_from_checkpoint} \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zindi-z3yfXQo9-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
