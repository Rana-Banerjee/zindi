{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/zindi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.cache/pypoetry/virtualenvs/zindi-LtLKIbXv-py3.9/lib/python3.9/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "# %cd /home/rana/Projects/zindi\n",
    "%cd /root/zindi/\n",
    "import yaml\n",
    "with open('common/config.yaml', 'r') as f:\n",
    "    config = yaml.load(f, Loader=yaml.SafeLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#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\n",
    "# !huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Device specific params\n",
    "import os\n",
    "os.environ['model_name_or_path'] = \"Helsinki-NLP/opus-mt-af-fr\"\n",
    "os.environ['per_device_eval_batch_size']=\"320\"\n",
    "os.environ['per_device_train_batch_size']=\"320\"\n",
    "# use_cpu=False\n",
    "os.environ['save_steps']=\"250\"\n",
    "os.environ['num_train_epochs']=\"10000\"\n",
    "os.environ['logging_steps']=\"10\"\n",
    "os.environ['label_smoothing_factor']=\"0.00001\"\n",
    "os.environ['learning_rate']=\"5e-04\"\n",
    "os.environ['gradient_accumulation_steps']=\"8\"\n",
    "os.environ['generation_max_length']=\"150\"\n",
    "os.environ['generation_num_beams']=\"1\"\n",
    "os.environ['max_source_length']= \"150\"\n",
    "os.environ['warmup_steps']=\"10\"\n",
    "os.environ['weight_decay']=\"0.00001\"\n",
    "os.environ['seed']=\"42\"\n",
    "os.environ['eval_steps']=\"50\"\n",
    "os.environ['save_total_limit']=\"3\"\n",
    "os.environ['overwrite_output_dir']=\"True\"\n",
    "os.environ['fp16']=\"True\"\n",
    "os.environ['fp16_backend']=\"auto\"\n",
    "os.environ['fp16_full_eval']=\"True\"\n",
    "os.environ['full_determinism']=\"True\"\n",
    "os.environ['run_name']=\"marian-1\"\n",
    "os.environ['output_dir']=\"models/marian/marian_output\"\n",
    "os.environ['logging_dir']=\"models/marian/logs\"\n",
    "\n",
    "os.environ['dataset_name'] = \"uvci/Koumankan_mt_dyu_fr\"\n",
    "os.environ['source_lang']=\"dyu\"\n",
    "os.environ['target_lang']=\"fr\"\n",
    "os.environ['dataset_config_name']= \"default\"\n",
    "os.environ['predict_with_generate']=\"True\"\n",
    "os.environ['dataloader_drop_last']=\"True\"\n",
    "\n",
    "os.environ['jit_mode_eval']=\"False\"\n",
    "os.environ['do_eval']=\"True\"\n",
    "os.environ['do_predict']=\"False\"\n",
    "os.environ['do_train']=\"True\"\n",
    "\n",
    "\n",
    "# model_name_or_path = \"Helsinki-NLP/opus-mt-af-fr\"\n",
    "# per_device_eval_batch_size=1\n",
    "# per_device_train_batch_size=1\n",
    "# # use_cpu=False\n",
    "# save_steps=2000\n",
    "# num_train_epochs=10000.0\n",
    "# logging_steps=10\n",
    "# label_smoothing_factor=0.00001\n",
    "# learning_rate=5e-04\n",
    "# gradient_accumulation_steps=8\n",
    "# generation_max_length=150\n",
    "# generation_num_beams=1\n",
    "# max_source_length= 150\n",
    "# warmup_steps=10\n",
    "# weight_decay=0.00001\n",
    "# seed=42\n",
    "# eval_steps=50\n",
    "# save_total_limit=5\n",
    "# overwrite_output_dir=\"True\"\n",
    "# fp16=\"True\"\n",
    "# fp16_backend=\"auto\"\n",
    "# fp16_full_eval=\"True\"\n",
    "# full_determinism=\"True\"\n",
    "# run_name=\"marian-1\"\n",
    "# output_dir=\"../models/marian/marian_output\"\n",
    "\n",
    "# dataset_name = \"uvci/Koumankan_mt_dyu_fr\"\n",
    "# source_lang=\"dyu\"\n",
    "# target_lang=\"fr\"\n",
    "# dataset_config_name= \"default\"\n",
    "# predict_with_generate=\"True\"\n",
    "# dataloader_drop_last=\"True\"\n",
    "\n",
    "# jit_mode_eval=\"False\"\n",
    "# do_eval=\"True\"\n",
    "# do_predict=\"False\"\n",
    "# do_train=\"True\"\n",
    "\n",
    "# Seq2SeqTrainingArguments(\n",
    "# _n_gpu=0,\n",
    "# accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
    "# adafactor=False,\n",
    "# adam_beta1=0.9,\n",
    "# adam_beta2=0.999,\n",
    "# adam_epsilon=1e-08,\n",
    "# auto_find_batch_size=False,\n",
    "# batch_eval_metrics=False,\n",
    "# bf16=False,\n",
    "# bf16_full_eval=False,\n",
    "# data_seed=None,\n",
    "# dataloader_num_workers=0,\n",
    "# dataloader_persistent_workers=False,\n",
    "# dataloader_pin_memory=True,\n",
    "# dataloader_prefetch_factor=None,\n",
    "# ddp_backend=None,\n",
    "# ddp_broadcast_buffers=None,\n",
    "# ddp_bucket_cap_mb=None,\n",
    "# ddp_find_unused_parameters=None,\n",
    "# ddp_timeout=1800,\n",
    "# debug=[],\n",
    "# deepspeed=None,\n",
    "# disable_tqdm=False,\n",
    "# dispatch_batches=None,\n",
    "# eval_accumulation_steps=None,\n",
    "# eval_delay=0,\n",
    "# eval_do_concat_batches=True,\n",
    "# eval_on_start=False,\n",
    "# eval_strategy=no,\n",
    "# eval_use_gather_object=False,\n",
    "# evaluation_strategy=None,\n",
    "# fp16_opt_level=O1,\n",
    "# fsdp=[],\n",
    "# fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
    "# fsdp_min_num_params=0,\n",
    "# fsdp_transformer_layer_cls_to_wrap=None,\n",
    "# gradient_checkpointing=False,\n",
    "# gradient_checkpointing_kwargs=None,\n",
    "# greater_is_better=None,\n",
    "# group_by_length=False,\n",
    "# half_precision_backend=auto,\n",
    "# hub_always_push=False,\n",
    "# hub_model_id=None,\n",
    "# hub_private_repo=False,\n",
    "# hub_strategy=every_save,\n",
    "# hub_token=<HUB_TOKEN>,\n",
    "# ignore_data_skip=False,\n",
    "# include_inputs_for_metrics=False,\n",
    "# include_num_input_tokens_seen=False,\n",
    "# include_tokens_per_second=False,\n",
    "# label_names=None,\n",
    "# length_column_name=length,\n",
    "# load_best_model_at_end=False,\n",
    "# local_rank=0,\n",
    "# log_level=passive,\n",
    "# log_level_replica=warning,\n",
    "# log_on_each_node=True,\n",
    "# logging_first_step=False,\n",
    "# logging_nan_inf_filter=True,\n",
    "# logging_strategy=steps,\n",
    "# lr_scheduler_kwargs={},\n",
    "# lr_scheduler_type=linear,\n",
    "# max_grad_norm=1.0,\n",
    "# max_steps=-1,\n",
    "# metric_for_best_model=None,\n",
    "# mp_parameters=,\n",
    "# neftune_noise_alpha=None,\n",
    "# no_cuda=False,\n",
    "# optim=adamw_torch,\n",
    "# optim_args=None,\n",
    "# optim_target_modules=None,\n",
    "# past_index=-1,\n",
    "# predict_with_generate=True,\n",
    "# prediction_loss_only=False,\n",
    "# push_to_hub=False,\n",
    "# push_to_hub_model_id=None,\n",
    "# push_to_hub_organization=None,\n",
    "# push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
    "# ray_scope=last,\n",
    "# remove_unused_columns=True,\n",
    "# report_to=[],\n",
    "# restore_callback_states_from_checkpoint=False,\n",
    "# save_on_each_node=False,\n",
    "# save_only_model=False,\n",
    "# save_safetensors=True,\n",
    "# save_strategy=steps,\n",
    "# skip_memory_metrics=True,\n",
    "# sortish_sampler=False,\n",
    "# split_batches=None,\n",
    "# tf32=None,\n",
    "# torch_compile=False,\n",
    "# torch_compile_backend=None,\n",
    "# torch_compile_mode=None,\n",
    "# torch_empty_cache_steps=None,\n",
    "# torchdynamo=None,\n",
    "# tpu_metrics_debug=False,\n",
    "# tpu_num_cores=None,\n",
    "# use_ipex=False,\n",
    "# use_legacy_prediction_loop=False,\n",
    "# use_mps_device=False,\n",
    "# warmup_ratio=0.0,\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_path = \"models/marian/marian_output/base_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/zindi/transformers/src/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('models/marian/marian_output/base_model/tokenizer_config.json',\n",
       " 'models/marian/marian_output/base_model/special_tokens_map.json',\n",
       " 'models/marian/marian_output/base_model/vocab.json',\n",
       " 'models/marian/marian_output/base_model/source.spm',\n",
       " 'models/marian/marian_output/base_model/target.spm',\n",
       " 'models/marian/marian_output/base_model/added_tokens.json')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save tokenizer in base_path\n",
    "from transformers import MarianTokenizer\n",
    "tokenizer = MarianTokenizer.from_pretrained(os.environ['model_name_or_path'])\n",
    "tokenizer.save_pretrained(base_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59421]], 'forced_eos_token_id': 0}\n",
      "Some weights of MarianMTModel were not initialized from the model checkpoint at models/marian/marian_output/base_model and are newly initialized because the shapes did not match:\n",
      "- model.decoder.layers.0.fc1.bias: found shape torch.Size([2048]) in the checkpoint and torch.Size([1024]) in the model instantiated\n",
      "- model.decoder.layers.0.fc1.weight: found shape torch.Size([2048, 512]) in the checkpoint and torch.Size([1024, 512]) in the model instantiated\n",
      "- model.decoder.layers.0.fc2.weight: found shape torch.Size([512, 2048]) in the checkpoint and torch.Size([512, 1024]) in the model instantiated\n",
      "- model.decoder.layers.1.fc1.bias: found shape torch.Size([2048]) in the checkpoint and torch.Size([1024]) in the model instantiated\n",
      "- model.decoder.layers.1.fc1.weight: found shape torch.Size([2048, 512]) in the checkpoint and torch.Size([1024, 512]) in the model instantiated\n",
      "- model.decoder.layers.1.fc2.weight: found shape torch.Size([512, 2048]) in the checkpoint and torch.Size([512, 1024]) in the model instantiated\n",
      "- model.decoder.layers.2.fc1.bias: found shape torch.Size([2048]) in the checkpoint and torch.Size([1024]) in the model instantiated\n",
      "- model.decoder.layers.2.fc1.weight: found shape torch.Size([2048, 512]) in the checkpoint and torch.Size([1024, 512]) in the model instantiated\n",
      "- model.decoder.layers.2.fc2.weight: found shape torch.Size([512, 2048]) in the checkpoint and torch.Size([512, 1024]) in the model instantiated\n",
      "- model.decoder.layers.3.fc1.bias: found shape torch.Size([2048]) in the checkpoint and torch.Size([1024]) in the model instantiated\n",
      "- model.decoder.layers.3.fc1.weight: found shape torch.Size([2048, 512]) in the checkpoint and torch.Size([1024, 512]) in the model instantiated\n",
      "- model.decoder.layers.3.fc2.weight: found shape torch.Size([512, 2048]) in the checkpoint and torch.Size([512, 1024]) in the model instantiated\n",
      "- model.decoder.layers.4.fc1.bias: found shape torch.Size([2048]) in the checkpoint and torch.Size([1024]) in the model instantiated\n",
      "- model.decoder.layers.4.fc1.weight: found shape torch.Size([2048, 512]) in the checkpoint and torch.Size([1024, 512]) in the model instantiated\n",
      "- model.decoder.layers.4.fc2.weight: found shape torch.Size([512, 2048]) in the checkpoint and torch.Size([512, 1024]) in the model instantiated\n",
      "- model.decoder.layers.5.fc1.bias: found shape torch.Size([2048]) in the checkpoint and torch.Size([1024]) in the model instantiated\n",
      "- model.decoder.layers.5.fc1.weight: found shape torch.Size([2048, 512]) in the checkpoint and torch.Size([1024, 512]) in the model instantiated\n",
      "- model.decoder.layers.5.fc2.weight: found shape torch.Size([512, 2048]) in the checkpoint and torch.Size([512, 1024]) in the model instantiated\n",
      "- model.encoder.layers.0.fc1.bias: found shape torch.Size([2048]) in the checkpoint and torch.Size([1024]) in the model instantiated\n",
      "- model.encoder.layers.0.fc1.weight: found shape torch.Size([2048, 512]) in the checkpoint and torch.Size([1024, 512]) in the model instantiated\n",
      "- model.encoder.layers.0.fc2.weight: found shape torch.Size([512, 2048]) in the checkpoint and torch.Size([512, 1024]) in the model instantiated\n",
      "- model.encoder.layers.1.fc1.bias: found shape torch.Size([2048]) in the checkpoint and torch.Size([1024]) in the model instantiated\n",
      "- model.encoder.layers.1.fc1.weight: found shape torch.Size([2048, 512]) in the checkpoint and torch.Size([1024, 512]) in the model instantiated\n",
      "- model.encoder.layers.1.fc2.weight: found shape torch.Size([512, 2048]) in the checkpoint and torch.Size([512, 1024]) in the model instantiated\n",
      "- model.encoder.layers.2.fc1.bias: found shape torch.Size([2048]) in the checkpoint and torch.Size([1024]) in the model instantiated\n",
      "- model.encoder.layers.2.fc1.weight: found shape torch.Size([2048, 512]) in the checkpoint and torch.Size([1024, 512]) in the model instantiated\n",
      "- model.encoder.layers.2.fc2.weight: found shape torch.Size([512, 2048]) in the checkpoint and torch.Size([512, 1024]) in the model instantiated\n",
      "- model.encoder.layers.3.fc1.bias: found shape torch.Size([2048]) in the checkpoint and torch.Size([1024]) in the model instantiated\n",
      "- model.encoder.layers.3.fc1.weight: found shape torch.Size([2048, 512]) in the checkpoint and torch.Size([1024, 512]) in the model instantiated\n",
      "- model.encoder.layers.3.fc2.weight: found shape torch.Size([512, 2048]) in the checkpoint and torch.Size([512, 1024]) in the model instantiated\n",
      "- model.encoder.layers.4.fc1.bias: found shape torch.Size([2048]) in the checkpoint and torch.Size([1024]) in the model instantiated\n",
      "- model.encoder.layers.4.fc1.weight: found shape torch.Size([2048, 512]) in the checkpoint and torch.Size([1024, 512]) in the model instantiated\n",
      "- model.encoder.layers.4.fc2.weight: found shape torch.Size([512, 2048]) in the checkpoint and torch.Size([512, 1024]) in the model instantiated\n",
      "- model.encoder.layers.5.fc1.bias: found shape torch.Size([2048]) in the checkpoint and torch.Size([1024]) in the model instantiated\n",
      "- model.encoder.layers.5.fc1.weight: found shape torch.Size([2048, 512]) in the checkpoint and torch.Size([1024, 512]) in the model instantiated\n",
      "- model.encoder.layers.5.fc2.weight: found shape torch.Size([512, 2048]) in the checkpoint and torch.Size([512, 1024]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'bad_words_ids': [[59421]], 'forced_eos_token_id': 0}\n"
     ]
    }
   ],
   "source": [
    "### Update model config:\n",
    "import json\n",
    "\n",
    "# Load model:\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(os.environ['model_name_or_path'] )\n",
    "# Save model\n",
    "model.save_pretrained(base_model_path)\n",
    "# Update config\n",
    "config_path = base_model_path+'/config.json'\n",
    "with open(config_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "data['decoder_attention_heads']=4\n",
    "data['decoder_ffn_dim']=1024\n",
    "data['decoder_layers']=6\n",
    "data['decoder_vocab_size']=59422\n",
    "data['dropout']=0.3\n",
    "data['encoder_attention_heads']=4\n",
    "data['encoder_ffn_dim']=1024\n",
    "data['encoder_layers']=6\n",
    "data['max_length']=512\n",
    "data['max_position_embeddings']=512\n",
    "data['num_beams']=1\n",
    "data['num_hidden_layers']=6\n",
    "data['torch_dtype']=\"float32\"\n",
    "data['vocab_size']=59422\n",
    "\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(data, f, indent=2)\n",
    "\n",
    "# Update model_name_or_path\n",
    "model_name_or_path = base_model_path\n",
    "os.environ['model_name_or_path'] = base_model_path\n",
    "\n",
    "# Load model with updated config and save it\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name_or_path, ignore_mismatched_sizes=True)\n",
    "# Save model\n",
    "model.save_pretrained(base_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GenerationConfig\n",
    "\n",
    "translation_generation_config = GenerationConfig(\n",
    "    num_beams=1,\n",
    "    max_length=128\n",
    ")\n",
    "translation_generation_config.save_pretrained(base_model_path, \"translation_generation_config.json\")\n",
    "os.environ['generation_config']=base_model_path+\"/translation_generation_config.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/13/2024 09:18:36 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: True\n",
      "08/13/2024 09:18:36 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n",
      "_n_gpu=1,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=True,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_on_start=False,\n",
      "eval_steps=50.0,\n",
      "eval_strategy=no,\n",
      "eval_use_gather_object=False,\n",
      "evaluation_strategy=None,\n",
      "fp16=True,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=True,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=True,\n",
      "generation_config=models/marian/marian_output/base_model/translation_generation_config.json,\n",
      "generation_max_length=150,\n",
      "generation_num_beams=1,\n",
      "gradient_accumulation_steps=8,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=1e-05,\n",
      "learning_rate=0.0005,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/marian/logs,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=10,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=10000.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=../models/marian/marian_output,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=320,\n",
      "predict_with_generate=True,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=marian-1,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=500,\n",
      "save_strategy=steps,\n",
      "save_total_limit=5,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "sortish_sampler=False,\n",
      "split_batches=None,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torch_empty_cache_steps=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=10,\n",
      "weight_decay=1e-05,\n",
      ")\n",
      "Overwrite dataset info from restored data version if exists.\n",
      "08/13/2024 09:18:39 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
      "Loading Dataset info from /root/.cache/huggingface/datasets/uvci___koumankan_mt_dyu_fr/default/0.0.0/5207d8690e8c37ec07a1954dc7441c348f8242fa\n",
      "08/13/2024 09:18:39 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/uvci___koumankan_mt_dyu_fr/default/0.0.0/5207d8690e8c37ec07a1954dc7441c348f8242fa\n",
      "Found cached dataset koumankan_mt_dyu_fr (/root/.cache/huggingface/datasets/uvci___koumankan_mt_dyu_fr/default/0.0.0/5207d8690e8c37ec07a1954dc7441c348f8242fa)\n",
      "08/13/2024 09:18:39 - INFO - datasets.builder - Found cached dataset koumankan_mt_dyu_fr (/root/.cache/huggingface/datasets/uvci___koumankan_mt_dyu_fr/default/0.0.0/5207d8690e8c37ec07a1954dc7441c348f8242fa)\n",
      "Loading Dataset info from /root/.cache/huggingface/datasets/uvci___koumankan_mt_dyu_fr/default/0.0.0/5207d8690e8c37ec07a1954dc7441c348f8242fa\n",
      "08/13/2024 09:18:39 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/uvci___koumankan_mt_dyu_fr/default/0.0.0/5207d8690e8c37ec07a1954dc7441c348f8242fa\n",
      "[INFO|configuration_utils.py:731] 2024-08-13 09:18:39,391 >> loading configuration file models/marian/marian_output/base_model/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-08-13 09:18:39,395 >> Model config MarianConfig {\n",
      "  \"_name_or_path\": \"models/marian/marian_output/base_model\",\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      59421\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 4,\n",
      "  \"decoder_ffn_dim\": 1024,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 59421,\n",
      "  \"decoder_vocab_size\": 59422,\n",
      "  \"dropout\": 0.3,\n",
      "  \"encoder_attention_heads\": 4,\n",
      "  \"encoder_ffn_dim\": 1024,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 59421,\n",
      "  \"scale_embedding\": true,\n",
      "  \"share_encoder_decoder_embeddings\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.45.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 59422\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2267] 2024-08-13 09:18:39,397 >> loading file source.spm\n",
      "[INFO|tokenization_utils_base.py:2267] 2024-08-13 09:18:39,397 >> loading file target.spm\n",
      "[INFO|tokenization_utils_base.py:2267] 2024-08-13 09:18:39,397 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:2267] 2024-08-13 09:18:39,397 >> loading file target_vocab.json\n",
      "[INFO|tokenization_utils_base.py:2267] 2024-08-13 09:18:39,397 >> loading file tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2267] 2024-08-13 09:18:39,397 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2267] 2024-08-13 09:18:39,397 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2267] 2024-08-13 09:18:39,397 >> loading file tokenizer.json\n",
      "[INFO|modeling_utils.py:3654] 2024-08-13 09:18:39,894 >> loading weights file models/marian/marian_output/base_model/model.safetensors\n",
      "[INFO|configuration_utils.py:1038] 2024-08-13 09:18:39,902 >> Generate config GenerationConfig {\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      59421\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 59421,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"max_length\": 512,\n",
      "  \"pad_token_id\": 59421\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:4489] 2024-08-13 09:18:41,019 >> All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "[INFO|modeling_utils.py:4497] 2024-08-13 09:18:41,019 >> All the weights of MarianMTModel were initialized from the model checkpoint at models/marian/marian_output/base_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "[INFO|configuration_utils.py:991] 2024-08-13 09:18:41,021 >> loading configuration file models/marian/marian_output/base_model/generation_config.json\n",
      "[INFO|configuration_utils.py:1038] 2024-08-13 09:18:41,021 >> Generate config GenerationConfig {\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      59421\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 59421,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"max_length\": 512,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 59421,\n",
      "  \"renormalize_logits\": true\n",
      "}\n",
      "\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/uvci___koumankan_mt_dyu_fr/default/0.0.0/5207d8690e8c37ec07a1954dc7441c348f8242fa/cache-6bf484a7222a26ab.arrow\n",
      "08/13/2024 09:18:41 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/uvci___koumankan_mt_dyu_fr/default/0.0.0/5207d8690e8c37ec07a1954dc7441c348f8242fa/cache-6bf484a7222a26ab.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/uvci___koumankan_mt_dyu_fr/default/0.0.0/5207d8690e8c37ec07a1954dc7441c348f8242fa/cache-7007a426aa676333.arrow\n",
      "08/13/2024 09:18:42 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/uvci___koumankan_mt_dyu_fr/default/0.0.0/5207d8690e8c37ec07a1954dc7441c348f8242fa/cache-7007a426aa676333.arrow\n",
      "/root/.cache/pypoetry/virtualenvs/zindi-LtLKIbXv-py3.9/lib/python3.9/site-packages/accelerate/accelerator.py:488: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
      "[INFO|trainer.py:655] 2024-08-13 09:18:43,370 >> Using auto half precision backend\n",
      "[INFO|configuration_utils.py:991] 2024-08-13 09:18:43,371 >> loading configuration file models/marian/marian_output/base_model/translation_generation_config.json\n",
      "[INFO|configuration_utils.py:1038] 2024-08-13 09:18:43,371 >> Generate config GenerationConfig {\n",
      "  \"max_length\": 128\n",
      "}\n",
      "\n",
      "[INFO|trainer.py:2141] 2024-08-13 09:18:43,894 >> ***** Running training *****\n",
      "[INFO|trainer.py:2142] 2024-08-13 09:18:43,894 >>   Num examples = 8,065\n",
      "[INFO|trainer.py:2143] 2024-08-13 09:18:43,894 >>   Num Epochs = 10,000\n",
      "[INFO|trainer.py:2144] 2024-08-13 09:18:43,894 >>   Instantaneous batch size per device = 320\n",
      "[INFO|trainer.py:2147] 2024-08-13 09:18:43,894 >>   Total train batch size (w. parallel, distributed & accumulation) = 2,560\n",
      "[INFO|trainer.py:2148] 2024-08-13 09:18:43,894 >>   Gradient Accumulation steps = 8\n",
      "[INFO|trainer.py:2149] 2024-08-13 09:18:43,894 >>   Total optimization steps = 30,000\n",
      "[INFO|trainer.py:2150] 2024-08-13 09:18:43,895 >>   Number of trainable parameters = 61,967,360\n",
      "{'loss': 8.2719, 'grad_norm': inf, 'learning_rate': 0.00045000000000000004, 'epoch': 3.2}\n",
      "{'loss': 6.7513, 'grad_norm': 3.8495850563049316, 'learning_rate': 0.0004998499499833278, 'epoch': 6.4}\n",
      "{'loss': 6.0438, 'grad_norm': 2.341137170791626, 'learning_rate': 0.0004996832277425809, 'epoch': 9.6}\n",
      "{'loss': 5.4539, 'grad_norm': 3.622429609298706, 'learning_rate': 0.000499516505501834, 'epoch': 12.8}\n",
      "{'loss': 4.8779, 'grad_norm': 1.834934115409851, 'learning_rate': 0.000499349783261087, 'epoch': 16.0}\n",
      "{'loss': 4.3274, 'grad_norm': 2.3658013343811035, 'learning_rate': 0.0004991830610203401, 'epoch': 19.2}\n",
      "{'loss': 3.8459, 'grad_norm': 2.655965805053711, 'learning_rate': 0.0004990163387795932, 'epoch': 22.4}\n",
      "{'loss': 3.374, 'grad_norm': 1.4833061695098877, 'learning_rate': 0.0004988496165388463, 'epoch': 25.6}\n",
      "{'loss': 2.9078, 'grad_norm': 1.7301448583602905, 'learning_rate': 0.0004986828942980994, 'epoch': 28.8}\n",
      "{'loss': 2.4795, 'grad_norm': 1.4129297733306885, 'learning_rate': 0.0004985161720573525, 'epoch': 32.0}\n",
      "{'loss': 2.1057, 'grad_norm': 1.0435816049575806, 'learning_rate': 0.0004983494498166055, 'epoch': 35.2}\n",
      "{'loss': 1.8053, 'grad_norm': 1.353895664215088, 'learning_rate': 0.0004981827275758586, 'epoch': 38.4}\n",
      "{'loss': 1.5519, 'grad_norm': 1.1665349006652832, 'learning_rate': 0.0004980160053351117, 'epoch': 41.6}\n",
      "{'loss': 1.3532, 'grad_norm': 1.090944766998291, 'learning_rate': 0.0004978492830943648, 'epoch': 44.8}\n",
      "{'loss': 1.1814, 'grad_norm': 1.2475641965866089, 'learning_rate': 0.0004976825608536179, 'epoch': 48.0}\n",
      "{'loss': 1.0406, 'grad_norm': 0.9118926525115967, 'learning_rate': 0.000497515838612871, 'epoch': 51.2}\n",
      "{'loss': 0.917, 'grad_norm': 1.0402618646621704, 'learning_rate': 0.0004973491163721241, 'epoch': 54.4}\n",
      "{'loss': 0.8089, 'grad_norm': 0.9511024355888367, 'learning_rate': 0.0004971823941313772, 'epoch': 57.6}\n",
      "{'loss': 0.7137, 'grad_norm': 0.9443137645721436, 'learning_rate': 0.0004970156718906302, 'epoch': 60.8}\n",
      "{'loss': 0.6369, 'grad_norm': 0.9622547030448914, 'learning_rate': 0.0004968489496498833, 'epoch': 64.0}\n",
      "{'loss': 0.5625, 'grad_norm': 0.8395998477935791, 'learning_rate': 0.0004966822274091364, 'epoch': 67.2}\n",
      "{'loss': 0.4981, 'grad_norm': 0.6484025716781616, 'learning_rate': 0.0004965155051683894, 'epoch': 70.4}\n",
      "{'loss': 0.4398, 'grad_norm': 0.6910131573677063, 'learning_rate': 0.0004963487829276426, 'epoch': 73.6}\n",
      "{'loss': 0.3951, 'grad_norm': 0.6167826652526855, 'learning_rate': 0.0004961820606868957, 'epoch': 76.8}\n",
      "{'loss': 0.352, 'grad_norm': 0.622724175453186, 'learning_rate': 0.0004960153384461488, 'epoch': 80.0}\n",
      "{'loss': 0.3147, 'grad_norm': 0.6139539480209351, 'learning_rate': 0.0004958486162054018, 'epoch': 83.2}\n",
      "{'loss': 0.2808, 'grad_norm': 0.5546964406967163, 'learning_rate': 0.0004956818939646549, 'epoch': 86.4}\n",
      "{'loss': 0.2549, 'grad_norm': 0.5292230248451233, 'learning_rate': 0.000495515171723908, 'epoch': 89.6}\n",
      "{'loss': 0.2305, 'grad_norm': 0.5014447569847107, 'learning_rate': 0.000495348449483161, 'epoch': 92.8}\n",
      "{'loss': 0.2079, 'grad_norm': 0.48351946473121643, 'learning_rate': 0.0004951817272424142, 'epoch': 96.0}\n",
      "{'loss': 0.19, 'grad_norm': 0.5746673345565796, 'learning_rate': 0.0004950150050016673, 'epoch': 99.2}\n",
      "{'loss': 0.1716, 'grad_norm': 0.4609386920928955, 'learning_rate': 0.0004948482827609204, 'epoch': 102.4}\n",
      "{'loss': 0.1582, 'grad_norm': 0.4565381407737732, 'learning_rate': 0.0004946815605201734, 'epoch': 105.6}\n",
      "{'loss': 0.1443, 'grad_norm': 0.42363104224205017, 'learning_rate': 0.0004945148382794265, 'epoch': 108.8}\n",
      "{'loss': 0.1343, 'grad_norm': 0.4157748818397522, 'learning_rate': 0.0004943481160386796, 'epoch': 112.0}\n",
      "{'loss': 0.1243, 'grad_norm': 0.4006548523902893, 'learning_rate': 0.0004941813937979326, 'epoch': 115.2}\n",
      "{'loss': 0.1157, 'grad_norm': 0.3919816017150879, 'learning_rate': 0.0004940146715571857, 'epoch': 118.4}\n",
      "{'loss': 0.108, 'grad_norm': 0.3993150293827057, 'learning_rate': 0.0004938479493164389, 'epoch': 121.6}\n",
      "{'loss': 0.0996, 'grad_norm': 0.34205082058906555, 'learning_rate': 0.000493681227075692, 'epoch': 124.8}\n",
      "{'loss': 0.0948, 'grad_norm': 0.3829672038555145, 'learning_rate': 0.000493514504834945, 'epoch': 128.0}\n",
      "{'loss': 0.0884, 'grad_norm': 0.32587340474128723, 'learning_rate': 0.0004933477825941981, 'epoch': 131.2}\n",
      "{'loss': 0.0821, 'grad_norm': 0.3042207956314087, 'learning_rate': 0.0004931810603534512, 'epoch': 134.4}\n",
      "{'loss': 0.0783, 'grad_norm': 0.30302053689956665, 'learning_rate': 0.0004930143381127042, 'epoch': 137.6}\n",
      "{'loss': 0.0727, 'grad_norm': 0.30608028173446655, 'learning_rate': 0.0004928476158719573, 'epoch': 140.8}\n",
      "{'loss': 0.0689, 'grad_norm': 0.3222033381462097, 'learning_rate': 0.0004926808936312105, 'epoch': 144.0}\n",
      "{'loss': 0.0679, 'grad_norm': 0.3067787289619446, 'learning_rate': 0.0004925141713904636, 'epoch': 147.2}\n",
      "{'loss': 0.0648, 'grad_norm': 0.30298131704330444, 'learning_rate': 0.0004923474491497166, 'epoch': 150.4}\n",
      "{'loss': 0.0608, 'grad_norm': 0.28183040022850037, 'learning_rate': 0.0004921807269089697, 'epoch': 153.6}\n",
      "{'loss': 0.0584, 'grad_norm': 0.2855949401855469, 'learning_rate': 0.0004920140046682228, 'epoch': 156.8}\n",
      "{'loss': 0.0555, 'grad_norm': 0.2790936827659607, 'learning_rate': 0.0004918472824274758, 'epoch': 160.0}\n",
      "  2%|▌                                   | 500/30000 [27:02<26:26:57,  3.23s/it][INFO|trainer.py:3510] 2024-08-13 09:45:46,003 >> Saving model checkpoint to ../models/marian/marian_output/checkpoint-500\n",
      "[WARNING|configuration_utils.py:448] 2024-08-13 09:45:46,003 >> Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'bad_words_ids': [[59421]], 'forced_eos_token_id': 0}\n",
      "[INFO|configuration_utils.py:472] 2024-08-13 09:45:46,004 >> Configuration saved in ../models/marian/marian_output/checkpoint-500/config.json\n",
      "[INFO|configuration_utils.py:807] 2024-08-13 09:45:46,005 >> Configuration saved in ../models/marian/marian_output/checkpoint-500/generation_config.json\n",
      "[INFO|modeling_utils.py:2778] 2024-08-13 09:45:46,420 >> Model weights saved in ../models/marian/marian_output/checkpoint-500/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2684] 2024-08-13 09:45:46,421 >> tokenizer config file saved in ../models/marian/marian_output/checkpoint-500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2693] 2024-08-13 09:45:46,421 >> Special tokens file saved in ../models/marian/marian_output/checkpoint-500/special_tokens_map.json\n",
      "{'loss': 0.0526, 'grad_norm': 0.2552623748779297, 'learning_rate': 0.0004916805601867289, 'epoch': 163.2}\n",
      "{'loss': 0.05, 'grad_norm': 0.2664744257926941, 'learning_rate': 0.000491513837945982, 'epoch': 166.4}\n",
      "{'loss': 0.0482, 'grad_norm': 0.24649611115455627, 'learning_rate': 0.0004913471157052352, 'epoch': 169.6}\n",
      "{'loss': 0.0462, 'grad_norm': 0.23577572405338287, 'learning_rate': 0.0004911803934644881, 'epoch': 172.8}\n",
      "{'loss': 0.0441, 'grad_norm': 0.22032149136066437, 'learning_rate': 0.0004910136712237412, 'epoch': 176.0}\n",
      "{'loss': 0.043, 'grad_norm': 0.23267927765846252, 'learning_rate': 0.0004908469489829944, 'epoch': 179.2}\n",
      "{'loss': 0.0411, 'grad_norm': 0.22190427780151367, 'learning_rate': 0.0004906802267422474, 'epoch': 182.4}\n",
      "{'loss': 0.0394, 'grad_norm': 0.22091424465179443, 'learning_rate': 0.0004905135045015005, 'epoch': 185.6}\n",
      "{'loss': 0.0381, 'grad_norm': 0.22004473209381104, 'learning_rate': 0.0004903467822607536, 'epoch': 188.8}\n",
      "{'loss': 0.0377, 'grad_norm': 0.23157592117786407, 'learning_rate': 0.0004901800600200067, 'epoch': 192.0}\n",
      "{'loss': 0.0364, 'grad_norm': 0.2048962414264679, 'learning_rate': 0.0004900133377792597, 'epoch': 195.2}\n",
      "{'loss': 0.0347, 'grad_norm': 0.20869643986225128, 'learning_rate': 0.0004898466155385128, 'epoch': 198.4}\n",
      "{'loss': 0.0334, 'grad_norm': 0.20095935463905334, 'learning_rate': 0.0004896798932977659, 'epoch': 201.6}\n",
      "{'loss': 0.0323, 'grad_norm': 0.20069436728954315, 'learning_rate': 0.000489513171057019, 'epoch': 204.8}\n",
      "{'loss': 0.0315, 'grad_norm': 0.1991214156150818, 'learning_rate': 0.0004893464488162721, 'epoch': 208.0}\n",
      "{'loss': 0.0306, 'grad_norm': 0.2008180022239685, 'learning_rate': 0.0004891797265755252, 'epoch': 211.2}\n",
      "{'loss': 0.0296, 'grad_norm': 0.19403889775276184, 'learning_rate': 0.0004890130043347783, 'epoch': 214.4}\n",
      "{'loss': 0.0293, 'grad_norm': 0.18595950305461884, 'learning_rate': 0.0004888462820940313, 'epoch': 217.6}\n",
      "{'loss': 0.0289, 'grad_norm': 0.20910343527793884, 'learning_rate': 0.0004886795598532844, 'epoch': 220.8}\n",
      "{'loss': 0.0284, 'grad_norm': 0.1936698853969574, 'learning_rate': 0.0004885128376125375, 'epoch': 224.0}\n",
      "{'loss': 0.0273, 'grad_norm': 0.18738695979118347, 'learning_rate': 0.0004883461153717906, 'epoch': 227.2}\n",
      "{'loss': 0.0261, 'grad_norm': 0.1818099170923233, 'learning_rate': 0.0004881793931310437, 'epoch': 230.4}\n",
      "{'loss': 0.0247, 'grad_norm': 0.18158867955207825, 'learning_rate': 0.0004880126708902968, 'epoch': 233.6}\n",
      "  2%|▉                                   | 737/30000 [39:49<26:00:54,  3.20s/it]"
     ]
    }
   ],
   "source": [
    "!python transformers/examples/pytorch/translation/run_translation.py \\\n",
    "--per_device_train_batch_size $$per_device_train_batch_size \\\n",
    "--save_steps $$save_steps \\\n",
    "--num_train_epochs $$num_train_epochs \\\n",
    "--logging_steps $$logging_steps \\\n",
    "--label_smoothing_factor $$label_smoothing_factor \\\n",
    "--learning_rate $$learning_rate \\\n",
    "--run_name $$run_name \\\n",
    "--output_dir $$output_dir \\\n",
    "--logging_dir $$logging_dir \\\n",
    "--eval_steps $$eval_steps \\\n",
    "--gradient_accumulation_steps $$gradient_accumulation_steps \\\n",
    "--model_name_or_path  $$model_name_or_path  \\\n",
    "--dataset_name  $$dataset_name  \\\n",
    "--generation_max_length $$generation_max_length \\\n",
    "--generation_num_beams $$generation_num_beams \\\n",
    "--source_lang $$source_lang \\\n",
    "--target_lang $$target_lang \\\n",
    "--dataset_config_name $$dataset_config_name \\\n",
    "--predict_with_generate $$predict_with_generate \\\n",
    "--max_source_length $$max_source_length \\\n",
    "--dataloader_drop_last $$dataloader_drop_last \\\n",
    "--warmup_steps $$warmup_steps \\\n",
    "--weight_decay $$weight_decay \\\n",
    "--save_total_limit $$save_total_limit \\\n",
    "--seed $$seed \\\n",
    "--overwrite_output_dir $$overwrite_output_dir \\\n",
    "--jit_mode_eval $$jit_mode_eval \\\n",
    "--do_eval $$do_eval \\\n",
    "--do_predict $$do_predict \\\n",
    "--do_train $$do_train \\\n",
    "--fp16 $$fp16 \\\n",
    "--fp16_backend $$fp16_backend \\\n",
    "--fp16_full_eval $$fp16_full_eval \\\n",
    "--full_determinism $$full_determinism \\\n",
    "--generation_config $$generation_config\n",
    "# --resume_from_checkpoint {resume_from_checkpoint} \n",
    "# --use_cpu {use_cpu} \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zindi-z3yfXQo9-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
