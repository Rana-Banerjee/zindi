{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.cache/pypoetry/virtualenvs/zindi-LtLKIbXv-py3.9/lib/python3.9/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n",
      "/root/.cache/pypoetry/virtualenvs/zindi-LtLKIbXv-py3.9/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/zindi\n"
     ]
    }
   ],
   "source": [
    "# %cd /home/rana/Projects/zindi\n",
    "%cd /root/zindi/\n",
    "import yaml\n",
    "import json\n",
    "import os   \n",
    "import shutil\n",
    "\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "with open('common/config.yaml', 'r') as f:\n",
    "    config = yaml.load(f, Loader=yaml.SafeLoader)\n",
    "\n",
    "def delete_file_if_exists(file_path):\n",
    "    if os.path.exists(file_path):\n",
    "        os.remove(file_path)\n",
    "        print(f\"File {file_path} has been deleted.\")\n",
    "    else:\n",
    "        print(f\"File {file_path} does not exist.\")\n",
    "\n",
    "\n",
    "def copy_file_or_directory(source, destination):\n",
    "    try:\n",
    "        if os.path.isfile(source):\n",
    "            # If source is a file, copy it directly\n",
    "            shutil.copy2(source, destination)\n",
    "            print(f\"File copied successfully from {source} to {destination}\")\n",
    "        elif os.path.isdir(source):\n",
    "            # If source is a directory, copy all files within it\n",
    "            if not os.path.exists(destination):\n",
    "                os.makedirs(destination)\n",
    "            for item in os.listdir(source):\n",
    "                s = os.path.join(source, item)\n",
    "                d = os.path.join(destination, item)\n",
    "                if os.path.isfile(s):\n",
    "                    shutil.copy2(s, d)\n",
    "                    print(f\"File copied successfully from {s} to {d}\")\n",
    "        else:\n",
    "            print(f\"Source {source} is neither a file nor a directory.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Source not found: {source}\")\n",
    "    except PermissionError:\n",
    "        print(\"Permission denied. Check file permissions.\")\n",
    "    except shutil.SameFileError:\n",
    "        print(\"Source and destination are the same file.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "def create_or_clean_directory(path):\n",
    "    if os.path.exists(path):\n",
    "        # Path exists, so clean it\n",
    "        for item in os.listdir(path):\n",
    "            item_path = os.path.join(path, item)\n",
    "            if os.path.isfile(item_path):\n",
    "                os.unlink(item_path)\n",
    "            elif os.path.isdir(item_path):\n",
    "                shutil.rmtree(item_path)\n",
    "        print(f\"Cleaned existing directory: {path}\")\n",
    "    else:\n",
    "        # Path doesn't exist, so create it\n",
    "        os.makedirs(path)\n",
    "        print(f\"Created new directory: {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#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\n",
    "# !huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Device specific params\n",
    "os.environ['model_name_or_path'] = \"models/marian/marian_output/base_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned existing directory: models/marian/marian_output/base_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59421]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File models/marian/marian_output/base_model/generation_config.json has been deleted.\n",
      "File copied successfully from tokenizer_custom/combined_pad_V2/source.spm to models/marian/marian_output/base_model/source.spm\n",
      "File copied successfully from tokenizer_custom/combined_pad_V2/special_tokens_map.json to models/marian/marian_output/base_model/special_tokens_map.json\n",
      "File copied successfully from tokenizer_custom/combined_pad_V2/target.spm to models/marian/marian_output/base_model/target.spm\n",
      "File copied successfully from tokenizer_custom/combined_pad_V2/tokenizer_config.json to models/marian/marian_output/base_model/tokenizer_config.json\n",
      "File copied successfully from tokenizer_custom/combined_pad_V2/vocab.json to models/marian/marian_output/base_model/vocab.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "first = True\n",
    "# first = False\n",
    "\n",
    "# base_model_path = \"models/marian/marian_output/base_model\"\n",
    "custom_tokenizer=\"tokenizer_custom/combined_pad_V2\"\n",
    "\n",
    "create_or_clean_directory(os.environ['model_name_or_path'])\n",
    "if first:\n",
    "    # Download the model\n",
    "    # Load model:\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(config.get('model_name_or_path'))\n",
    "    model.save_pretrained(os.environ['model_name_or_path'])\n",
    "    delete_file_if_exists(os.environ['model_name_or_path']+'/generation_config.json')\n",
    "    copy_file_or_directory(custom_tokenizer, os.environ['model_name_or_path'])\n",
    "else:\n",
    "    # Copy checkpoint to base model path\n",
    "    copy_file_or_directory(config.get('model_name_or_path'), os.environ['model_name_or_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MarianMTModel were not initialized from the model checkpoint at models/marian/marian_output/base_model and are newly initialized because the shapes did not match:\n",
      "- final_logits_bias: found shape torch.Size([1, 59422]) in the checkpoint and torch.Size([1, 32000]) in the model instantiated\n",
      "- model.decoder.layers.0.fc1.bias: found shape torch.Size([2048]) in the checkpoint and torch.Size([8]) in the model instantiated\n",
      "- model.decoder.layers.0.fc1.weight: found shape torch.Size([2048, 512]) in the checkpoint and torch.Size([8, 512]) in the model instantiated\n",
      "- model.decoder.layers.0.fc2.weight: found shape torch.Size([512, 2048]) in the checkpoint and torch.Size([512, 8]) in the model instantiated\n",
      "- model.decoder.layers.1.fc1.bias: found shape torch.Size([2048]) in the checkpoint and torch.Size([8]) in the model instantiated\n",
      "- model.decoder.layers.1.fc1.weight: found shape torch.Size([2048, 512]) in the checkpoint and torch.Size([8, 512]) in the model instantiated\n",
      "- model.decoder.layers.1.fc2.weight: found shape torch.Size([512, 2048]) in the checkpoint and torch.Size([512, 8]) in the model instantiated\n",
      "- model.decoder.layers.2.fc1.bias: found shape torch.Size([2048]) in the checkpoint and torch.Size([8]) in the model instantiated\n",
      "- model.decoder.layers.2.fc1.weight: found shape torch.Size([2048, 512]) in the checkpoint and torch.Size([8, 512]) in the model instantiated\n",
      "- model.decoder.layers.2.fc2.weight: found shape torch.Size([512, 2048]) in the checkpoint and torch.Size([512, 8]) in the model instantiated\n",
      "- model.decoder.layers.3.fc1.bias: found shape torch.Size([2048]) in the checkpoint and torch.Size([8]) in the model instantiated\n",
      "- model.decoder.layers.3.fc1.weight: found shape torch.Size([2048, 512]) in the checkpoint and torch.Size([8, 512]) in the model instantiated\n",
      "- model.decoder.layers.3.fc2.weight: found shape torch.Size([512, 2048]) in the checkpoint and torch.Size([512, 8]) in the model instantiated\n",
      "- model.decoder.layers.4.fc1.bias: found shape torch.Size([2048]) in the checkpoint and torch.Size([8]) in the model instantiated\n",
      "- model.decoder.layers.4.fc1.weight: found shape torch.Size([2048, 512]) in the checkpoint and torch.Size([8, 512]) in the model instantiated\n",
      "- model.decoder.layers.4.fc2.weight: found shape torch.Size([512, 2048]) in the checkpoint and torch.Size([512, 8]) in the model instantiated\n",
      "- model.decoder.layers.5.fc1.bias: found shape torch.Size([2048]) in the checkpoint and torch.Size([8]) in the model instantiated\n",
      "- model.decoder.layers.5.fc1.weight: found shape torch.Size([2048, 512]) in the checkpoint and torch.Size([8, 512]) in the model instantiated\n",
      "- model.decoder.layers.5.fc2.weight: found shape torch.Size([512, 2048]) in the checkpoint and torch.Size([512, 8]) in the model instantiated\n",
      "- model.encoder.layers.0.fc1.bias: found shape torch.Size([2048]) in the checkpoint and torch.Size([8]) in the model instantiated\n",
      "- model.encoder.layers.0.fc1.weight: found shape torch.Size([2048, 512]) in the checkpoint and torch.Size([8, 512]) in the model instantiated\n",
      "- model.encoder.layers.0.fc2.weight: found shape torch.Size([512, 2048]) in the checkpoint and torch.Size([512, 8]) in the model instantiated\n",
      "- model.encoder.layers.1.fc1.bias: found shape torch.Size([2048]) in the checkpoint and torch.Size([8]) in the model instantiated\n",
      "- model.encoder.layers.1.fc1.weight: found shape torch.Size([2048, 512]) in the checkpoint and torch.Size([8, 512]) in the model instantiated\n",
      "- model.encoder.layers.1.fc2.weight: found shape torch.Size([512, 2048]) in the checkpoint and torch.Size([512, 8]) in the model instantiated\n",
      "- model.encoder.layers.2.fc1.bias: found shape torch.Size([2048]) in the checkpoint and torch.Size([8]) in the model instantiated\n",
      "- model.encoder.layers.2.fc1.weight: found shape torch.Size([2048, 512]) in the checkpoint and torch.Size([8, 512]) in the model instantiated\n",
      "- model.encoder.layers.2.fc2.weight: found shape torch.Size([512, 2048]) in the checkpoint and torch.Size([512, 8]) in the model instantiated\n",
      "- model.encoder.layers.3.fc1.bias: found shape torch.Size([2048]) in the checkpoint and torch.Size([8]) in the model instantiated\n",
      "- model.encoder.layers.3.fc1.weight: found shape torch.Size([2048, 512]) in the checkpoint and torch.Size([8, 512]) in the model instantiated\n",
      "- model.encoder.layers.3.fc2.weight: found shape torch.Size([512, 2048]) in the checkpoint and torch.Size([512, 8]) in the model instantiated\n",
      "- model.encoder.layers.4.fc1.bias: found shape torch.Size([2048]) in the checkpoint and torch.Size([8]) in the model instantiated\n",
      "- model.encoder.layers.4.fc1.weight: found shape torch.Size([2048, 512]) in the checkpoint and torch.Size([8, 512]) in the model instantiated\n",
      "- model.encoder.layers.4.fc2.weight: found shape torch.Size([512, 2048]) in the checkpoint and torch.Size([512, 8]) in the model instantiated\n",
      "- model.encoder.layers.5.fc1.bias: found shape torch.Size([2048]) in the checkpoint and torch.Size([8]) in the model instantiated\n",
      "- model.encoder.layers.5.fc1.weight: found shape torch.Size([2048, 512]) in the checkpoint and torch.Size([8, 512]) in the model instantiated\n",
      "- model.encoder.layers.5.fc2.weight: found shape torch.Size([512, 2048]) in the checkpoint and torch.Size([512, 8]) in the model instantiated\n",
      "- model.shared.weight: found shape torch.Size([59422, 512]) in the checkpoint and torch.Size([32000, 512]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 128, 'num_beams': 2, 'bad_words_ids': [[31999]], 'forced_eos_token_id': 2}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "598"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \"_name_or_path\": \"models/marian/marian_output/base_model\",\n",
    "#   \"_num_labels\": 3,\n",
    "\n",
    "#   \"activation_function\": \"swish\",\n",
    "#   \"add_bias_logits\": false,\n",
    "#   \"add_final_layer_norm\": false,\n",
    "#   \"architectures\": [\n",
    "#     \"MarianMTModel\"\n",
    "#   ],\n",
    "  \n",
    "#   \"bad_words_ids\": [\n",
    "#     [\n",
    "#       31999\n",
    "#     ]\n",
    "#   ],\n",
    "#   \"bos_token_id\": 1,\n",
    "#   \"decoder_attention_heads\": 8,\n",
    "#   \"decoder_ffn_dim\": 2048,\n",
    "#   \"decoder_layers\": 6,\n",
    "#   \"decoder_start_token_id\": 31999,\n",
    "#   \"decoder_vocab_size\": 32000,\n",
    "#   \"dropout\": 0.75,\n",
    "#   \"encoder_attention_heads\": 8,\n",
    "#   \"encoder_ffn_dim\": 2048,\n",
    "#   \"encoder_layers\": 6,\n",
    "#   \"eos_token_id\": 2,\n",
    "#   \"forced_eos_token_id\": 2,\n",
    "#   \"id2label\": {\n",
    "#     \"0\": \"LABEL_0\",\n",
    "#     \"1\": \"LABEL_1\",\n",
    "#     \"2\": \"LABEL_2\"\n",
    "#   },\n",
    "#   \"init_std\": 0.02,\n",
    "#   \"is_encoder_decoder\": true,\n",
    "#   \"label2id\": {\n",
    "#     \"LABEL_0\": 0,\n",
    "#     \"LABEL_1\": 1,\n",
    "#     \"LABEL_2\": 2\n",
    "#   },\n",
    "#   \"max_length\": 128,\n",
    "#   \"max_position_embeddings\": 512,\n",
    "#   \"model_type\": \"marian\",\n",
    "#   \"normalize_before\": false,\n",
    "#   \"normalize_embedding\": false,\n",
    "#   \"num_beams\": 2,\n",
    "#   \"num_hidden_layers\": 6,\n",
    "#   \"pad_token_id\": 31999,\n",
    "#   \"scale_embedding\": true,\n",
    "#   \"share_encoder_decoder_embeddings\": true,\n",
    "#   \"static_position_embeddings\": true,\n",
    "#   \"torch_dtype\": \"float32\",\n",
    "#   \"transformers_version\": \"4.45.0.dev0\",\n",
    "#   \"use_cache\": true,\n",
    "#   \"vocab_size\": 32000\n",
    "\n",
    "import gc\n",
    "# Modify the model config\n",
    "config_path = os.environ['model_name_or_path']+'/config.json'\n",
    "with open(config_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "    data['decoder_attention_heads']=2 #8\n",
    "    data['decoder_ffn_dim']= 512 #2048\n",
    "    data['dropout']=0.5\n",
    "    data['encoder_attention_heads']=2 #8\n",
    "    data['encoder_ffn_dim']= 512 #2048\n",
    "    data['decoder_layers']=3\n",
    "    data['encoder_layers']=3\n",
    "    # data['max_position_embeddings']=512\n",
    "    data['num_hidden_layers']=3\n",
    "    # data['torch_dtype']=\"float32\"\n",
    "    data[\"activation_dropout\"]= 0.2\n",
    "    data[\"attention_dropout\"]= 0.2\n",
    "    data[\"classif_dropout\"]= 0.2\n",
    "    data[\"classifier_dropout\"]= 0.2\n",
    "    data[\"d_model\"]= 128\n",
    "    data[\"decoder_layerdrop\"]= 0.1\n",
    "    data[\"encoder_layerdrop\"]= 0.1\n",
    "\n",
    "    data['max_length']=128\n",
    "    data['num_beams']=2\n",
    "    data[\"bos_token_id\"]= 1\n",
    "    data[\"eos_token_id\"]= 2\n",
    "    data[\"forced_eos_token_id\"]= 2\n",
    "    data[\"bad_words_ids\"]= [[31999]]\n",
    "    data[\"decoder_start_token_id\"]= 31999\n",
    "    data['decoder_vocab_size']=32000\n",
    "    data[\"pad_token_id\"]= 31999\n",
    "    data['vocab_size']=32000\n",
    "\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(data, f, indent=2)    \n",
    "\n",
    "# Load model with updated config and save it\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(os.environ['model_name_or_path'], ignore_mismatched_sizes=True)\n",
    "# Save model\n",
    "model.save_pretrained(os.environ['model_name_or_path'])\n",
    "\n",
    "del model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['dataset_name'] = \"uvci/Koumankan_mt_dyu_fr\"\n",
    "os.environ['generation_config'] = os.environ['model_name_or_path']+'/generation_config.json'\n",
    "os.environ['source_lang']=\"dyu\"\n",
    "os.environ['target_lang']=\"fr\"\n",
    "os.environ['dataset_config_name']= \"default\"\n",
    "os.environ['per_device_eval_batch_size']=\"480\"\n",
    "os.environ['per_device_train_batch_size']=\"480\"\n",
    "# use_cpu=False\n",
    "os.environ['save_steps']=\"100\"\n",
    "os.environ['eval_steps']=\"100\"\n",
    "os.environ['num_train_epochs']=\"10000\"\n",
    "os.environ['logging_steps']=\"10\"\n",
    "os.environ['save_total_limit']=\"10\"\n",
    "os.environ['overwrite_output_dir']=\"True\"\n",
    "os.environ['run_name']=\"marian-1\"\n",
    "os.environ['output_dir']=\"models/marian/marian_output\"\n",
    "os.environ['logging_dir']=\"models/marian/logs\"\n",
    "os.environ['predict_with_generate']=\"True\"\n",
    "os.environ['dataloader_drop_last']=\"True\"\n",
    "os.environ['jit_mode_eval']=\"False\"\n",
    "# os.environ['do_eval']=\"True\"\n",
    "os.environ['do_predict']=\"False\"\n",
    "os.environ['do_train']=\"True\"\n",
    "\n",
    "### Config\n",
    "os.environ['label_smoothing_factor']=\"0.00001\"\n",
    "os.environ['learning_rate']=\"1e-04\"\n",
    "os.environ['gradient_accumulation_steps']=\"4\"\n",
    "os.environ['generation_max_length']=\"128\"\n",
    "os.environ['generation_num_beams']=\"2\"\n",
    "os.environ['max_source_length']= \"128\"\n",
    "os.environ['warmup_steps']=\"10\"\n",
    "os.environ['weight_decay']=\"0.00001\"\n",
    "os.environ['seed']=\"42\"\n",
    "os.environ['fp16']=\"False\"\n",
    "os.environ['fp16_backend']=\"auto\"\n",
    "os.environ['fp16_full_eval']=\"False\"\n",
    "os.environ['full_determinism']=\"True\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import GenerationConfig\n",
    "\n",
    "# # Create a custom generation config\n",
    "# custom_gen_config = GenerationConfig(\n",
    "#     bad_words_ids=[[31999]],\n",
    "#     bos_token_id=1,\n",
    "#     decoder_start_token_id=31999,\n",
    "#     eos_token_id=2,\n",
    "#     forced_eos_token_id=2,\n",
    "#     pad_token_id=31999,\n",
    "#     num_beams=1,\n",
    "#     max_length=128\n",
    "#     # Add any other parameters you want to override\n",
    "# )\n",
    "\n",
    "# custom_gen_config.save_pretrained(base_model_path, \"generation_config.json\")\n",
    "# os.environ['generation_config']=base_model_path+\"/generation_config.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/16/2024 09:54:48 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "08/16/2024 09:54:48 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n",
      "_n_gpu=1,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=True,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_on_start=False,\n",
      "eval_steps=100,\n",
      "eval_strategy=steps,\n",
      "eval_use_gather_object=False,\n",
      "evaluation_strategy=None,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=True,\n",
      "generation_config=models/marian/marian_output/base_model/generation_config.json,\n",
      "generation_max_length=128,\n",
      "generation_num_beams=2,\n",
      "gradient_accumulation_steps=4,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=1e-05,\n",
      "learning_rate=0.0001,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=models/marian/logs,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=10,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=10000.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=models/marian/marian_output,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=480,\n",
      "per_device_train_batch_size=480,\n",
      "predict_with_generate=True,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=marian-1,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=100,\n",
      "save_strategy=steps,\n",
      "save_total_limit=10,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "sortish_sampler=False,\n",
      "split_batches=None,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torch_empty_cache_steps=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=10,\n",
      "weight_decay=1e-05,\n",
      ")\n",
      "Overwrite dataset info from restored data version if exists.\n",
      "08/16/2024 09:54:51 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
      "Loading Dataset info from /root/.cache/huggingface/datasets/uvci___koumankan_mt_dyu_fr/default/0.0.0/5207d8690e8c37ec07a1954dc7441c348f8242fa\n",
      "08/16/2024 09:54:51 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/uvci___koumankan_mt_dyu_fr/default/0.0.0/5207d8690e8c37ec07a1954dc7441c348f8242fa\n",
      "Found cached dataset koumankan_mt_dyu_fr (/root/.cache/huggingface/datasets/uvci___koumankan_mt_dyu_fr/default/0.0.0/5207d8690e8c37ec07a1954dc7441c348f8242fa)\n",
      "08/16/2024 09:54:51 - INFO - datasets.builder - Found cached dataset koumankan_mt_dyu_fr (/root/.cache/huggingface/datasets/uvci___koumankan_mt_dyu_fr/default/0.0.0/5207d8690e8c37ec07a1954dc7441c348f8242fa)\n",
      "Loading Dataset info from /root/.cache/huggingface/datasets/uvci___koumankan_mt_dyu_fr/default/0.0.0/5207d8690e8c37ec07a1954dc7441c348f8242fa\n",
      "08/16/2024 09:54:51 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/uvci___koumankan_mt_dyu_fr/default/0.0.0/5207d8690e8c37ec07a1954dc7441c348f8242fa\n",
      "[INFO|configuration_utils.py:731] 2024-08-16 09:54:51,641 >> loading configuration file models/marian/marian_output/base_model/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-08-16 09:54:51,647 >> Model config MarianConfig {\n",
      "  \"_name_or_path\": \"models/marian/marian_output/base_model\",\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      31999\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 2,\n",
      "  \"decoder_ffn_dim\": 8,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 31999,\n",
      "  \"decoder_vocab_size\": 32000,\n",
      "  \"dropout\": 0.5,\n",
      "  \"encoder_attention_heads\": 2,\n",
      "  \"encoder_ffn_dim\": 8,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 128,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 2,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 31999,\n",
      "  \"scale_embedding\": true,\n",
      "  \"share_encoder_decoder_embeddings\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.45.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2267] 2024-08-16 09:54:51,649 >> loading file source.spm\n",
      "[INFO|tokenization_utils_base.py:2267] 2024-08-16 09:54:51,650 >> loading file target.spm\n",
      "[INFO|tokenization_utils_base.py:2267] 2024-08-16 09:54:51,650 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:2267] 2024-08-16 09:54:51,650 >> loading file target_vocab.json\n",
      "[INFO|tokenization_utils_base.py:2267] 2024-08-16 09:54:51,650 >> loading file tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2267] 2024-08-16 09:54:51,650 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2267] 2024-08-16 09:54:51,650 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2267] 2024-08-16 09:54:51,650 >> loading file tokenizer.json\n",
      "[INFO|modeling_utils.py:3654] 2024-08-16 09:54:52,409 >> loading weights file models/marian/marian_output/base_model/model.safetensors\n",
      "[INFO|configuration_utils.py:1038] 2024-08-16 09:54:52,425 >> Generate config GenerationConfig {\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      31999\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 1,\n",
      "  \"decoder_start_token_id\": 31999,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"num_beams\": 2,\n",
      "  \"pad_token_id\": 31999\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:4489] 2024-08-16 09:54:54,496 >> All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "[INFO|modeling_utils.py:4497] 2024-08-16 09:54:54,496 >> All the weights of MarianMTModel were initialized from the model checkpoint at models/marian/marian_output/base_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "[INFO|configuration_utils.py:991] 2024-08-16 09:54:54,499 >> loading configuration file models/marian/marian_output/base_model/generation_config.json\n",
      "[INFO|configuration_utils.py:1038] 2024-08-16 09:54:54,500 >> Generate config GenerationConfig {\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      31999\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 1,\n",
      "  \"decoder_start_token_id\": 31999,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"num_beams\": 2,\n",
      "  \"pad_token_id\": 31999\n",
      "}\n",
      "\n",
      "Running tokenizer on train dataset:   0%|       | 0/8065 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/uvci___koumankan_mt_dyu_fr/default/0.0.0/5207d8690e8c37ec07a1954dc7441c348f8242fa/cache-6a6945058669b6c0.arrow\n",
      "08/16/2024 09:54:55 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/uvci___koumankan_mt_dyu_fr/default/0.0.0/5207d8690e8c37ec07a1954dc7441c348f8242fa/cache-6a6945058669b6c0.arrow\n",
      "Running tokenizer on train dataset: 100%|█| 8065/8065 [00:02<00:00, 3906.03 exam\n",
      "Running tokenizer on validation dataset:   0%|  | 0/1471 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/uvci___koumankan_mt_dyu_fr/default/0.0.0/5207d8690e8c37ec07a1954dc7441c348f8242fa/cache-7207420cab42d583.arrow\n",
      "08/16/2024 09:54:58 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/uvci___koumankan_mt_dyu_fr/default/0.0.0/5207d8690e8c37ec07a1954dc7441c348f8242fa/cache-7207420cab42d583.arrow\n",
      "Running tokenizer on validation dataset: 100%|█| 1471/1471 [00:00<00:00, 4022.96\n",
      "Running tokenizer on prediction dataset:   0%|  | 0/1393 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/uvci___koumankan_mt_dyu_fr/default/0.0.0/5207d8690e8c37ec07a1954dc7441c348f8242fa/cache-566f4699b92fe95d.arrow\n",
      "08/16/2024 09:54:59 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/uvci___koumankan_mt_dyu_fr/default/0.0.0/5207d8690e8c37ec07a1954dc7441c348f8242fa/cache-566f4699b92fe95d.arrow\n",
      "Running tokenizer on prediction dataset: 100%|█| 1393/1393 [00:00<00:00, 5702.28\n",
      "[INFO|configuration_utils.py:991] 2024-08-16 09:55:00,615 >> loading configuration file models/marian/marian_output/base_model/generation_config.json\n",
      "[INFO|configuration_utils.py:1038] 2024-08-16 09:55:00,615 >> Generate config GenerationConfig {\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      31999\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 1,\n",
      "  \"decoder_start_token_id\": 31999,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"num_beams\": 2,\n",
      "  \"pad_token_id\": 31999\n",
      "}\n",
      "\n",
      "[INFO|trainer.py:2160] 2024-08-16 09:55:01,531 >> ***** Running training *****\n",
      "[INFO|trainer.py:2161] 2024-08-16 09:55:01,531 >>   Num examples = 8,065\n",
      "[INFO|trainer.py:2162] 2024-08-16 09:55:01,531 >>   Num Epochs = 10,000\n",
      "[INFO|trainer.py:2163] 2024-08-16 09:55:01,531 >>   Instantaneous batch size per device = 480\n",
      "[INFO|trainer.py:2166] 2024-08-16 09:55:01,531 >>   Total train batch size (w. parallel, distributed & accumulation) = 1,920\n",
      "[INFO|trainer.py:2167] 2024-08-16 09:55:01,531 >>   Gradient Accumulation steps = 4\n",
      "[INFO|trainer.py:2168] 2024-08-16 09:55:01,531 >>   Total optimization steps = 40,000\n",
      "[INFO|trainer.py:2169] 2024-08-16 09:55:01,533 >>   Number of trainable parameters = 35,430,496\n",
      "{'loss': 11.0123, 'grad_norm': 8.179410934448242, 'learning_rate': 0.0001, 'epoch': 2.5}\n",
      "{'loss': 8.7525, 'grad_norm': 4.102041721343994, 'learning_rate': 9.997499374843712e-05, 'epoch': 5.0}\n",
      "{'loss': 7.3868, 'grad_norm': 1.8526946306228638, 'learning_rate': 9.994998749687422e-05, 'epoch': 7.5}\n",
      "{'loss': 6.7463, 'grad_norm': 1.2598847150802612, 'learning_rate': 9.992498124531134e-05, 'epoch': 10.0}\n",
      "{'loss': 6.5179, 'grad_norm': 1.2037115097045898, 'learning_rate': 9.989997499374844e-05, 'epoch': 12.5}\n",
      "{'loss': 6.3752, 'grad_norm': 0.8402299284934998, 'learning_rate': 9.987496874218555e-05, 'epoch': 15.0}\n",
      "{'loss': 6.2198, 'grad_norm': 0.8306780457496643, 'learning_rate': 9.984996249062266e-05, 'epoch': 17.5}\n",
      "{'loss': 6.1214, 'grad_norm': 1.7344168424606323, 'learning_rate': 9.982495623905978e-05, 'epoch': 20.0}\n",
      "{'loss': 6.0394, 'grad_norm': 0.8078987002372742, 'learning_rate': 9.979994998749688e-05, 'epoch': 22.5}\n",
      "{'loss': 5.9768, 'grad_norm': 0.9016606211662292, 'learning_rate': 9.977494373593399e-05, 'epoch': 25.0}\n",
      "  0%|                                    | 100/40000 [03:07<20:29:11,  1.85s/it][INFO|trainer.py:3864] 2024-08-16 09:58:09,446 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3866] 2024-08-16 09:58:09,446 >>   Num examples = 1471\n",
      "[INFO|trainer.py:3869] 2024-08-16 09:58:09,446 >>   Batch size = 480\n",
      "[INFO|configuration_utils.py:1038] 2024-08-16 09:58:09,532 >> Generate config GenerationConfig {\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      31999\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 1,\n",
      "  \"decoder_start_token_id\": 31999,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"num_beams\": 2,\n",
      "  \"pad_token_id\": 31999\n",
      "}\n",
      "\n",
      "\n",
      "  0%|                                                     | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      " 67%|██████████████████████████████               | 2/3 [00:22<00:11, 11.04s/it]\u001b[A\n",
      "100%|█████████████████████████████████████████████| 3/3 [00:44<00:00, 15.84s/it]\u001b[A\n",
      "{'eval_loss': 6.103870868682861, 'eval_bleu': 0.0144, 'eval_gen_len': 27.1736, 'eval_runtime': 74.001, 'eval_samples_per_second': 19.878, 'eval_steps_per_second': 0.054, 'epoch': 25.0}\n",
      "\n",
      "  0%|                                    | 100/40000 [04:21<20:29:11,  1.85s/it]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:3548] 2024-08-16 09:59:23,447 >> Saving model checkpoint to models/marian/marian_output/checkpoint-100\n",
      "[WARNING|configuration_utils.py:448] 2024-08-16 09:59:23,448 >> Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 128, 'num_beams': 2, 'bad_words_ids': [[31999]], 'forced_eos_token_id': 2}\n",
      "[INFO|configuration_utils.py:472] 2024-08-16 09:59:23,450 >> Configuration saved in models/marian/marian_output/checkpoint-100/config.json\n",
      "[INFO|configuration_utils.py:807] 2024-08-16 09:59:23,451 >> Configuration saved in models/marian/marian_output/checkpoint-100/generation_config.json\n",
      "[INFO|modeling_utils.py:2778] 2024-08-16 09:59:23,777 >> Model weights saved in models/marian/marian_output/checkpoint-100/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2684] 2024-08-16 09:59:23,778 >> tokenizer config file saved in models/marian/marian_output/checkpoint-100/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2693] 2024-08-16 09:59:23,778 >> Special tokens file saved in models/marian/marian_output/checkpoint-100/special_tokens_map.json\n",
      "[INFO|trainer.py:3640] 2024-08-16 09:59:24,348 >> Deleting older checkpoint [models/marian/marian_output/checkpoint-100] due to args.save_total_limit\n",
      "{'loss': 5.9118, 'grad_norm': 1.0361502170562744, 'learning_rate': 9.97499374843711e-05, 'epoch': 27.5}\n",
      "{'loss': 5.8569, 'grad_norm': 1.5289067029953003, 'learning_rate': 9.97249312328082e-05, 'epoch': 30.0}\n",
      "{'loss': 5.8001, 'grad_norm': 0.7091296911239624, 'learning_rate': 9.969992498124532e-05, 'epoch': 32.5}\n",
      "{'loss': 5.7474, 'grad_norm': 1.052289366722107, 'learning_rate': 9.967491872968243e-05, 'epoch': 35.0}\n",
      "{'loss': 5.6908, 'grad_norm': 1.021533727645874, 'learning_rate': 9.964991247811953e-05, 'epoch': 37.5}\n",
      "{'loss': 5.6337, 'grad_norm': 1.2014777660369873, 'learning_rate': 9.962490622655665e-05, 'epoch': 40.0}\n",
      "{'loss': 5.5715, 'grad_norm': 0.6490069031715393, 'learning_rate': 9.959989997499375e-05, 'epoch': 42.5}\n",
      "{'loss': 5.5118, 'grad_norm': 0.7344726920127869, 'learning_rate': 9.957489372343087e-05, 'epoch': 45.0}\n",
      "{'loss': 5.4494, 'grad_norm': 0.7406790256500244, 'learning_rate': 9.954988747186797e-05, 'epoch': 47.5}\n",
      "{'loss': 5.3734, 'grad_norm': 0.8911320567131042, 'learning_rate': 9.952488122030508e-05, 'epoch': 50.0}\n",
      "  0%|▏                                   | 200/40000 [07:29<20:29:35,  1.85s/it][INFO|trainer.py:3864] 2024-08-16 10:02:31,425 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3866] 2024-08-16 10:02:31,425 >>   Num examples = 1471\n",
      "[INFO|trainer.py:3869] 2024-08-16 10:02:31,425 >>   Batch size = 480\n",
      "\n",
      "  0%|                                                     | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      " 67%|██████████████████████████████               | 2/3 [00:24<00:12, 12.11s/it]\u001b[A\n",
      "100%|█████████████████████████████████████████████| 3/3 [00:48<00:00, 17.07s/it]\u001b[A\n",
      "{'eval_loss': 5.753169536590576, 'eval_bleu': 0.1267, 'eval_gen_len': 33.8069, 'eval_runtime': 80.4731, 'eval_samples_per_second': 18.279, 'eval_steps_per_second': 0.05, 'epoch': 50.0}\n",
      "\n",
      "  0%|▏                                   | 200/40000 [08:50<20:29:35,  1.85s/it]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:3548] 2024-08-16 10:03:51,898 >> Saving model checkpoint to models/marian/marian_output/checkpoint-200\n",
      "[WARNING|configuration_utils.py:448] 2024-08-16 10:03:51,899 >> Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 128, 'num_beams': 2, 'bad_words_ids': [[31999]], 'forced_eos_token_id': 2}\n",
      "[INFO|configuration_utils.py:472] 2024-08-16 10:03:51,901 >> Configuration saved in models/marian/marian_output/checkpoint-200/config.json\n",
      "[INFO|configuration_utils.py:807] 2024-08-16 10:03:51,902 >> Configuration saved in models/marian/marian_output/checkpoint-200/generation_config.json\n",
      "[INFO|modeling_utils.py:2778] 2024-08-16 10:03:52,276 >> Model weights saved in models/marian/marian_output/checkpoint-200/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2684] 2024-08-16 10:03:52,278 >> tokenizer config file saved in models/marian/marian_output/checkpoint-200/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2693] 2024-08-16 10:03:52,278 >> Special tokens file saved in models/marian/marian_output/checkpoint-200/special_tokens_map.json\n",
      "[INFO|trainer.py:3640] 2024-08-16 10:03:52,850 >> Deleting older checkpoint [models/marian/marian_output/checkpoint-200] due to args.save_total_limit\n",
      "{'loss': 5.2989, 'grad_norm': 1.1042877435684204, 'learning_rate': 9.949987496874218e-05, 'epoch': 52.5}\n",
      "{'loss': 5.2281, 'grad_norm': 0.9118249416351318, 'learning_rate': 9.94748687171793e-05, 'epoch': 55.0}\n",
      "{'loss': 5.1661, 'grad_norm': 0.8382934927940369, 'learning_rate': 9.944986246561641e-05, 'epoch': 57.5}\n",
      "{'loss': 5.0959, 'grad_norm': 0.8353557586669922, 'learning_rate': 9.942485621405352e-05, 'epoch': 60.0}\n",
      "{'loss': 5.0352, 'grad_norm': 0.9970402717590332, 'learning_rate': 9.939984996249064e-05, 'epoch': 62.5}\n",
      "{'loss': 4.9631, 'grad_norm': 0.9638427495956421, 'learning_rate': 9.937484371092773e-05, 'epoch': 65.0}\n",
      "{'loss': 4.8983, 'grad_norm': 0.925538957118988, 'learning_rate': 9.934983745936485e-05, 'epoch': 67.5}\n",
      "{'loss': 4.8267, 'grad_norm': 0.8475202322006226, 'learning_rate': 9.932483120780196e-05, 'epoch': 70.0}\n",
      "{'loss': 4.765, 'grad_norm': 1.0917160511016846, 'learning_rate': 9.929982495623906e-05, 'epoch': 72.5}\n",
      "{'loss': 4.7073, 'grad_norm': 1.100833773612976, 'learning_rate': 9.927481870467618e-05, 'epoch': 75.0}\n",
      "  1%|▎                                   | 300/40000 [11:58<20:41:28,  1.88s/it][INFO|trainer.py:3864] 2024-08-16 10:07:00,528 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3866] 2024-08-16 10:07:00,529 >>   Num examples = 1471\n",
      "[INFO|trainer.py:3869] 2024-08-16 10:07:00,529 >>   Batch size = 480\n",
      "\n",
      "  0%|                                                     | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      " 67%|██████████████████████████████               | 2/3 [00:21<00:10, 10.88s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 5.503365993499756, 'eval_bleu': 0.2016, 'eval_gen_len': 23.8674, 'eval_runtime': 72.2743, 'eval_samples_per_second': 20.353, 'eval_steps_per_second': 0.055, 'epoch': 75.0}\n",
      "  1%|▎                                   | 300/40000 [13:11<20:41:28,  1.88s/it]\n",
      "100%|█████████████████████████████████████████████| 3/3 [00:51<00:00, 15.22s/it]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:3548] 2024-08-16 10:08:12,803 >> Saving model checkpoint to models/marian/marian_output/checkpoint-300\n",
      "[WARNING|configuration_utils.py:448] 2024-08-16 10:08:12,805 >> Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 128, 'num_beams': 2, 'bad_words_ids': [[31999]], 'forced_eos_token_id': 2}\n",
      "[INFO|configuration_utils.py:472] 2024-08-16 10:08:12,806 >> Configuration saved in models/marian/marian_output/checkpoint-300/config.json\n",
      "[INFO|configuration_utils.py:807] 2024-08-16 10:08:12,807 >> Configuration saved in models/marian/marian_output/checkpoint-300/generation_config.json\n",
      "[INFO|modeling_utils.py:2778] 2024-08-16 10:08:13,175 >> Model weights saved in models/marian/marian_output/checkpoint-300/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2684] 2024-08-16 10:08:13,176 >> tokenizer config file saved in models/marian/marian_output/checkpoint-300/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2693] 2024-08-16 10:08:13,177 >> Special tokens file saved in models/marian/marian_output/checkpoint-300/special_tokens_map.json\n",
      "[INFO|trainer.py:3640] 2024-08-16 10:08:13,749 >> Deleting older checkpoint [models/marian/marian_output/checkpoint-300] due to args.save_total_limit\n",
      "{'loss': 4.6454, 'grad_norm': 0.8517469763755798, 'learning_rate': 9.924981245311327e-05, 'epoch': 77.5}\n",
      "{'loss': 4.5754, 'grad_norm': 0.781134843826294, 'learning_rate': 9.92248062015504e-05, 'epoch': 80.0}\n",
      "{'loss': 4.52, 'grad_norm': 0.8392588496208191, 'learning_rate': 9.91997999499875e-05, 'epoch': 82.5}\n",
      "{'loss': 4.4576, 'grad_norm': 0.8455008864402771, 'learning_rate': 9.917479369842461e-05, 'epoch': 85.0}\n",
      "{'loss': 4.3958, 'grad_norm': 0.8294346332550049, 'learning_rate': 9.914978744686171e-05, 'epoch': 87.5}\n",
      "{'loss': 4.3355, 'grad_norm': 0.7775831818580627, 'learning_rate': 9.912478119529883e-05, 'epoch': 90.0}\n",
      "{'loss': 4.2704, 'grad_norm': 0.8418691158294678, 'learning_rate': 9.909977494373594e-05, 'epoch': 92.5}\n",
      "{'loss': 4.2061, 'grad_norm': 0.8856616616249084, 'learning_rate': 9.907476869217305e-05, 'epoch': 95.0}\n",
      "{'loss': 4.1466, 'grad_norm': 0.9181632399559021, 'learning_rate': 9.904976244061017e-05, 'epoch': 97.5}\n",
      "{'loss': 4.0854, 'grad_norm': 0.920511782169342, 'learning_rate': 9.902475618904726e-05, 'epoch': 100.0}\n",
      "  1%|▎                                   | 400/40000 [16:18<20:26:29,  1.86s/it][INFO|trainer.py:3864] 2024-08-16 10:11:20,117 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3866] 2024-08-16 10:11:20,117 >>   Num examples = 1471\n",
      "[INFO|trainer.py:3869] 2024-08-16 10:11:20,117 >>   Batch size = 480\n",
      "\n",
      "  0%|                                                     | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      " 67%|██████████████████████████████               | 2/3 [00:20<00:10, 10.44s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 5.607150077819824, 'eval_bleu': 0.4631, 'eval_gen_len': 21.5417, 'eval_runtime': 69.0953, 'eval_samples_per_second': 21.289, 'eval_steps_per_second': 0.058, 'epoch': 100.0}\n",
      "  1%|▎                                   | 400/40000 [17:27<20:26:29,  1.86s/it]\n",
      "100%|█████████████████████████████████████████████| 3/3 [00:48<00:00, 14.46s/it]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:3548] 2024-08-16 10:12:29,212 >> Saving model checkpoint to models/marian/marian_output/checkpoint-400\n",
      "[WARNING|configuration_utils.py:448] 2024-08-16 10:12:29,213 >> Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 128, 'num_beams': 2, 'bad_words_ids': [[31999]], 'forced_eos_token_id': 2}\n",
      "[INFO|configuration_utils.py:472] 2024-08-16 10:12:29,215 >> Configuration saved in models/marian/marian_output/checkpoint-400/config.json\n",
      "[INFO|configuration_utils.py:807] 2024-08-16 10:12:29,216 >> Configuration saved in models/marian/marian_output/checkpoint-400/generation_config.json\n",
      "[INFO|modeling_utils.py:2778] 2024-08-16 10:12:29,821 >> Model weights saved in models/marian/marian_output/checkpoint-400/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2684] 2024-08-16 10:12:29,822 >> tokenizer config file saved in models/marian/marian_output/checkpoint-400/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2693] 2024-08-16 10:12:29,823 >> Special tokens file saved in models/marian/marian_output/checkpoint-400/special_tokens_map.json\n",
      "{'loss': 4.0312, 'grad_norm': 0.8807122111320496, 'learning_rate': 9.899974993748438e-05, 'epoch': 102.5}\n",
      "{'loss': 3.9652, 'grad_norm': 0.8739885091781616, 'learning_rate': 9.897474368592148e-05, 'epoch': 105.0}\n",
      "{'loss': 3.907, 'grad_norm': 1.03908371925354, 'learning_rate': 9.894973743435859e-05, 'epoch': 107.5}\n",
      "{'loss': 3.849, 'grad_norm': 0.9161563515663147, 'learning_rate': 9.892473118279571e-05, 'epoch': 110.0}\n",
      "{'loss': 3.7932, 'grad_norm': 0.883877158164978, 'learning_rate': 9.889972493123282e-05, 'epoch': 112.5}\n",
      "{'loss': 3.7343, 'grad_norm': 0.8882622122764587, 'learning_rate': 9.887471867966992e-05, 'epoch': 115.0}\n",
      "{'loss': 3.675, 'grad_norm': 0.9390276074409485, 'learning_rate': 9.884971242810703e-05, 'epoch': 117.5}\n",
      "{'loss': 3.6252, 'grad_norm': 0.9528010487556458, 'learning_rate': 9.882470617654415e-05, 'epoch': 120.0}\n",
      "{'loss': 3.5662, 'grad_norm': 0.9126774668693542, 'learning_rate': 9.879969992498124e-05, 'epoch': 122.5}\n",
      "{'loss': 3.5095, 'grad_norm': 0.8690001368522644, 'learning_rate': 9.877469367341836e-05, 'epoch': 125.0}\n",
      "  1%|▍                                   | 500/40000 [20:35<20:19:13,  1.85s/it][INFO|trainer.py:3864] 2024-08-16 10:15:37,387 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3866] 2024-08-16 10:15:37,387 >>   Num examples = 1471\n",
      "[INFO|trainer.py:3869] 2024-08-16 10:15:37,387 >>   Batch size = 480\n",
      "\n",
      "  0%|                                                     | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      " 67%|██████████████████████████████               | 2/3 [00:21<00:10, 10.76s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 5.740093231201172, 'eval_bleu': 0.6042, 'eval_gen_len': 22.5896, 'eval_runtime': 70.8091, 'eval_samples_per_second': 20.774, 'eval_steps_per_second': 0.056, 'epoch': 125.0}\n",
      "  1%|▍                                   | 500/40000 [21:46<20:19:13,  1.85s/it]\n",
      "100%|█████████████████████████████████████████████| 3/3 [00:50<00:00, 14.93s/it]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:3548] 2024-08-16 10:16:48,196 >> Saving model checkpoint to models/marian/marian_output/checkpoint-500\n",
      "[WARNING|configuration_utils.py:448] 2024-08-16 10:16:48,198 >> Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 128, 'num_beams': 2, 'bad_words_ids': [[31999]], 'forced_eos_token_id': 2}\n",
      "[INFO|configuration_utils.py:472] 2024-08-16 10:16:48,199 >> Configuration saved in models/marian/marian_output/checkpoint-500/config.json\n",
      "[INFO|configuration_utils.py:807] 2024-08-16 10:16:48,200 >> Configuration saved in models/marian/marian_output/checkpoint-500/generation_config.json\n",
      "[INFO|modeling_utils.py:2778] 2024-08-16 10:16:48,838 >> Model weights saved in models/marian/marian_output/checkpoint-500/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2684] 2024-08-16 10:16:48,840 >> tokenizer config file saved in models/marian/marian_output/checkpoint-500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2693] 2024-08-16 10:16:48,840 >> Special tokens file saved in models/marian/marian_output/checkpoint-500/special_tokens_map.json\n",
      "{'loss': 3.4688, 'grad_norm': 1.0023964643478394, 'learning_rate': 9.874968742185547e-05, 'epoch': 127.5}\n",
      "{'loss': 3.4153, 'grad_norm': 1.1485170125961304, 'learning_rate': 9.872468117029257e-05, 'epoch': 130.0}\n",
      "{'loss': 3.3618, 'grad_norm': 0.9178339242935181, 'learning_rate': 9.86996749187297e-05, 'epoch': 132.5}\n",
      "{'loss': 3.3181, 'grad_norm': 0.9097700715065002, 'learning_rate': 9.867466866716679e-05, 'epoch': 135.0}\n",
      "{'loss': 3.2638, 'grad_norm': 0.8580099940299988, 'learning_rate': 9.86496624156039e-05, 'epoch': 137.5}\n",
      "{'loss': 3.2284, 'grad_norm': 0.9518342614173889, 'learning_rate': 9.862465616404101e-05, 'epoch': 140.0}\n",
      "{'loss': 3.175, 'grad_norm': 0.9070876836776733, 'learning_rate': 9.859964991247812e-05, 'epoch': 142.5}\n",
      "{'loss': 3.1394, 'grad_norm': 1.1763328313827515, 'learning_rate': 9.857464366091524e-05, 'epoch': 145.0}\n",
      "{'loss': 3.091, 'grad_norm': 1.0549811124801636, 'learning_rate': 9.854963740935235e-05, 'epoch': 147.5}\n",
      "{'loss': 3.0562, 'grad_norm': 0.978490948677063, 'learning_rate': 9.852463115778945e-05, 'epoch': 150.0}\n",
      "  2%|▌                                   | 600/40000 [24:56<20:28:33,  1.87s/it][INFO|trainer.py:3864] 2024-08-16 10:19:58,465 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3866] 2024-08-16 10:19:58,465 >>   Num examples = 1471\n",
      "[INFO|trainer.py:3869] 2024-08-16 10:19:58,465 >>   Batch size = 480\n",
      "\n",
      "  0%|                                                     | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      " 67%|██████████████████████████████               | 2/3 [00:21<00:10, 10.96s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 5.844571113586426, 'eval_bleu': 0.6061, 'eval_gen_len': 23.0493, 'eval_runtime': 71.4341, 'eval_samples_per_second': 20.592, 'eval_steps_per_second': 0.056, 'epoch': 150.0}\n",
      "  2%|▌                                   | 600/40000 [26:08<20:28:33,  1.87s/it]\n",
      "100%|█████████████████████████████████████████████| 3/3 [00:50<00:00, 15.02s/it]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:3548] 2024-08-16 10:21:09,899 >> Saving model checkpoint to models/marian/marian_output/checkpoint-600\n",
      "[WARNING|configuration_utils.py:448] 2024-08-16 10:21:09,901 >> Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 128, 'num_beams': 2, 'bad_words_ids': [[31999]], 'forced_eos_token_id': 2}\n",
      "[INFO|configuration_utils.py:472] 2024-08-16 10:21:09,902 >> Configuration saved in models/marian/marian_output/checkpoint-600/config.json\n",
      "[INFO|configuration_utils.py:807] 2024-08-16 10:21:09,903 >> Configuration saved in models/marian/marian_output/checkpoint-600/generation_config.json\n",
      "[INFO|modeling_utils.py:2778] 2024-08-16 10:21:10,510 >> Model weights saved in models/marian/marian_output/checkpoint-600/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2684] 2024-08-16 10:21:10,511 >> tokenizer config file saved in models/marian/marian_output/checkpoint-600/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2693] 2024-08-16 10:21:10,512 >> Special tokens file saved in models/marian/marian_output/checkpoint-600/special_tokens_map.json\n",
      "{'loss': 3.0134, 'grad_norm': 0.8952773809432983, 'learning_rate': 9.849962490622656e-05, 'epoch': 152.5}\n",
      "{'loss': 2.9739, 'grad_norm': 0.9279576539993286, 'learning_rate': 9.847461865466368e-05, 'epoch': 155.0}\n",
      "{'loss': 2.9335, 'grad_norm': 0.8906298279762268, 'learning_rate': 9.844961240310078e-05, 'epoch': 157.5}\n",
      "{'loss': 2.905, 'grad_norm': 0.9650862216949463, 'learning_rate': 9.842460615153789e-05, 'epoch': 160.0}\n",
      "{'loss': 2.8707, 'grad_norm': 0.8867849707603455, 'learning_rate': 9.839959989997501e-05, 'epoch': 162.5}\n",
      "{'loss': 2.8271, 'grad_norm': 0.9014183878898621, 'learning_rate': 9.83745936484121e-05, 'epoch': 165.0}\n",
      "{'loss': 2.7959, 'grad_norm': 0.9335613250732422, 'learning_rate': 9.834958739684922e-05, 'epoch': 167.5}\n",
      "{'loss': 2.7651, 'grad_norm': 0.8987115025520325, 'learning_rate': 9.832458114528632e-05, 'epoch': 170.0}\n",
      "{'loss': 2.7305, 'grad_norm': 0.902864933013916, 'learning_rate': 9.829957489372343e-05, 'epoch': 172.5}\n",
      "{'loss': 2.7001, 'grad_norm': 0.9539226293563843, 'learning_rate': 9.827456864216054e-05, 'epoch': 175.0}\n",
      "  2%|▋                                   | 700/40000 [29:16<20:11:54,  1.85s/it][INFO|trainer.py:3864] 2024-08-16 10:24:17,545 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3866] 2024-08-16 10:24:17,545 >>   Num examples = 1471\n",
      "[INFO|trainer.py:3869] 2024-08-16 10:24:17,545 >>   Batch size = 480\n",
      "\n",
      "  0%|                                                     | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      " 67%|██████████████████████████████               | 2/3 [00:20<00:10, 10.46s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 5.957082271575928, 'eval_bleu': 0.8129, 'eval_gen_len': 21.6451, 'eval_runtime': 69.6198, 'eval_samples_per_second': 21.129, 'eval_steps_per_second': 0.057, 'epoch': 175.0}\n",
      "  2%|▋                                   | 700/40000 [30:25<20:11:54,  1.85s/it]\n",
      "100%|█████████████████████████████████████████████| 3/3 [00:49<00:00, 14.56s/it]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:3548] 2024-08-16 10:25:27,165 >> Saving model checkpoint to models/marian/marian_output/checkpoint-700\n",
      "[WARNING|configuration_utils.py:448] 2024-08-16 10:25:27,167 >> Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 128, 'num_beams': 2, 'bad_words_ids': [[31999]], 'forced_eos_token_id': 2}\n",
      "[INFO|configuration_utils.py:472] 2024-08-16 10:25:27,168 >> Configuration saved in models/marian/marian_output/checkpoint-700/config.json\n",
      "[INFO|configuration_utils.py:807] 2024-08-16 10:25:27,169 >> Configuration saved in models/marian/marian_output/checkpoint-700/generation_config.json\n",
      "[INFO|modeling_utils.py:2778] 2024-08-16 10:25:27,806 >> Model weights saved in models/marian/marian_output/checkpoint-700/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2684] 2024-08-16 10:25:27,808 >> tokenizer config file saved in models/marian/marian_output/checkpoint-700/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2693] 2024-08-16 10:25:27,808 >> Special tokens file saved in models/marian/marian_output/checkpoint-700/special_tokens_map.json\n",
      "{'loss': 2.6697, 'grad_norm': 0.8885135650634766, 'learning_rate': 9.824956239059765e-05, 'epoch': 177.5}\n",
      "{'loss': 2.6382, 'grad_norm': 0.8902716636657715, 'learning_rate': 9.822455613903477e-05, 'epoch': 180.0}\n",
      "{'loss': 2.6137, 'grad_norm': 0.8916510939598083, 'learning_rate': 9.819954988747187e-05, 'epoch': 182.5}\n",
      "{'loss': 2.5852, 'grad_norm': 0.8855715394020081, 'learning_rate': 9.817454363590898e-05, 'epoch': 185.0}\n",
      "{'loss': 2.5512, 'grad_norm': 0.9340006113052368, 'learning_rate': 9.814953738434609e-05, 'epoch': 187.5}\n",
      "{'loss': 2.5316, 'grad_norm': 0.8990892171859741, 'learning_rate': 9.81245311327832e-05, 'epoch': 190.0}\n",
      "{'loss': 2.4936, 'grad_norm': 0.9003200531005859, 'learning_rate': 9.809952488122031e-05, 'epoch': 192.5}\n",
      "{'loss': 2.4782, 'grad_norm': 0.9772427082061768, 'learning_rate': 9.807451862965742e-05, 'epoch': 195.0}\n",
      "{'loss': 2.4455, 'grad_norm': 0.8947064280509949, 'learning_rate': 9.804951237809454e-05, 'epoch': 197.5}\n",
      "{'loss': 2.4256, 'grad_norm': 0.86408531665802, 'learning_rate': 9.802450612653163e-05, 'epoch': 200.0}\n",
      "  2%|▋                                   | 800/40000 [33:35<20:09:58,  1.85s/it][INFO|trainer.py:3864] 2024-08-16 10:28:36,591 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3866] 2024-08-16 10:28:36,592 >>   Num examples = 1471\n",
      "[INFO|trainer.py:3869] 2024-08-16 10:28:36,592 >>   Batch size = 480\n",
      "\n",
      "  0%|                                                     | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      " 67%|██████████████████████████████               | 2/3 [00:20<00:10, 10.16s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 6.058101177215576, 'eval_bleu': 0.7956, 'eval_gen_len': 19.6146, 'eval_runtime': 67.5801, 'eval_samples_per_second': 21.767, 'eval_steps_per_second': 0.059, 'epoch': 200.0}\n",
      "  2%|▋                                   | 800/40000 [34:42<20:09:58,  1.85s/it]\n",
      "100%|█████████████████████████████████████████████| 3/3 [00:47<00:00, 14.09s/it]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:3548] 2024-08-16 10:29:44,172 >> Saving model checkpoint to models/marian/marian_output/checkpoint-800\n",
      "[WARNING|configuration_utils.py:448] 2024-08-16 10:29:44,173 >> Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 128, 'num_beams': 2, 'bad_words_ids': [[31999]], 'forced_eos_token_id': 2}\n",
      "[INFO|configuration_utils.py:472] 2024-08-16 10:29:44,175 >> Configuration saved in models/marian/marian_output/checkpoint-800/config.json\n",
      "[INFO|configuration_utils.py:807] 2024-08-16 10:29:44,176 >> Configuration saved in models/marian/marian_output/checkpoint-800/generation_config.json\n",
      "[INFO|modeling_utils.py:2778] 2024-08-16 10:29:44,845 >> Model weights saved in models/marian/marian_output/checkpoint-800/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2684] 2024-08-16 10:29:44,846 >> tokenizer config file saved in models/marian/marian_output/checkpoint-800/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2693] 2024-08-16 10:29:44,847 >> Special tokens file saved in models/marian/marian_output/checkpoint-800/special_tokens_map.json\n",
      "{'loss': 2.3988, 'grad_norm': 0.8657212257385254, 'learning_rate': 9.799949987496875e-05, 'epoch': 202.5}\n",
      "{'loss': 2.3777, 'grad_norm': 0.9887462258338928, 'learning_rate': 9.797449362340586e-05, 'epoch': 205.0}\n",
      "{'loss': 2.3531, 'grad_norm': 0.9503695368766785, 'learning_rate': 9.794948737184296e-05, 'epoch': 207.5}\n",
      "{'loss': 2.3248, 'grad_norm': 0.917745053768158, 'learning_rate': 9.792448112028007e-05, 'epoch': 210.0}\n",
      "{'loss': 2.314, 'grad_norm': 0.8568497896194458, 'learning_rate': 9.789947486871719e-05, 'epoch': 212.5}\n",
      "{'loss': 2.2823, 'grad_norm': 0.878533661365509, 'learning_rate': 9.78744686171543e-05, 'epoch': 215.0}\n",
      "{'loss': 2.2621, 'grad_norm': 0.8605183959007263, 'learning_rate': 9.78494623655914e-05, 'epoch': 217.5}\n",
      "{'loss': 2.2449, 'grad_norm': 0.9070330858230591, 'learning_rate': 9.782445611402851e-05, 'epoch': 220.0}\n",
      "{'loss': 2.2254, 'grad_norm': 0.8832852840423584, 'learning_rate': 9.779944986246561e-05, 'epoch': 222.5}\n",
      "{'loss': 2.2007, 'grad_norm': 0.8621450662612915, 'learning_rate': 9.777444361090273e-05, 'epoch': 225.0}\n",
      "  2%|▊                                   | 900/40000 [37:50<20:16:54,  1.87s/it][INFO|trainer.py:3864] 2024-08-16 10:32:52,327 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3866] 2024-08-16 10:32:52,327 >>   Num examples = 1471\n",
      "[INFO|trainer.py:3869] 2024-08-16 10:32:52,327 >>   Batch size = 480\n",
      "\n",
      "  0%|                                                     | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      " 67%|██████████████████████████████               | 2/3 [00:20<00:10, 10.47s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 6.16868782043457, 'eval_bleu': 0.8954, 'eval_gen_len': 19.8028, 'eval_runtime': 68.7283, 'eval_samples_per_second': 21.403, 'eval_steps_per_second': 0.058, 'epoch': 225.0}\n",
      "  2%|▊                                   | 900/40000 [38:59<20:16:54,  1.87s/it]\n",
      "100%|█████████████████████████████████████████████| 3/3 [00:48<00:00, 14.38s/it]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:3548] 2024-08-16 10:34:01,056 >> Saving model checkpoint to models/marian/marian_output/checkpoint-900\n",
      "[WARNING|configuration_utils.py:448] 2024-08-16 10:34:01,057 >> Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 128, 'num_beams': 2, 'bad_words_ids': [[31999]], 'forced_eos_token_id': 2}\n",
      "[INFO|configuration_utils.py:472] 2024-08-16 10:34:01,059 >> Configuration saved in models/marian/marian_output/checkpoint-900/config.json\n",
      "[INFO|configuration_utils.py:807] 2024-08-16 10:34:01,059 >> Configuration saved in models/marian/marian_output/checkpoint-900/generation_config.json\n",
      "[INFO|modeling_utils.py:2778] 2024-08-16 10:34:01,672 >> Model weights saved in models/marian/marian_output/checkpoint-900/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2684] 2024-08-16 10:34:01,674 >> tokenizer config file saved in models/marian/marian_output/checkpoint-900/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2693] 2024-08-16 10:34:01,674 >> Special tokens file saved in models/marian/marian_output/checkpoint-900/special_tokens_map.json\n",
      "{'loss': 2.1808, 'grad_norm': 0.8383036851882935, 'learning_rate': 9.774943735933984e-05, 'epoch': 227.5}\n",
      "{'loss': 2.1568, 'grad_norm': 0.8852257132530212, 'learning_rate': 9.772443110777695e-05, 'epoch': 230.0}\n",
      "{'loss': 2.1343, 'grad_norm': 0.8498998880386353, 'learning_rate': 9.769942485621407e-05, 'epoch': 232.5}\n",
      "{'loss': 2.1315, 'grad_norm': 0.8802058696746826, 'learning_rate': 9.767441860465116e-05, 'epoch': 235.0}\n",
      "{'loss': 2.1043, 'grad_norm': 0.9825389981269836, 'learning_rate': 9.764941235308828e-05, 'epoch': 237.5}\n",
      "{'loss': 2.0854, 'grad_norm': 0.8695357441902161, 'learning_rate': 9.762440610152539e-05, 'epoch': 240.0}\n",
      "{'loss': 2.0691, 'grad_norm': 0.8505498766899109, 'learning_rate': 9.759939984996249e-05, 'epoch': 242.5}\n",
      "{'loss': 2.0502, 'grad_norm': 0.8763676881790161, 'learning_rate': 9.75743935983996e-05, 'epoch': 245.0}\n",
      "{'loss': 2.0301, 'grad_norm': 0.878303587436676, 'learning_rate': 9.754938734683672e-05, 'epoch': 247.5}\n",
      "{'loss': 2.0184, 'grad_norm': 0.8784624934196472, 'learning_rate': 9.752438109527382e-05, 'epoch': 250.0}\n",
      "  2%|▉                                  | 1000/40000 [42:08<20:09:48,  1.86s/it][INFO|trainer.py:3864] 2024-08-16 10:37:10,360 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3866] 2024-08-16 10:37:10,361 >>   Num examples = 1471\n",
      "[INFO|trainer.py:3869] 2024-08-16 10:37:10,361 >>   Batch size = 480\n",
      "\n",
      "  0%|                                                     | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      " 67%|██████████████████████████████               | 2/3 [00:19<00:09,  9.85s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 6.23810338973999, 'eval_bleu': 1.0815, 'eval_gen_len': 17.6319, 'eval_runtime': 66.1711, 'eval_samples_per_second': 22.23, 'eval_steps_per_second': 0.06, 'epoch': 250.0}\n",
      "  2%|▉                                  | 1000/40000 [43:14<20:09:48,  1.86s/it]\n",
      "100%|█████████████████████████████████████████████| 3/3 [00:46<00:00, 13.78s/it]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:3548] 2024-08-16 10:38:16,532 >> Saving model checkpoint to models/marian/marian_output/checkpoint-1000\n",
      "[WARNING|configuration_utils.py:448] 2024-08-16 10:38:16,533 >> Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 128, 'num_beams': 2, 'bad_words_ids': [[31999]], 'forced_eos_token_id': 2}\n",
      "[INFO|configuration_utils.py:472] 2024-08-16 10:38:16,535 >> Configuration saved in models/marian/marian_output/checkpoint-1000/config.json\n",
      "[INFO|configuration_utils.py:807] 2024-08-16 10:38:16,536 >> Configuration saved in models/marian/marian_output/checkpoint-1000/generation_config.json\n",
      "[INFO|modeling_utils.py:2778] 2024-08-16 10:38:17,167 >> Model weights saved in models/marian/marian_output/checkpoint-1000/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2684] 2024-08-16 10:38:17,168 >> tokenizer config file saved in models/marian/marian_output/checkpoint-1000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2693] 2024-08-16 10:38:17,169 >> Special tokens file saved in models/marian/marian_output/checkpoint-1000/special_tokens_map.json\n",
      "{'loss': 2.0003, 'grad_norm': 0.8812715411186218, 'learning_rate': 9.749937484371093e-05, 'epoch': 252.5}\n",
      "{'loss': 1.9783, 'grad_norm': 0.8720788359642029, 'learning_rate': 9.747436859214805e-05, 'epoch': 255.0}\n",
      "{'loss': 1.9668, 'grad_norm': 0.8926851749420166, 'learning_rate': 9.744936234058514e-05, 'epoch': 257.5}\n",
      "{'loss': 1.9484, 'grad_norm': 0.9048218727111816, 'learning_rate': 9.742435608902226e-05, 'epoch': 260.0}\n",
      "{'loss': 1.9324, 'grad_norm': 0.867774248123169, 'learning_rate': 9.739934983745937e-05, 'epoch': 262.5}\n",
      "{'loss': 1.9163, 'grad_norm': 1.0857411623001099, 'learning_rate': 9.737434358589648e-05, 'epoch': 265.0}\n",
      "{'loss': 1.903, 'grad_norm': 0.8716219067573547, 'learning_rate': 9.73493373343336e-05, 'epoch': 267.5}\n",
      "{'loss': 1.8808, 'grad_norm': 0.9057334065437317, 'learning_rate': 9.732433108277069e-05, 'epoch': 270.0}\n",
      "{'loss': 1.8733, 'grad_norm': 0.8595719337463379, 'learning_rate': 9.729932483120781e-05, 'epoch': 272.5}\n",
      "{'loss': 1.8504, 'grad_norm': 0.855239987373352, 'learning_rate': 9.727431857964491e-05, 'epoch': 275.0}\n",
      "  3%|▉                                  | 1100/40000 [46:23<20:01:12,  1.85s/it][INFO|trainer.py:3864] 2024-08-16 10:41:25,481 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3866] 2024-08-16 10:41:25,482 >>   Num examples = 1471\n",
      "[INFO|trainer.py:3869] 2024-08-16 10:41:25,482 >>   Batch size = 480\n",
      "\n",
      "  0%|                                                     | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      " 67%|██████████████████████████████               | 2/3 [00:20<00:10, 10.08s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 6.370576858520508, 'eval_bleu': 1.1258, 'eval_gen_len': 17.9521, 'eval_runtime': 66.3918, 'eval_samples_per_second': 22.156, 'eval_steps_per_second': 0.06, 'epoch': 275.0}\n",
      "  3%|▉                                  | 1100/40000 [47:30<20:01:12,  1.85s/it]\n",
      "100%|█████████████████████████████████████████████| 3/3 [00:47<00:00, 13.91s/it]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:3548] 2024-08-16 10:42:31,874 >> Saving model checkpoint to models/marian/marian_output/checkpoint-1100\n",
      "[WARNING|configuration_utils.py:448] 2024-08-16 10:42:31,875 >> Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 128, 'num_beams': 2, 'bad_words_ids': [[31999]], 'forced_eos_token_id': 2}\n",
      "[INFO|configuration_utils.py:472] 2024-08-16 10:42:31,877 >> Configuration saved in models/marian/marian_output/checkpoint-1100/config.json\n",
      "[INFO|configuration_utils.py:807] 2024-08-16 10:42:31,878 >> Configuration saved in models/marian/marian_output/checkpoint-1100/generation_config.json\n",
      "[INFO|modeling_utils.py:2778] 2024-08-16 10:42:32,552 >> Model weights saved in models/marian/marian_output/checkpoint-1100/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2684] 2024-08-16 10:42:32,553 >> tokenizer config file saved in models/marian/marian_output/checkpoint-1100/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2693] 2024-08-16 10:42:32,553 >> Special tokens file saved in models/marian/marian_output/checkpoint-1100/special_tokens_map.json\n",
      "{'loss': 1.8458, 'grad_norm': 0.8790155649185181, 'learning_rate': 9.724931232808202e-05, 'epoch': 277.5}\n",
      "{'loss': 1.8251, 'grad_norm': 0.8649742007255554, 'learning_rate': 9.722430607651914e-05, 'epoch': 280.0}\n",
      "{'loss': 1.8075, 'grad_norm': 0.8483395576477051, 'learning_rate': 9.719929982495625e-05, 'epoch': 282.5}\n",
      "{'loss': 1.7949, 'grad_norm': 0.8766046762466431, 'learning_rate': 9.717429357339335e-05, 'epoch': 285.0}\n",
      "{'loss': 1.7897, 'grad_norm': 0.879315197467804, 'learning_rate': 9.714928732183046e-05, 'epoch': 287.5}\n",
      "{'loss': 1.7728, 'grad_norm': 0.8967851400375366, 'learning_rate': 9.712428107026758e-05, 'epoch': 290.0}\n",
      "{'loss': 1.7568, 'grad_norm': 0.8545645475387573, 'learning_rate': 9.709927481870467e-05, 'epoch': 292.5}\n",
      "{'loss': 1.7454, 'grad_norm': 0.8847944736480713, 'learning_rate': 9.707426856714179e-05, 'epoch': 295.0}\n",
      "{'loss': 1.7339, 'grad_norm': 1.0275734663009644, 'learning_rate': 9.70492623155789e-05, 'epoch': 297.5}\n",
      "{'loss': 1.7206, 'grad_norm': 0.8647608160972595, 'learning_rate': 9.7024256064016e-05, 'epoch': 300.0}\n",
      "  3%|█                                  | 1200/40000 [50:38<19:51:28,  1.84s/it][INFO|trainer.py:3864] 2024-08-16 10:45:39,822 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3866] 2024-08-16 10:45:39,822 >>   Num examples = 1471\n",
      "[INFO|trainer.py:3869] 2024-08-16 10:45:39,822 >>   Batch size = 480\n",
      "\n",
      "  0%|                                                     | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      " 67%|██████████████████████████████               | 2/3 [00:19<00:09,  9.70s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 6.457111358642578, 'eval_bleu': 1.2026, 'eval_gen_len': 16.0958, 'eval_runtime': 64.5822, 'eval_samples_per_second': 22.777, 'eval_steps_per_second': 0.062, 'epoch': 300.0}\n",
      "  3%|█                                  | 1200/40000 [51:42<19:51:28,  1.84s/it]\n",
      "100%|█████████████████████████████████████████████| 3/3 [00:45<00:00, 13.39s/it]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:3548] 2024-08-16 10:46:44,405 >> Saving model checkpoint to models/marian/marian_output/checkpoint-1200\n",
      "[WARNING|configuration_utils.py:448] 2024-08-16 10:46:44,406 >> Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 128, 'num_beams': 2, 'bad_words_ids': [[31999]], 'forced_eos_token_id': 2}\n",
      "[INFO|configuration_utils.py:472] 2024-08-16 10:46:44,408 >> Configuration saved in models/marian/marian_output/checkpoint-1200/config.json\n",
      "[INFO|configuration_utils.py:807] 2024-08-16 10:46:44,409 >> Configuration saved in models/marian/marian_output/checkpoint-1200/generation_config.json\n",
      "[INFO|modeling_utils.py:2778] 2024-08-16 10:46:45,041 >> Model weights saved in models/marian/marian_output/checkpoint-1200/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2684] 2024-08-16 10:46:45,042 >> tokenizer config file saved in models/marian/marian_output/checkpoint-1200/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2693] 2024-08-16 10:46:45,043 >> Special tokens file saved in models/marian/marian_output/checkpoint-1200/special_tokens_map.json\n",
      "{'loss': 1.701, 'grad_norm': 0.8488897085189819, 'learning_rate': 9.699924981245312e-05, 'epoch': 302.5}\n",
      "{'loss': 1.6954, 'grad_norm': 0.8782679438591003, 'learning_rate': 9.697424356089023e-05, 'epoch': 305.0}\n",
      "{'loss': 1.6784, 'grad_norm': 0.8125242590904236, 'learning_rate': 9.694923730932734e-05, 'epoch': 307.5}\n",
      "{'loss': 1.6715, 'grad_norm': 0.8457626700401306, 'learning_rate': 9.692423105776444e-05, 'epoch': 310.0}\n",
      "{'loss': 1.6521, 'grad_norm': 0.8886157274246216, 'learning_rate': 9.689922480620155e-05, 'epoch': 312.5}\n",
      "{'loss': 1.6407, 'grad_norm': 0.8926604390144348, 'learning_rate': 9.687421855463867e-05, 'epoch': 315.0}\n",
      "{'loss': 1.634, 'grad_norm': 0.8541426658630371, 'learning_rate': 9.684921230307578e-05, 'epoch': 317.5}\n",
      "{'loss': 1.625, 'grad_norm': 1.053877830505371, 'learning_rate': 9.682420605151288e-05, 'epoch': 320.0}\n",
      "  3%|█                                  | 1284/40000 [54:21<19:52:08,  1.85s/it]^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/zindi/transformers/examples/pytorch/translation/run_translation.py\", line 697, in <module>\n",
      "    main()\n",
      "  File \"/root/zindi/transformers/examples/pytorch/translation/run_translation.py\", line 612, in main\n",
      "    train_result = trainer.train(resume_from_checkpoint=checkpoint)\n",
      "  File \"/root/zindi/transformers/src/transformers/trainer.py\", line 1964, in train\n",
      "    return inner_training_loop(\n",
      "  File \"/root/zindi/transformers/src/transformers/trainer.py\", line 2305, in _inner_training_loop\n",
      "    tr_loss_step = self.training_step(model, inputs)\n",
      "  File \"/root/zindi/transformers/src/transformers/trainer.py\", line 3361, in training_step\n",
      "    loss = self.compute_loss(model, inputs)\n",
      "  File \"/root/zindi/transformers/src/transformers/trainer.py\", line 3408, in compute_loss\n",
      "    outputs = model(**inputs)\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/zindi-LtLKIbXv-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/zindi-LtLKIbXv-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/root/zindi/transformers/src/transformers/models/marian/modeling_marian.py\", line 1399, in forward\n",
      "    outputs = self.model(\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/zindi-LtLKIbXv-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/zindi-LtLKIbXv-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/root/zindi/transformers/src/transformers/models/marian/modeling_marian.py\", line 1194, in forward\n",
      "    decoder_outputs = self.decoder(\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/zindi-LtLKIbXv-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/zindi-LtLKIbXv-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/root/zindi/transformers/src/transformers/models/marian/modeling_marian.py\", line 994, in forward\n",
      "    layer_outputs = decoder_layer(\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/zindi-LtLKIbXv-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/zindi-LtLKIbXv-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/root/zindi/transformers/src/transformers/models/marian/modeling_marian.py\", line 404, in forward\n",
      "    hidden_states, self_attn_weights, present_key_value = self.self_attn(\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/zindi-LtLKIbXv-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/root/.cache/pypoetry/virtualenvs/zindi-LtLKIbXv-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/root/zindi/transformers/src/transformers/models/marian/modeling_marian.py\", line 185, in forward\n",
      "    key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n",
      "  File \"/root/zindi/transformers/src/transformers/models/marian/modeling_marian.py\", line 140, in _shape\n",
      "    return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
      "KeyboardInterrupt\n",
      "  3%|█                                  | 1284/40000 [54:22<27:19:43,  2.54s/it]\n"
     ]
    }
   ],
   "source": [
    "!python transformers/examples/pytorch/translation/run_translation.py \\\n",
    "--per_device_train_batch_size $$per_device_train_batch_size \\\n",
    "--per_device_eval_batch_size $$per_device_eval_batch_size \\\n",
    "--save_steps $$save_steps \\\n",
    "--num_train_epochs $$num_train_epochs \\\n",
    "--logging_steps $$logging_steps \\\n",
    "--label_smoothing_factor $$label_smoothing_factor \\\n",
    "--learning_rate $$learning_rate \\\n",
    "--run_name $$run_name \\\n",
    "--output_dir $$output_dir \\\n",
    "--logging_dir $$logging_dir \\\n",
    "--eval_steps $$eval_steps \\\n",
    "--gradient_accumulation_steps $$gradient_accumulation_steps \\\n",
    "--model_name_or_path  $$model_name_or_path  \\\n",
    "--dataset_name  $$dataset_name  \\\n",
    "--generation_max_length $$generation_max_length \\\n",
    "--generation_num_beams $$generation_num_beams \\\n",
    "--source_lang $$source_lang \\\n",
    "--target_lang $$target_lang \\\n",
    "--dataset_config_name $$dataset_config_name \\\n",
    "--predict_with_generate $$predict_with_generate \\\n",
    "--max_source_length $$max_source_length \\\n",
    "--dataloader_drop_last $$dataloader_drop_last \\\n",
    "--warmup_steps $$warmup_steps \\\n",
    "--weight_decay $$weight_decay \\\n",
    "--save_total_limit $$save_total_limit \\\n",
    "--seed $$seed \\\n",
    "--overwrite_output_dir $$overwrite_output_dir \\\n",
    "--jit_mode_eval $$jit_mode_eval \\\n",
    "--do_train $$do_train \\\n",
    "--fp16 $$fp16 \\\n",
    "--fp16_backend $$fp16_backend \\\n",
    "--fp16_full_eval $$fp16_full_eval \\\n",
    "--full_determinism $$full_determinism \\\n",
    "--predict_with_generate true \\\n",
    "--do_eval true \\\n",
    "--do_predict true \\\n",
    "--eval_strategy steps \\\n",
    "--generation_config $$generation_config \n",
    "# --resume_from_checkpoint {resume_from_checkpoint} \n",
    "# --use_cpu {use_cpu} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zindi-z3yfXQo9-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
