services:
  translation_inference_util:
    command: --model_name=model
    container_name: translation_inference_ctutil
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 2G
    image: zindi-image:0.2.5
    platform: linux/amd64
    ports:
    - 8080:8080
    working_dir: /app
